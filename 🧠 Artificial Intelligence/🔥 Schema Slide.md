---
aliases:
tags:
  - schema
dg-publish: true
---
# 1 Agents

> Ãˆ qualunque cosa che puÃ² interagire

Possiede:

- **Sensori** â†’ per percepire l'ambiente
- **Attuatori** â†’ per effettuare azioni nell'ambiente

$f:P^{*}\to A$

Dove:

- $P^{*}$ â†’ Ã¨ la storia delle percezioni
- $A$ â†’ Ã¨ l'insieme delle possibili azioni di un agent 

![[allegati/Image.png|711x394]]

+ *The First Agent*
    semplice senza valutare nessun tipo di valutazione, completa l'obbiettivo senza considerare il tempo o i cicli. Non considera i cambiamenti intermedi

```cpp
function Reflex-Vacuum-Agent( [location,status]) returns an action
  if status = Dirty then return Suck
  else if location = A then return Right
  else if location = B then return Left
```

**Rational Agents** â†’ Ã¨ un agente che sceglie le proprie azioni allo scopo di massimizzare le metriche i performance, dato lo storico delle percezioni ^RationalAgent

- Non Ã¨ onnisciente
- Non prevede il futuro
- Non ha sempre successo

*AMBIENTE STATICO* â†’ ossia completamente osservabile e prevedibile (azioni deterministiche con regole chiare) incoraggiando ad imparare comportamenti fissi.

- La soluzione ottima non cambia, stesso obbiettivo
- Soluzione fragile
    - *Esempio Vespa*: che come c'Ã¨ un cambiamento riparte dall'azione che scaturisce la percezione quindi spreco di cicli inutile

## 1.1 PEAS

***P***erformance metric

***E***nviroment

***A***ctuators

***S***ensors

PiÃ¹ Ã¨ ristretto l'ambiente â†’ piÃ¹ l'agente sarÃ  semplice da costruire

- Agent Architecture = $A + S$
- Agent = Architecture + Function
+ Example Taxi

Performance metric: safety, destination, profits, legality, comfort, ....
Environment: streets/freeways, traffic, pedestrians, weather, ...
Actuators: steering, accelerator, brake, horn, speaker/display, â€¦
Sensors: camera, accelerometers, gauges, engine sensors, keyboard, GPSâ€¦

## 1.2 Enviroments - Ambienti

![[allegati/Image (2).png]]

> **Single-Agent** â†’ ossia che nell'ambiente agisce un solo agente e non deve competere con altri

> Real Agent â†’ Ã¨ come il taxi non comprende nessuna delle caratteristiche, il che lo rende molto complicato da realizzare

- **Table-Driven Agent** â†’ ossia prende decisioni consultando una **tabella di lookup** in cui, per ogni possibile percezione (o sequenza di percezioni), Ã¨ giÃ  memorizzata lâ€™azione da compiere.
    - ComplessitÃ  alta per la creazione della tabella
- **Reflex Agent** â†’ insieme di regole condizionali dirette â€œcondizione â†’ azioneâ€ ^ReflexAgent

![[Image (3).png|755x455]]

***Effectiveness-Efficiency Trade-off***

| **TA**                                     | **SRA**                                                            |
| ------------------------------------------ | ------------------------------------------------------------------ |
| Alti costi per gestire e creare la tabella | Riduce i costi creando solo delle condizioni di attivazione        |
| Scala esponenzialmente il tempo â†’ $O(n^2)$ | Scala linearmente il tempo â†’ $O(n)$                                |
|                                            | Semplifica un approccio giÃ  base e stupido â†’ osservabile e piccolo |

## 1.3 Tipi di Agenti

### 1.3.1 Model-Based Agent

^5938be

![[Image (4).png|748x383]]

Aggiunge â†’ *STATE* al [[#^ReflexAgent]

- Stima lo stato attuale dell'ambiente basandosi sullo stato interno dell'agente
    - Lo stato interno dipende dallo storico delle percezioni
- Lo stato interno serve per catturare il modello delle transizioni dell'ambiente
    - Conseguenze dell'azione dell'agente
    - Dinamiche dell'ambiente
- *Sensor Model* â†’ come i sensori dell'agente rappresentano il mondo - rumore e limitazioni
    - Approssima l'intero stato â†’ efficace in ambianti parzialmente osservabili

### 1.3.2 Goal-Based Agent

![[Image (5).png|702x430]]

â“Massimizzare la ricompensa immediata vs massimizzare la ricompensa a lungo termine

- **Search** â†’ come Ã¨ meglio navigare tra le possibili soluzioni? Decision Tree
- **Planning** â†’ sfruttare la conoscenza del mondo per eseguire le migliori azioni

> I modelli goal-based rappresentano il loro obbiettivo (*goal*) e possono modificarlo

### 1.3.3 Utility-Based Agent

![[Image (6).png|727x448]]

> Invece di utilizzare un solo stato, assegna un punteggio ad ogni stato interno che indica quanto Ã¨ buono

- Permette di analizzare varie traiettorie e quindi tenere in considerazione non solo se arrivo al traguardo ma anche come ci arrivo â†’ permette di selezionare la soluzione migliore tra quelle esistenti risparmiando sulle mosse da fare per esempio

Ãˆ fondamentale quando:

- ci sono stati finali **non tutti equivalenti** (es. vincere con 10 mosse vs con 100 mosse),
- lâ€™ambiente Ã¨ **stocastico** (es. backgammon),
- ci sono piÃ¹ obiettivi in conflitto (es. â€œarrivare a destinazione **velocemente ma in sicurezza**â€)

### 1.3.4 Learning-Based Agent

![[Image (7).png|782x329]]

Tutti i modelli di agenti possono essere learning-based o no.

L'apprendimento puÃ² modificare qualche componente.

- **Elementi prestazionali** â†’ i precedenti agenti ricevevano il contesto e sceglievano l'azione da effettuare
- **Critic** â†’ guida l'apprendimento tramite la valutazione del comportamento attuale con la metrica delle performance esterne
- **Problem** â†’ allontanarsi dal comportamento greedy provando nuove azioni (greedy = sceglie l'opzione migliore senza considerare altre soluzioni) 

## 1.4 Summary

- **Agenti** interagiscono con **l'ambiente** tramite **attuatori** e **sensori**
- La **funzione** degli agenti â†’  [Agent = Architecture + Function](craftdocs://open?blockId=4F5E2847-6565-43CC-A12A-EB81130E4DFF&spaceId=4b28628f-7dfd-54ce-27f9-28712867f81f) â†’ descrive cosa fa l'agente in tutte le circostanze
- La **misurazione delle prestazioni** valuta la sequenza dell'ambiente
- Un [[#^RationalAgent]]
- [PEAS](./ğŸ”¥ Schema Slide.md#peas) â†’ descrive gli ambienti di attivitÃ 
- [Enviroments - Ambienti](./ğŸ”¥ Schema Slide.md#enviroments--ambienti) â†’ Osservabile, Deterministico, Episodico, Statico, Discreto
- Molti tipi di Agenti
    - [[#^ReflexAgent]]
    - [[#1.3.1 Model-Based Agent]]
    - [[#1.3.2 Goal-Based Agent]]]
    - [[#1.3.2 Goal-Based Agent]]

---

# 2 Search

> Search Alorithm â†’ dato un problema, ritorna una sequenza di azioni, che formano un path verso lo stato *goal* o *fail*

**Open-Loop** - *offline* â†’ un agente cerca la sequenza ottimale di azioni e successivamente la esegue
- *Completamente osservabile*
- *Deterministico* â†’ risposte dell'ambiente conosciute

**Closed-Loop** - *online* â†’ l'agente valuta continuamente i feedback provenienti dall'ambiente
- *parzialmente osservabile*
- *Stocastico* â†’ le risposte variano
	- Devo controllare il mondo per sapere dove sono, in quale stato

![[Image (8).png|752x448]]

> **Scelte di Design â†’ Guida veloce**

- **Deterministica e completamente osservabile** â†’ problema a stato singolo
    - l'agente sa esattamente in quel stato si troverÃ  â†’ la soluzione Ã¨ una sequenza di stati o azioni
- > **Non osservabile** â†’ problema conforme senza sensore
    - L'agente non percepisce lo stato â†’ soluzione Ã¨ una sequenza di azioni
- *Non deterministico e parzialmente osservabile** â†’ problema di contingenza
    - le percezioni prendono nuove informazioni riguardo allo stato
    - la soluzione Ã¨ un piano contingente o una policy
    - spesso ricerca interlacciata
- **Spazio degli Stati Sconosciuto** â†’ problema di esplorazione (online - closed-loop)
    - l'agente deve scoprire lo spazio degli stati

![[Image (9).png|741x278]]

![[Image (10).png|738x361]]

## 2.1 Tree Search

> Inizia dagli stati giÃ  visitati e gli espande â†’ genera gli stati successivi

- > **Frontiera** â†’ insieme di nodi non espansi che separano i nodi espansi da quelli non raggiunti
- > **Nodi Raggiunti** â†’ *frontiera* + *nodi espansi*
- > Ricerca **Uniforme**

```cpp
function Tree-Search(problem, strategy) return a solution or failure
  inizializza con initial state del problema
  loop do
    if non ci sono candidati return failure
    scegli un nodo da espandere in base alla strategia
    if nodo contine goal state return soluzione
    else espandi il nodo e aggiungi il nodo alla ricerca ad albero
  end
```

![[Image (11).png]]

![[Image (12).png]]

- **State** â†’ uno stato Ã¨ una rappresentazione di una configurazione fisica
- **Nodo** â†’ Ã¨ una struttura dati che costituisce una parte della ricerca ad albero
    - include genitori, figli. profonditÃ  e costo dei path (cammini)
- **La funzione di Espansione**
    - Crea nuovi nodi
    - Riempie i vari campi
    - Usa la funzione *Successore* del problema per creare gli stati corrispondenti

> La ricerca ad albero non corrisponde all'intero spazio degli stati â†’ un insieme di satti finito puÃ² portare ad una ricerca infinita (loops, path ridondanti)

> Nodi nella ricerca ad albero = stati dello spazio

Gli stati sono tutti diversi nello spazio

I nodi non sono unici in un albero di ricerca â†’ se abbiamo un nodo che ritorna ad un altro giÃ  esplorato devo comunque selezionarlo

### 2.1.1 Scegliere una Strategia di Ricerca

> Una strategia di ricerca Ã¨ definita **in base all'ordine con cui i nodi vengono espansi**

Una **strategia** Ã¨ **valutata** tramite queste dimensioni:

- **Completezza** â†’ trova sempre una soluzione
- **ComplessitÃ  di Tempo** â†’ numero di nodi generati o espansi
- **ComplessitÃ  dello Spazio** â†’ numero massimo di nodi in memoria
- **Ottimale** â†’ trova sempre la soluzione con meno costo

ComplessitÃ  di Tempo e Spazio sono misurati in:

- $b$ â†’ fattore massimo di ramificazione dell'albero di ricerca (quanti nodi espando)
- $d$ â†’ profonditÃ  della soluzione migliore (minor costo)
- $m$ â†’ massima profonditÃ  dello spazio degli stati (puÃ² essere $\infty$)

## 2.2 Uninformed Tree Search - Non Informata

> Una ricerca non informata usa solo le informazioni disponibili quando definiamo il problema, non sappiamo quanto siamo lontani dal goal (obbiettivo)

- Breadth-first search
- Uniform-cost search
- Depth-first search
- Depth-limited search
- Iterative deepening search

### 2.2.1 Breadth-First Search
- Frontiera â†’ Ã¨ una coda FIFO
- Espande prima in larghezza poi in profonditÃ 

![[Image (13).png]]

![[Image (14).png]]

| **Completo**    | **Time**                                         | **Space**                      | **Optimal**                                       |
| --------------- | ------------------------------------------------ | ------------------------------ | ------------------------------------------------- |
| **Si**          | $\mathbf{O}(b^d)$                                | $\mathbf{O}(b^d)$              | **Si**                                            |
| Se $b$ Ã¨ finito | Dato che fa tutti i nodi ad una certa profonditÃ  | Prende tutti i nodi in memoria | Se il costo per step Ã¨ $1$ non ottimo in generale |

#### Dijkstra's Algorithm â†’ ricerca costo uniforme

> Le azioni hanno un costo differente â†’ espandere i nodi con il cammino di costo minimo

> La frontiera Ã¨ una coda pesata â†’ i path con costo minore vanno per primi

| **Completo**                                          | **Time**                                                                                                                | **Space**                | **Optimal**                                                |
| ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ------------------------ | ---------------------------------------------------------- |
| **Si**                                                | $\mathbf{O}(b^{1+[\frac{C^*}{\eta}]})$                                                                                  | $\mathbf{O}(b^d)$        | **Si**                                                     |
| Se il costo minimo delle azioni Ã¨ positivo ossia $>0$ | Dove $C^*$ Ã¨ il costo del path ottimale â†’ puÃ² eplorare percorsi a basso costo che non servono a niente puÃ² essere $b^d$ | **Stesso della memoria** | I nodi sono espansi in ordine in base al costo del cammino |

### 2.2.2 Depth-First Search

> Espande prima i nodi in profonditÃ 

> Coda â†’ LIFO â†’ mette i nodi in cima (Ultimo ad entrare primo ad uscire Last In - First Out)

![[Image (15).png]]

![[Image (16).png]]

| **Completo**                 | **Time**                                                                       | **Space**        | **Optimal** |
| ---------------------------- | ------------------------------------------------------------------------------ | ---------------- | ----------- |
| **Si**                       | $\mathbf{O}(b^m)$                                                              | $\mathbf{O}(bm)$ | No          |
| Se $b$ Ã¨ finito e senza loop | Dato che vado prima in profonditÃ  esploro prima tutti i fligli fino al massimo | **Lineare**      |             |

> DFS puÃ² essere implementato in vari modi:

- > **Depth-Limited search** â†’ con profonditÃ  di esplorazione fissata
- > **Iterative Deeping Search** â†’ iterazioni con valore di profonditÃ  che aumenta
- > **Bidirectional Search** â†’ iniziamo la ricerca dallo stato iniziale e dallo stato obbiettivo verso lo stato iniziale

#### Come Riconoscere uno Stato Ripetuto?

- **Ricordare gli stati raggiunti**
    - Posso riconoscere e conservare i path di costo minore
    - Se ho spazio
- Non controllo se il problema non produce path ridondanti
- **Controlla solo i loop**
    - Conserva i genitori per ogni nodo
    - Segue la catena dei parenti per vedere se ricade in un loop
    - Posso seguire la catena fino al root â†’ puntatori al padre
    - **Non c'Ã¨ bisogno di memoria in piÃ¹** solo **piÃ¹ tempo per controllare la catena**

### 2.2.3 Sommario â†’ Uninformed Search

![[Image (17).png]]

## 2.3 Heauristic - Informed Search

> Espandere i nodi piÃ¹ promettenti

> Promettenti Ã¨ una valutazione euristica â†’ propone una stima del minor costo da uno stato al nodo $n$ fino al goal

> Frontiera â†’ Ã¨ una coda pesata e ordinata in base al valore euristico

Best-First Search Algorithm:

- Greedy Search
- A* Search

### 2.3.1 Greedy Search

> Seleziono ad ogni nodo quello con il costo euristico minore

- Non ha meccanismi per trovare il cammino migliore guardando i successivi
- Potrebbe portare a cammini di costo maggiore anche se il primo passo Ã¨ piÃ¹ conveniente

| **Completo**                                                                                                | **Time**                                       | **Space**                     | **Optimal** |
| ----------------------------------------------------------------------------------------------------------- | ---------------------------------------------- | ----------------------------- | ----------- |
| No                                                                                                          | $\mathbf{O}(b^m)$                              | $\mathbf{O}(b^m)$             | No          |
| PuÃ² rimanere incastrato in un loop.<br/>Completo in uno spazio finito con il controllo degli stati ripetuti | Una buona euristca puÃ² portare a miglioramenti | Tiene tutti i nodi in memoria |             |

### 2.3.2 A* Search

> Idea â†’ evitare di espandere i cammini giÃ  costosi

> Controllo se un cammino Ã¨ piÃ¹ costoso di un altro cammino che ho giÃ  esplorato se si allora continuo il cammino con il costo minore e cosÃ¬ via

> In questo modo tengo in considerazione il cammino di costo minimo generale, in modo da poter scegliere quello minore analizzando anche cammini alternativi

**Funzione di Valutazione** â†’ $f(n)=g(n)+h(n)$

- $g(n)$ = costo fino a questo momento per raggiungere $n$ 
- $h(n)$ = costo stimato al goal da $n$ â†’ euristica greedy
- $f(n)$ = costo stimato del path migliore passando attraverso $n$ per raggiungere il goal
- Questa funzione valuta ad ogni nodo il costo decidendo come proseguire

**ComplessitÃ  di Spazio** â†’ la complessitÃ  scala con il numero di nodi espanso

- $A*$ espande tutti i nodi con $f(n)<C^*$
- $A*$ espande qualche nodo con $f(n)=C^*$
- $A*$ non espande nodi con $f(n)>C^*$
- Con $C^*$ costo del path ottimale 

**ComplessitÃ  di Tempo** â†’ Ã¨ esponenziale nell'errore di $h$

A* utilizza un euristica *ammissibile*

- $h(n)\le h^*(n)$ dove $h^*(n)$ Ã¨ il vero costo da $n$
- $h(n)\ge 0$ tale che $h(G)=0$ per ogni goal $G$
- Un euristica ammissibile non sovrastima mai la distanza dal goal

***A** Ã¨ completa**

***A$^*$* Ã¨ ottima se *h* Ã¨ ammissibile**

> **Euristica Consistente** â†’ $h(n)\le c(n,a,n \prime)+h(n \prime),\ \forall n\prime$

- PiÃ¹ forte dell'ammissibilitÃ  â†’ il costo dei cammini Ã¨ monotona crescente
    - Ossia il valore di $f(n)$ non scende mai lungo il cammino
- Quando raggiungi uno stato Ã¨ giÃ  sulla la strada ottimale per arrivare ad $n$
    - Se l'euristica fosse solo ammissibile invece potrei estrarre un nodo con un cammino sub-ottimale e doverlo riconsiderare in seguito

![[Image (18).png]]

### 2.3.3 Problema
> A* visita troppi nodi â†’ richiede che vengano esplorati ed espansi molti nodi
> â†’ **A* Pesato**: sub-optimal ma abbastanza buono
> $f(n)=g(n)+W\ h(n),\ \ W>1$
> Trova una soluzione con costo tra $C^*$ e $W\ C^*$
> -> Spesso vicina a $C^*$

#### Contorni con A*

![[Image (19).png|714x431]]

![[Image (20).png]]

![[Image (21).png]]

#### Beam Search
LaÂ **beam search**Â Ã¨ un algoritmo di ricercaÂ basato suÂ **euristiche** Â che esplora un grafo espandendo il nodo piÃ¹ promettente in un insieme limitato di nodi.
La beam search  simile aÂ [[#2.2.1 Breadth-First Search]] limitata.Â che mira a ridurre i requisiti di memoria. Nella beam search Ã¨ tenuto in considerazione solo un numero predefinito di migliori soluzioni parziali.Â Di conseguenza, Ã¨ considerato unÂ algoritmo greedy.
- Lâ€™euristica non Ã¨ necessariamente ammissibile nÃ© ottimale
- **Obiettivo tipico**: generare sequenze plausibili in problemi di decoding
- Mantiene iÂ **B migliori cammini parziali**Â in un albero di ricerca.
- Ad ogni passo: espande ognuno, valuta i successori, sceglie i B migliori, scarta gli altri.
- Non garantisce la soluzione ottimale

---
# 3 Local Search
Cosa vogliamo:
- Veloce
- Economico
- Abbastanza buono
- Ricerca non sistematica
	- Non ci interessa come ci arriviamo all'obbiettivo ma vogliamo arrivarci

> Local search restituisce una soluzione non un path
> - Non salva gli stati visitati
> - Non esplora l'intero spazio degli stati

## 3.1 Hill Climbing
> Hill-climbing puÃ² rimanere bloccato
	- Ottimo locale
	- Greedy Local Search


> [!NOTE] Gradient Descent
> Se uso il gradiente per valutare e guidare la funzione di ricerca otteniamo la discesa del gradiente.

- si blocca su i massimi locali
- Si blocca su parti pari (flat)
### 3.1.1 Soluzioni
**Random Restart**
- Riesce a scapapre dall'ottimo locale
- Completo ma molto lento
**Sideway Move** -> *movimento laterale*
- Riesce ad uscire dalle parti flat se non sono il massimo
- Fisso -> Si sposta di massimo $k$ step
![[image-1.png|525x302]]

```c title="Hill Climbing"
function Hill-Climbing(problem) returns a state that is a local maximum
    inputs: problem, a problem definition
    local variables: current, a node, neighbor, a node
    
    current â† Make-Node(Initial-State(problem))
    loop do
        neighbor â† a highest-valued successor of current
        if Value(neighbor) â‰¤ Value(current)
        then return State(current)
        current â† neighbor
end
```

## 3.2 Simulated Annealing
> Uscire dall'ottimo locale facendo un passo random
> - un passo random non rimane mai incastrato -> ma Ã¨ lento


> [!important] Simulated Annealing
> Corrisponde a -> *hill climbing $+$ random moves*
> - I movimenti random diventano meno frequenti nel tempo
> - I movimenti casuali migliorano con il tempo
> - **Temperature** -> Ã¨ un parametro che controlla il trade-off tra hill-climbing e movimenti random
> 	- La temperatura cambia con il tempo permettendo all'inizio di fare molti passi casuali e mano a mano che vado avanti i passi casuali diminuiscono
> 	- Alla fine $T=0$ -> nessun passo casuale


### 3.2.1 Temperature Scheduling
$$
e^{\frac{x}{T}},\quad T>0,\quad x>0
$$
- $x$ Grande -> Ã¨ una mossa molto sbagliata, c'Ã¨ una bassa probabilitÃ  di accettare lo spostamento
- Aumentando $T$ -> aumento la probabilitÃ  di accettare spostamenti
Se $T\to 0$ allora anneiling diventa deterministico -> non effettua mosse casuali
- Accetta solo spostamenti buoni $e^{\frac{x}{T}}\to 0$ 
**Anneiling Scheduling** -> inizia con T grande e lo diminuisce con il passare del tempo
- Se T viene diminuite abbastanza lentamente l'algoritmo converge al *massimo globale* 
	- Se invece Ã¨ troppo lento visito tutti gli stati -> lentissimo
- *Diminuzione Esponenziale* -> $T_t = k\ T_{t-1},\quad 0<k<1$
- *Power-law Decrese* -> $T_t=\frac{T_0}{t^a}$ es: $a=1$
### 3.2.2 Local Beam Search
> Invece di prendere solo la prima soluzione prendo le prime $k$
> Espando tutto e seleziono i top-$k$ stati
> - Ha senso quando la funzione non converge sicuramente


> [!important] Funzionamento
> Ãˆ la versione stocastica della [[#Beam Search]]
> Gli stati interagiscono con gli altri -> abbandono i path con meno importanza, concentrandomi su quelli piÃ¹ promettenti
> - Non Ã¨ una ricerca parallela su $k$ path
> - *Local beam -> parte da k stati, tiene i migliori k successori globalmente.*
> - **Non garantisce ottimalitÃ **

#### Beam search decoder in NLP

NLP -> Natural Language Processing
Tutto ciÃ² che concerne l'elaborazione del linguaggio naturale -> nell'esempio la traduzione da inglese ad italiano tramite encoder (trasforma la frase in vettori-embedding) e deconder con vari livelli di linar model e softmax per avere l'output
*Esempio:*
- *Input*: how are you?
- *Output*: come stai?
![[image-2.png]]
##### 3.3 Beam Search vs Greedy Search in NLP
**Greedy Search** -> semplicemente prende la parola piÃ¹ simile per ogni posizione
- Ogni parola Ã¨ considerata indipendente
- Funziona ma puÃ² essere migliorato
**Beam Search**
- Trova il path migliore e tiene in considerazione cosa ha giÃ  selezionato
- Prende $N$ migliori candidati per ogni posizione
- Espande gli attuali $N$ migliori candidati e prende i migliori $N$
- Nell'ultima posizione prende il miglior risultato tra gli $N$ candidati finali
##### Beam Search Decoder in NLP
Bisogna eseguire il decoder $N$ volte per ogni posizione:
- Costa tanto
- Ma il risultato spesso Ã¨ il migliore
![[image-3.png]]
## 3.3 Ambiente Non Deterministico

> Alcune azione hanno un risultato non deterministico

Belief State (Stato di Fede) -> un insieme di stati futuri per ogni stato corrente e coppia di azioni
- Non sai in anticipo dove ti porta l'azione
Conditional Plan -> la solzuione Ã¨ un albero (if-then-else chain) non una sequenza
- Puoi eseguire la soluzione e in base al risultato dell'ambiente scegliere l'azione corretta
- **AND-OR Search Trees** -> Ã¨ la soluzione per il conditional plan
	- **AND node** -> Fornito dall'ambiente insieme al set di tutti i possibili stati successivi
	- **OR node** -> Fornito dall'agente insieme all'azione da eseguire
### 3.3.1 Esempio Erratic Vacuum World
Non deterministic action
- Se lo stato corrente Ã¨ sporco, "suck" pulisce il quadrato corrispondente ma avvolte pulisce anche il quadrato adiacente
- Se lo stato corrente Ã¨ pulito, "suck" puÃ² depositare la sporcizia

"Left" e "Right" sono deterministiche

**Belief state**
- $Result(s,a)=?$
**Conditional plan**
- Â«suckÂ»; `if 5 do [Â«rightÂ», Â«suckÂ»]`
- Starting from state 1
- $Result(1, Â«suckÂ»)= \{5, 7\}$
> In questo modo se "suck" pulisce entrambi io avrÃ² che `if 5` non sarÃ  rispettato quindi faccio un altra azione
### 3.3.2 AND-OR Search Tree
- Necessario per evitare loop
- L'algoritmo Ã¨ leggermente piÃ¹ coprente ma l'albero si espande ancora
- Esiste una versione di $A^*$ per l'albero AND-OR

**Solzuione**
Un sotto-albero dove:
-  ogni foglia Ã¨ un goal
- Ha un azione per ogni nodo OR
- Include un ramo per tutti i risultati dei nodi AND
![[image-4.png|465x412]]

## 3.4 Searching Senza Osservazioni
- Non ci sono percezioni
- Senza sensori -> *Conformant Problem*

> L'agente deve stimale lo stato
> **Soluzione** -> Ã¨ una sequenza di azioni
> - Non puÃ² essere un conditional plan se non puoi ricevere un feedback dall'ambiente (non posso fare `if`)

**Belief Space** -> lo spazio delle rappresentazioni interne di ciÃ² che credo sia vero sul mondo (non certe)

Le azioni possono portare informazioni su possibili stati futuri
- *belief state* $= \{1,2,3,4,5,6,7,8\}$, *action*$=Â«rightÂ»$, *next belief state* $= ?$
- $\{2, 4, 6, 8\}$
#### Search in Conformant Problem
> Idea -> cercare nel belief space (completamente osservabile) invece che nello spazio degli stati (non osservabile)

- Belief state space = powerset of states -> 2ğ‘› possible beliefs instead of N
- Initial belief state = all the N states
- Le azioni *Actions(B)* che posso fare sono limitate dall'unione e l'intersezione di tutte le azioni *Actions(S)* pr ogni $S$ in $B$
- Transition model (for deterministic actions): Quando esegui unâ€™azione, non vai in un singolo stato, ma in unÂ **nuovo belief state**.
- **Goal Test**
	- **Possibly achieve**Â il goal: se almeno uno degli stati nel belief Ã¨ un goal.
	- **Necessarily achieve**Â il goal: seÂ _tutti_Â gli stati del belief sono goal.
- **Action cost**: Se lâ€™azione costa in modo diverso nei vari stati, allora devi trovare un criterio
	- Questo serve per decidere quanto â€œvaleâ€ unâ€™azione in un belief state.

##### Check Visited States
###### Come controllare gli stati visitati?
- Esempio:
	- ğµ1 = {5, 7}
	- ğµ2 = {1, 3, 5, 7}
- PuoiÂ **potare**Â (cioÃ¨ scartare) i rami di ğµ2, perchÃ© qualsiasi soluzione valida per $\{1, 3, 5, 7\}$ sarÃ  anche una soluzione per $\{5, 7\}$.
ğŸ‘‰ In generale: puoi scartare i rami che appartengono aÂ **superset**Â di belief state giÃ  visitati.
###### Ragionamento inverso
- Se hai giÃ  risolto il problema per un certo belief state, allora lo hai risolto anche per qualsiasiÂ **sottoinsieme**Â di esso.
###### ComplessitÃ 
- Nonostante queste ottimizzazioni, la ricerca nei problemi conformant (cioÃ¨ senza osservazioni, dove lâ€™agente non sa esattamente dove si trova) rimane molto costosa, perchÃ© lo spazio dei belief state Ã¨ enorme.
###### Ricerca incrementale nei belief state
1. Trova una soluzione che funziona per ilÂ **primo stato**Â in B.
2. Controlla se quella soluzione funziona anche per gli altri stati in B.
    - Se sÃ¬ â†’ fermati.
    - Se no â†’ riprova.
3. **Fallire velocemente**Â (cioÃ¨ accorgersi presto se una soluzione non funziona) puÃ² migliorare la velocitÃ  di convergenza
# 4 CSP - Problema di Soddisfazione del Vincolo
> **C**onstraint **S**atisfaction  **P**roblem

Introduciamo la rappresentazione degli stati come rappresentazione fattoriale
- Stato -> $X=\{X_1,\ X_2,\ ...,\ X_n\}$ con $n$ variabili
- Dominio -> $X_i\in D_i$ 
- La **conoscenza del dominio** Ã¨ espressa con un set di **vincoli C**
	- Es. $X_1\le 0,\ \ X_2\in [1,3], ...$ 


> [!info] $CSP(X, D, C)$
> **Goal Test** -> Assegno valori alle variabili per soddisfare i vincoli
> - **Completo vs Assegnazione Parziale** -> tutte o solo una parte delle variabili hanno un valore assegnato
> - **Assegnamento Consistente** -> Assegnare variabili soddisfa le condizioni
> - **Soluzione Parziale** -> un assegnamento parziale che Ã¨ consistente
> 
> Obbiettivo CSP Ã¨ trovare una soluzione completa e consistente


> [!important]  Factored Representation
> State Rappresentation -> Qui rappresentiÂ **ogni stato possibile come unâ€™unica entitÃ  indivisibile**Â (un â€œpuntoâ€ nello spazio degli stati).
>  
> Factored Representation -> Qui inveceÂ **uno stato non Ã¨ rappresentato come un singolo nodo â€œpiattoâ€**, ma comeÂ **una tupla di valori di variabili**:
> $$\text{Stato }S=(X_1â€‹=v_1â€‹,\ X_2â€‹=v_2â€‹,\ â€¦,\ X_nâ€‹=v_nâ€‹)$$
> Quindi:
> - Invece di rappresentareÂ **esplicitamente tutti gli stati**,
> - **rappresenti le variabili e i vincoli**Â che definiscono implicitamente lo spazio degli stati.


### 4.1.1 Esempio - Map Coloring
*Obbiettivo* -> colorare gli stati adiacenti con un colore diverso
![[image-9.png|274x226]]
Quindi abbiamo:
- $X = \{WA,\ NT,\ SA,\ Q,\ NSW,\ V,\ T\}$
- $D ={r,\ g,\ b}\ \ \forallğ‘‹_ğ‘–$
- $C =$
	- $WA â‰  ğ‘ğ‘‡$
	- $ğ‘Šğ´,\ ğ‘ğ‘‡ âˆˆ { ğ‘Ÿ,\ ğ‘” ,\ ğ‘Ÿ,\ ğ‘ ,\ ğ‘”,\ ğ‘Ÿ ,\ ğ‘”,\ ğ‘,\ â€¦ }$
	- Entrambi -> ripetere per ogni coppia di stati

![[image-10.png|274x242]]
> Le variabili sono i nodi e gli archi sono i vincoli
> Il grafico aiuta la ricerca, come possiamo vedere dal nodo "Tasmania" che non ha vincoli quindi possiamo assegnarli un colore casuale

| **Dominio**                    | **Vincoli**    | **RisolvibilitÃ **                                         |
| ------------------------------ | -------------- | --------------------------------------------------------- |
| Discreto finito                | Qualsiasi      | Decidibile (es. backtracking, CSP classici)               |
| Discreto infinito              | Lineari o meno | Difficile â†’ servono metodi speciali o bounds              |
| Discreto + vincoli lineari     | Lineari        | Solvibile (anche se NP-hard)                              |
| Discreto + vincoli non lineari | Non lineari    | **Indecidibile in generale**                              |
| Continuo + vincoli lineari     | Lineari        | **Solvibile in modo efficiente**Â (programmazione lineare) |
| Continuo + vincoli non lineari | Non lineari    | PiÃ¹ complesso; decidibilitÃ  dipende dal tipo di vincolo   |
## 4.2 Tipi di Vincoli
- **Unitario**
	- Una variabile coinvolta -> $A\ne 1$
- **Binario**
	- Due variabili coinvolte -> $A\le B$
- **Globale** -> vincoli con $n$ variabili coinvolte
	- Non necessariamente tutte le variabili
	- Alldif$(A,\ B,\ C,\ D)$
- **Vincoli di Preferenza**
	- Vincoli leggeri che rappresentano le preferenze
	- Modellato con un costo associato a ciascun incarico -> problema di ottimizzazione vincolato
	- Es. Linear Model

### 4.2.1 Vincoli Hyper-Graph
Non posso rappresentare i vincoli come archi dato che per definizione gli archi connettono solo due nodi
![[image-11.png]]
- **Nodi:**Â I cerchi (variabili lettera:Â F,T,U,W,R,O) e i quadrati (variabili di riporto:Â C1â€‹,C2â€‹,C3â€‹).
- **Archi:**Â Le linee che collegano i nodi rappresentano le relazioni o i vincoli tra le variabili definite dalle equazioni.

> Per vincoli con dominio finito, puoi sempre ridurre un hyper-graph in un grafico normale
> - vincoli globali possono essere divisi in un set di vincoli binari

Qualche vincolo popolare (es. Alldiff) gode di algoritmi studiati appositamente
- Non serve ridurre il grafo a vincoli binari
- Sono piÃ¹ facili da leggere da una prospettiva umana

## 4.3 Come Cercare in CSP
**Gli stati sono definiti dai valori assegnati finora**
*Initial State - stato iniziale:*
- Assegnazione vuota -> $\{\}$
- Fattore di ramificazione alla radice -> $=nd$

*Azioni:*
- Assegna un valore alle variabili non assegnate che non causa un conflitto con l'assegnamento corrente
- Fattore di ramificazione al secondo livello -> $=(n-1)d$

*Ogni soluzione completa appare alla profonditÃ  $n$:*
- Problema -> $n!$ o $d^n$ foglie ([[#^6f1c56|Riduce ComplessitÃ ]])
- Risoluzione -> Sfrutta la struttura del problema 

*Assegnamenti illegali* -> ritornano fallimento - non risolvibile
*Goal Test* -> controlla se l'assegnamento attuale Ã¨ completo

### 4.3.1 Backtracking Search

> Fissa un ordine tra gli stati -> rimuove la complessitÃ  $n!$
> Ossia -> Non câ€™Ã¨ bisogno di considerare tutti gli ordini delle variabili: possiamoÂ **fissare a priori un unico ordine**, e assegnare le variabili sempre in quellâ€™ordine.


> [!attention] Assegnazione Stati vs Singolo Nodo
> Se tu considerassiÂ **ogni stato come una possibile sequenza parziale di assegnamenti senza un ordine fisso tra le variabili**, allora:
> - Devi esplorareÂ **tutte le possibili permutazioni delle variabili**
> - Ci sonoÂ $n!$Â possibili ordini diversi in cui potresti assegnarle.
> 
> Invece di avereÂ **ogni nodo come â€œuna configurazione interaâ€ di n variabili**, ogni nodo nella ricerca corrisponde aÂ **â€œassegnare un valore a una singola variabileâ€**, secondo lâ€™ordine prefissato. ^6f1c56

**Ogni nodo Ã¨ considerato un singolo assegnamento** -> non l'intero stato
- solamente $d^n$ foglie 
- Costruisco un albero con ogni nodo che Ã¨ un assegnazione di una singola variabile non di un assegnazione di piÃ¹ variabili

Algoritmo non informato per CSP
- Depth-first co assegnamento a variabile singola
- La soluzione Ã¨ ancora un cammino nell'albero


> [!seealso] Logica Algoritmo
> Sceglie ripetutamente una variabile non assegnata, e poi prova tutti i valori nel dominio di quella variabile a turno, cercando di estendere ciascuno in una soluzione tramite una chiamata ricorsiva. Se la chiamata ha successo, la soluzione viene restituita e, se fallisce, l'assegnazione viene ripristinata allo stato precedente e proviamo il valore successivo. Se nessun valore funziona, restituiamo il fallimento.

![[image-12.png|536x438]]
- Con stati atomici, gli algoritmi non informati non sfruttano l'euristica
- Nella rappresentazione fattorizzata esistono euristiche indipendenti dal dominio
	- Possono migliorare la velocitÃ  di ricerca

## 4.4 Miglioramenti 
- [[#4.4.2 Miglioramenti Tramite Inferenza|4.4.2 Miglioramenti Tramite Inferenza]]
### 4.4.1 Miglioramenti di Ricerca
- [[#MEVs - Minimum Remaining Values|MEVs - Minimum Remaining Values]]
- [[#Degree Heuristic - Gradi di Euristica|Degree Heuristic - Gradi di Euristica]]
- [[#Least-Constraint Values|Least-Constraint Values]]

#### MEVs - Minimum Remaining Values
> Scegli la variabile con meno valori legali -> utilizza quella per avere un approccio "**prima fallimenti**" cosÃ¬ da fare una specie di **pruning** sui nodi con meno speranze cosÃ¬ da eliminarli subito

#### Degree Heuristic - Gradi di Euristica
>LaÂ **degree heuristic**Â sceglie, tra le variabili non ancora assegnate,Â **quella che Ã¨ coinvolta nel maggior numero di vincoli con altre variabili non assegnate**, cosÃ¬ da massimizzare la riduzione futura del branching.  
>Serve spesso comeÂ **criterio di spareggio**Â quando piÃ¹ variabili hanno lo stesso numero di valori legali (dopo la MRV).

#### Least-Constraint Values
> Questa Ã¨ la descrizione dellâ€™**euristica LCV (Least Constraining Value)**Â ğŸ‘¨â€ğŸ«
> - **Idea**: quando scegli il valore per una variabile, assegnaÂ **quello che elimina il minor numero di valori possibili per le altre variabili ancora non assegnate**, cosÃ¬ daÂ **lasciare piÃ¹ libertÃ  futura**.
> - **Differenza con MRV**:
> 	- **MRV (Minimum Remaining Values)**Â sceglieÂ _quale variabile_Â assegnare â†’ cerca di fallire presto per potare lâ€™albero.
> 	- **LCV (Least Constraining Value)**Â sceglieÂ _quale valore_Â assegnare â†’ cerca diÂ **non restringere troppo**Â il resto del problema, per non imboccare precocemente un vicolo cieco.
> 
> ğŸ‘‰ Entrambe accelerano la ricerca, maÂ **da prospettive opposte**: MRV anticipa i fallimenti, LCV mantiene aperte le possibilitÃ .

![[image-13.png]]

### 4.4.2 Miglioramenti Tramite Inferenza
- [[#Forward checking|Forward checking]]
- [[#Arc Consistency|Arc Consistency]]

#### Forward checking
Assegna un valore aÂ $X$  
Per ogni vincolo traÂ $X$Â eÂ $Y$:  
â†’Â **elimina**Â dal dominio diÂ $Y$Â tutti i valori che violano il vincolo con $X$.  
Se il dominio di Y rimane vuoto â†’Â **backtrack!**
- Significa che lâ€™assegnamento precedente era errato.
- Esistono molte strategie di backtracking

ğŸ‘‰ Questo Ã¨ un esempio diÂ **inferenza nei CSP**:
- Non si fa ricerca, siÂ **riduce lo spazio delle possibili assegnazioni**.
- Lâ€™inferenza puÃ² essere eseguitaÂ **prima**Â eÂ **durante**Â la ricerca.
	- Ad esempio,Â **prima della ricerca**Â si puÃ² imporre laÂ **coerenza di stato**Â eliminando i valori di dominio che violano vincoliÂ **unari**.

![[image-14.png]]

#### Arc Consistency
**coerenza dâ€™arco**
- UnaÂ **variabile X Ã¨ arc-consistent**Â rispetto a unâ€™altra variabile $Y$ se:  
$$\forall x \in \text{dom}(X), \ \exists y \in \text{dom}(Y) \ \text{tale che} \ (x,y) \ \text{rispetta il vincolo tra X e Y}.$$

ğŸ‘‰ In altre parole:Â **ogni valore nel dominio di X deve avere almeno un valore compatibile nel dominio di Y**.   
- UnÂ **grafo Ã¨ arc-consistent**Â seÂ **tutte le variabili sono arc-consistent rispetto a tutte le altre**Â (cioÃ¨ per ogni arco Xâ†’Y vale la condizione sopra).

> [!tip] Grafo Arc-Consistency
> - **Per ogni valore possibile di $X$**, deve esserciÂ **almeno un valore compatibile in $Y$**.
> - Se esiste un valoreÂ $x$Â che non ha nessunÂ $y$Â compatibile, allora quelÂ $x$Â va eliminato dal dominio diÂ $X$.
> 
> Quando elimini un valoreÂ xxÂ dalÂ **dominio di una variabile X**Â tramite AC-3 o altra inferenza
> - StaiÂ **riducendo lo spazio di ricerca**, perchÃ© quellâ€™assegnamentoÂ **non verrÃ  mai esplorato**Â durante il backtracking.
> - QuestoÂ **non elimina variabili**, elimina solo possibili valori per ciascuna variabile.

##### âš™ï¸Â Come rendere un grafo arc-consistent: algoritmo AC-3
Lâ€™algoritmoÂ **AC-3**Â Ã¨ un procedimento di inferenza cheÂ **rende il CSP arc-consistente**, oppure segnala fallimento se qualche dominio si svuota.

**Passi principali:**
1. **Inizializza una coda**Â con tutti gli archi (X â†’ Y) del CSP.
2. **Ripeti finchÃ© ci sono cambiamenti**:
    - Estrai un arco X â†’ Y dalla coda.
    - **Rendi X arc-consistent rispetto a Y**:
        - Elimina dal dominio di X tutti i valori x per cuiÂ **non esiste nessun y nel dominio di Y compatibile**con x.
    - Se il dominio di X Ã¨ stato modificato
        - Aggiungi alla codaÂ **tutti gli archi K â†’ X**, cioÃ¨ quelli che puntano verso X, perchÃ© il cambiamento potrebbe renderli incoerenti.
    - Se il dominio di una variabile si svuota â†’Â **fallimento**Â (il CSP non ha soluzione).
##### â±ï¸Â ComplessitÃ 
Per un CSP con:
-  **c**Â = numero di archi binari
- **d**Â = dimensione massima dei domini
ğŸ‘‰ La complessitÃ  Ã¨Â **O(c Â· dÂ³)**

- **Ogni arco**Â puÃ² essere controllato inÂ **O(dÂ²)**Â (perchÃ© bisogna confrontare ogni valore di X con ogni valore di Y).
- Ogni arco puÃ² essere reinserito nella coda al massimoÂ **d volte**, perchÃ© ogni variabile puÃ² perdere al massimo d valori â†’ da quiÂ **O(c Â· dÂ³)**.

##### ğŸ“Â Riassunto finale
- **Arc-consistency**Â = ogni valore di ogni variabile ha un supporto compatibile nei vicini.
- **AC-3**Â = algoritmo di inferenza che:
    - non fa ricerca,
    - riduce i domini eliminando valori impossibili,
    - puÃ² essere eseguito prima o durante la ricerca per potare fortemente lo spazio
- Se dopo AC-3 qualche dominio Ã¨ vuoto â†’Â **il CSP non Ã¨ risolvibile**.

#### Maintaining Arc Consistency
MAC (Maintaining Arc-Consistency)Â **estende il Forward Checking**Â chiamando AC-3 dopo ogni assegnamento durante la ricerca.
- Dato un assegnamento a ($X$), si chiama AC-3 con una coda contenente tutti gli archi ($Y \to X$), per tutte le variabili ($Y$) ancora non assegnate.
- Se AC-3 fallisce â†’Â **backtrack**.

MACÂ **potatura piÃ¹ aggressiva rispetto al Forward Checking**, perchÃ© verifica ricorsivamente eventuali incoerenze.

## 4.5 Local Search for CSP
Formulazione aÂ **stato completo**: ogni stato assegna un valore aÂ **tutte le variabili**.
- La ricerca cambia il valore diÂ **una variabile alla volta**.
    - Quindi, lo stato iniziale Ã¨ unÂ **assegnamento casuale di tutte le variabili**.

**Euristica Min-Conflicts**: scegli il valore cheÂ **violerebbe il minor numero di vincoli**, cosÃ¬ da avvicinarti alla soluzione.
- Ãˆ possibileÂ **pesare diversamente i vincoli**Â per dare prioritÃ  a quelli piÃ¹ importanti,
    - perchÃ© la soluzione finale potrebbe non essere ottimale rispetto a tutti i vincoli.
### 4.5.1 Min-Conflict Heuristic
**Euristica Min-Conflicts**: scegli il valore cheÂ **violerebbe il minor numero di vincoli**.
- Funziona molto bene,Â **tranne in una regione critica**.
    - Ad esempio, risolve il problema delleÂ **N-queens**Â in un numero di passi quasi costante, indipendentemente daÂ $N$Â (circa 50 passi).
### 4.5.2 Sfruttiamo la Struttura del Problema
Anche avendoÂ **un arsenale di milioni di trucchi**Â (e ce ne sono molti altri!), iÂ **CSP rimangono comunque molto difficili in generale**.
- Caso peggiore: ( $d^n$ ) foglie nellâ€™albero di ricerca (come abbiamo visto).
#### Scomposizione in sottoproblemi indipendenti
- Alcuni problemi possono essereÂ **divisi in sottoproblemi indipendenti**, ad esempio â€œTâ€ e â€œmainlandâ€.
- GliÂ **algoritmi sui grafi**Â possono identificare questeÂ **sottostrutture nel grafo dei vincoli**, come leÂ **componenti connesse**.
- LeÂ **componenti connesse**Â sono indipendenti tra loro, quindi laÂ **soluzione complessiva**Â del CSP Ã¨ semplicementeÂ **lâ€™unione delle soluzioni di ciascuna componente**.
#### âš¡ Vantaggio computazionale

- Se una componente ha $c < n$ variabili, allora la ricerca su quella componente scala come $d^c$ , invece di $d^n$.
- In altre parole,Â **scomporre il problema in sottoproblemi riduce drasticamente la complessitÃ **, sfruttando lâ€™indipendenza tra le variabili.

### 4.5.3 Tree-structured CSPs
Se ilÂ **grafo dei vincoli Ã¨ un albero**, risolvere il CSP costa $O(n d^2)$
- Lineare in $n!$

Dato unÂ **grafo dei vincoli senza cicli**Â (un albero), lâ€™algoritmo di ricerca procede cosÃ¬:
1. **Ordinamento topologico**: scegli una qualsiasi variabile come radice e ordina le variabili in modo che ciascuna sia figlia della propria variabile genitore.
2. **Rendi lâ€™albero arc-consistente**; se fallisce â†’ termina con fallimento.
    - Ci sono ( n-1 ) archi e ( d^2 ) combinazioni di valori da controllare per ciascun arco.
3. **Assegna a ogni nodo**Â un qualsiasi valore consistente con quello del genitore; se non esiste â†’ fallimento

> In questo casoÂ **non serve mai fare backtracking**.

![[image-15.png]]

### 4.5.4 ğŸŒ³ â€œPiantare alberi dove non ce ne sonoâ€

- Possiamo risolvereÂ **rapidamente CSP strutturati ad albero**.
- Ma cosa succede se il CSPÂ **non Ã¨ un albero**? Possiamo provare aÂ **forzarlo a diventarlo**.
#### Procedura per trasformare un CSP in albero
1. **Assegna un valore a una variabile**Â (scegli un nodo).
2. **Rendi gli archi coerenti**Â (arc-consistency).
3. **Rimuovi la variabile assegnata**

- Se il grafo risultanteÂ **puÃ² essere trasformato in un albero**, allora puoiÂ **verificare rapidamente se il CSP Ã¨ risolvibile**.
- Altrimenti, puoi provareÂ **un altro valore per la variabile**Â oÂ **scegliere un altro nodo da assegnare**.
#### âš¡ Cutset Conditioning

- Supponiamo di dover provare ( c ) variabili (ilÂ **cutset**), cioÃ¨ quelle che â€œrompono i cicliâ€ del grafo
- ComplessitÃ  approssimativa:  
$$
    O(d^c (n-c) d^2)  
$$
- Qui:
    - $d^c$ = tutte le combinazioni di valori per le variabili nel cutset
    - $n-c$ = numero di variabili rimanenti che ora formano un albero
    - $d^2$ = costo per verificare la coerenza sugli archi rimanenti

- In pratica, riduce drasticamente il problema, perchÃ©Â **la parte ciclica viene gestita separatamente**, mentre il resto Ã¨ un albero â†’ risolvibile in tempo lineare.
![[image-16.png]]
## 4.6 Riassunto

### 4.6.1 I. Fondamenti dei Constraint Satisfaction Problems (CSP)

#### 5.1.1 Definizione e Struttura dello Stato

- **Stato Fattorizzato:**Â Lo stato $X$ Ã¨ rappresentato da un insieme di $n$ variabili ${X_1, X_2, \dots, X_n}$.
- **Dominio ($D_i$):**Â Ogni variabile $X_i$ assume valori da un dominio $D_i$.
- **Vincoli ($C$):**Â La conoscenza del dominio Ã¨ espressa da un insieme di vincoli che devono essere soddisfatti (es. $X_1 \le 0$, $X_2 \in {1, 3}$).
- **Obiettivo del CSP:**Â Trovare unaÂ **soluzione completa e consistente**.
    - **Assegnazione Completa:**Â Tutte le variabili hanno un valore assegnato.
    - **Assegnazione Parziale:**Â Solo alcune variabili hanno un valore assegnato.
    - **Assegnazione Consistente:**Â Le variabili assegnate soddisfano i vincoli.
    - **Soluzione Parziale:**Â Un'assegnazione parziale che Ã¨ consistente.

#### 4.5.2 Rappresentazione dei Vincoli

- **Grafo dei Vincoli:**Â Le variabili sono rappresentate comeÂ **nodi**Â e i vincoli sono rappresentati comeÂ **archi**. La struttura del grafo puÃ² aiutare la ricerca.
- **Vincoli Comuni:**
    - **Vincoli Unari:**Â Coinvolgono una sola variabile (es. $A \ne 1$).
    - **Vincoli Binari:**Â Coinvolgono due variabili (es. $A \le B$).
    - **Vincoli Globali:**Â Coinvolgono un numero qualsiasi di variabili (non necessariamente tutte), es. $Alldiff(A, B, C, D)$.
- **Iper-Grafo dei Vincoli:**Â Necessario quando ci sono vincoli globali, poichÃ© un arco per definizione connette solo due nodi.
    - **Riduzione:**Â Per i vincoli con dominio finito, un iper-grafo puÃ² sempre essere ridotto a un grafo normale suddividendo i vincoli globali in un insieme di vincoli binari. Questo Ã¨ utile se si vogliono usare algoritmi per soli vincoli binari.
- **Vincoli di Preferenza (Soft Constraints):**Â Rappresentano una preferenza (es. se possibile, assegna questo rispetto a quello). Sono modellati tramite un costo associato a ciascuna assegnazione (diventando un problema di ottimizzazione vincolata).

### 4.6.2 II. Tecniche di Ricerca di Soluzioni
- [[#A. Framework di Ricerca (Stato Fattorizzato)|A. Framework di Ricerca (Stato Fattorizzato)]]
- [[#B. Ricerca Non Informata: Backtracking Search|B. Ricerca Non Informata: Backtracking Search]]
- [[#C. Miglioramenti tramite Euristiche (Domain-Independent)|C. Miglioramenti tramite Euristiche (Domain-Independent)]]
- [[#D. Ricerca Locale (Local Search)|D. Ricerca Locale (Local Search)]]
#### A. Framework di Ricerca (Stato Fattorizzato)

- **Stati:**Â Definiti dai valori assegnati finora.
- **Stato Iniziale:**Â L'assegnazione vuota ${\ }$.
- **Azioni:**Â Assegnare un valore a una variabile non assegnata che non Ã¨ in conflitto con l'assegnazione corrente.
- **ProfonditÃ  della Soluzione:**Â Ogni soluzione completa appare alla profonditÃ  $n$.
- **Problema di ComplessitÃ :**Â Il numero di foglie (senza ottimizzazioni) Ã¨ $n! d^n$.

#### B. Ricerca Non Informata: Backtracking Search

- **Algoritmo:**Â Backtracking Search Ã¨ essenzialmente una ricerca in profonditÃ  (Depth-First Search) con assegnazione a variabile singola.
- **Fissare l'Ordine:**Â Fissando un ordine per le variabili si elimina la complessitÃ  $n!$, riducendo il numero di foglie a $d^n$.
- **Processo:**Â Sceglie ripetutamente una variabile non assegnata, prova tutti i valori nel suo dominio, ed estende ricorsivamente l'assegnazione. Se la chiamata fallisce, l'assegnazione viene ripristinata e si prova il valore successivo. Se nessun valore funziona, restituisce fallimento.

#### C. Miglioramenti tramite Euristiche (Domain-Independent)

Nello stato fattorizzato esistono euristiche indipendenti dal dominio che velocizzano la ricerca.

- **1. Minimum Remaining Values (MRVs):**
    
    - **Scelta:**Â Scegliere la variabile con ilÂ **minor numero di valori legali**Â (possibili).
    - **Logica:**Â Cerca il fallimento il prima possibile (_Failure-First_) in modo che la potatura (_pruning_) avvenga prima.
- **2. Degree Heuristic:**
    
    - **Scelta:**Â Scegliere la variabile con ilÂ **maggior numero di vincoli**Â tra le variabili ancora da assegnare.
    - **Uso:**Â Serve comeÂ _tie-breaker_Â per variabili con lo stesso numero di mosse legali (stesso MRV).
- **3. Least-Constraining Values (LCVs):**
    
    - **Scelta:**Â Scegliere un valore che escluda ilÂ **minor numero di valori**Â nelle altre variabili ancora da assegnare.
    - **Logica:**Â Mantiene aperte le opzioni, cercando di evitare percorsi stretti che potrebbero essere sbagliati.

#### D. Ricerca Locale (Local Search)

- **Formulazione a Stato Completo:**Â Ogni stato assegna un valore aÂ _ogni_Â variabile.
- **Inizio:**Â Lo stato iniziale Ã¨ un'assegnazione casuale di tutte le variabili.
- **Azione:**Â La ricerca cambia il valore di una sola variabile alla volta.
- **Heuristica Min-Conflicts:**
    - **Scelta:**Â Scegliere il valore cheÂ **rompe il minor numero di vincoli**.
    - **Obiettivo:**Â Avvicina lo stato alla soluzione.
    - **Efficacia:**Â Funziona molto bene; ad esempio, risolve il problema delle N-regine in un numero costante di passi, indipendentemente da $N$.

### 4.6.3 III. Tecniche di Inferenza e Propagazione dei Vincoli

L'inferenza non Ã¨ una ricerca, ma riduce lo spazio di possibili assegnazioni. PuÃ² essere eseguita prima o durante la ricerca.

#### Forward Checking (FC)

- **Funzionamento:**Â Dopo aver assegnato un valore a una variabile $X$, per ogni vincolo $X-Y$, siÂ **eliminano i valori dal dominio di $Y$**Â che violano il vincolo.
- **Azione in caso di fallimento:**Â Se non rimangono valori in un dominio, si esegue ilÂ _backtrack_, poichÃ© l'assegnazione precedente era sbagliata.

#### Arc Consistency (AC)

- **Definizione:**Â Una variabile $X$ Ã¨Â **arc-consistent**Â se per ogni arco $X \to Y$, per ogni valore $x$ nel dominio di $X$, esiste almeno un valore $y$ nel dominio di $Y$ che Ã¨ permesso.
- **Grafo Arc-Consistent:**Â Un grafo Ã¨ arc-consistent se tutte le variabili sono arc-consistent.

#### Algoritmo AC-3

- **Scopo:**Â Rende un grafo arc-consistent o restituisce fallimento.
- **Processo:**
    1. Creare una coda con tutti gli archi.
    2. Ripetere finchÃ© non ci sono piÃ¹ cambiamenti:
        - Selezionare un arco casuale $X \to Y$ e rendere $X$ arc-consistent,Â **riducendo il dominio di $X$**.
        - Se viene rimosso un valore da $X$,Â **aggiungere tutti gli archi $K \to X$ alla coda**Â (perchÃ© $K$ potrebbe non essere piÃ¹ consistente rispetto al nuovo $X$).
        - Se una variabile rimane senza assegnazioni possibili, fallire.
- **ComplessitÃ :**Â $O(cd^3)$, dove $c$ Ã¨ il numero di archi binari e $d$ Ã¨ la dimensione del dominio.

#### Maintaining Arc Consistency (MAC)

- **Funzionamento:**Â MAC aumenta ilÂ _forward checking_Â chiamando AC-3 dopo ogni assegnazione durante la ricerca.
- **Invocazione:**Â Data un'assegnazione a $X$, MAC chiama AC-3 con una coda di archi $Y \to X$ (per tutte le $Y$ non ancora assegnate).
- **Potatura:**Â MAC pota piÃ¹ delÂ _forward checking_Â perchÃ© verifica ricorsivamente le incoerenze.

### 4.6.4 IV. Sfruttare la Struttura del Problema (Semplificazione)

Anche con molte euristiche, i CSP rimangono difficili in generale (worst-case $d^n$). Sfruttare la struttura puÃ² portare a grandi semplificazioni.

#### Componenti Connesse Indipendenti

- **Identificazione:**Â Gli algoritmi sui grafi possono identificare sottostrutture (componenti connesse) nel grafo dei vincoli.
- **Vantaggio:**Â I componenti connesse sono sottoproblemi indipendenti.
- **Soluzione:**Â La soluzione Ã¨ l'unione delle soluzioni dei componenti.
- **ScalabilitÃ :**Â La complessitÃ  scala come $d^c$, dove $c < n$ sono le variabili coinvolte nel sottoproblema.

#### CSPs a Struttura ad Albero (Tree-Structured CSPs)

- **Definizione:**Â Se il grafo dei vincoli Ã¨ un albero (senza cicli), il CSP Ã¨ notevolmente piÃ¹ facile.
- **Costo:**Â La risoluzione costa $O(nd^2)$, che Ã¨Â **lineare**Â in $n$ (numero di variabili).
- **Algoritmo (Nessun Backtrack):**
    1. **Ordinamento Topologico:**Â Scegliere una variabile come radice e ordinare le variabili in modo che ciascuna sia figlia del suo genitore.
    2. **Arc-Consistency:**Â Rendere l'albero arc-consistent (o fallire).
    3. **Assegnazione:**Â Assegnare a ogni nodo un valore consistente con il suo genitore (o fallire).

#### Cutset Conditioning (Piantare alberi dove non ci sono)

- **Scopo:**Â Forzare un CSP a diventare strutturato ad albero.
- **Processo:**Â Si assegna un valore a un sottoinsieme di variabili, chiamatoÂ **cutset**Â ($c$ variabili).
- **Vantaggio:**Â Se il grafo risultante (dopo aver rimosso e assegnato il cutset) puÃ² essere trasformato in un albero, si puÃ² verificare rapidamente se Ã¨ risolvibile.
- **ComplessitÃ :**Â $O(d^c (n-c) d^2)$. L'efficacia dipende dalla dimensione $c$ delÂ _cutset_.

# 5 Games
> **Competitive Game** -> Ricerca contraddittoria (*adversarial search*)
> - Giocatori fanno a turno
> - I giocatori hanno goal opposti, in contrasto
> - Somma zero -> uno vince l'altro perde
> - **Perfettamente informato -> fully observable**

- Stato iniziale -> $S_0$
- $\text{TO-MOVE}(s)$ -> Mossa per spostarsi nello stato $s$
- $\text{ACTIONS}(s)$ -> mosse legali per lo stato $s$
- $\text{RESULT}(s,\ a)$ -> dopo lo spostamento (transition model) il risultato Ã¨ il nuovo stato
- $\text{IS-TERMINAL}(s)$ -> il gioco Ã¨ finito o no
- $\text{UTILITY}(s;\ p)$ -> Funzione di utilitÃ  - payoff ottenuta su uno stato terminale $s$ dal giocatore $p$ -> ossia Ã¨ lo stato finale

![[image-17.png|559x174]]

## 5.1 Game Tree -> Min-Max Problem
> Stesse proprietÃ  della ricerca ad albero 
> - PuÃ² essere infinita se lo spazio degli stati Ã¨ infinito
> - PuÃ² essere infinito se lo spazio degli stati Ã¨ finito ma permette ripetizioni degli stati (posizioni)

Il grafico dello spazio degli stati ha i nodi che rappresentano gli stati e gli archi che rappresentano le possibili azioni
- Lo stesso stato puÃ² essere raggiunto da piÃ¹ direzioni - *path*
- The game tree Ã¨ *sovrapposto* al grafico -> nel gioco la radice Ã¨ la posizione di partenza

Non c'Ã¨ speranza di costruire l'intero game tree per qualsiasi gioco anche di dimensione ragionevole
-  $< 9!$ states for tictactoe $â‰ˆ 360 000$ states -> insostenibile
![[image-18.png|299x242]]

### 5.1.1 MIN - MAX
> La strategia per ogni giocatore Ã¨ un *piano condizionato* (**conditional plan**)
> - *Si puÃ² applicare a qualsiasi gioco deterministico con informazione perfetta*
> - Cosa fa MAX se MIN fa qualcosa?


Se il gioco ha solo come risultato win/lose
- Allora abbiamo una ricerca in un ambiente non deterministico
- Le mosse dell'avversario sono modellate come una distribuzione di probabilitÃ 
- Win=goal, lose = not goal
- **AND-OR albero di ricerca**

Se il risultato ha piÃ¹ valori -> **minmax**
- Ãˆ perfetto per giochi deterministici e con informazioni perfette

![[image-21.png|551x150]]
![[image-19.png|551x306]]

#### Minmax Value
Una volta ottenuto il valore minimo massimo per ogni nodo, puoi recuperare la strategia ottimale

Nelle foglie (nodi terminali) -> $\text{MINIMAX}(s) = \text{UTILITY}(s;\ p)$
- Il valore di utility -> ossia valore del risultato finale

Un nodo intermedio, non terminale -> $\text{MINIMAX}(s) =$ il valore dell'utility **per MAX**
- Ossia il valore **MINIMAX(s)** rappresenta il **valore di utilitÃ  garantito per MAX** se entrambi i giocatori (MAX e MIN) giocano in modo ottimale da quel nodo in poi.
- Ãˆ il miglior risultato che MAX puÃ² ottenere partendo daÂ $s$, considerando che MIN cercherÃ  di minimizzare il punteggio di MAX.

$$
\text{MINIMAX}(s)=\max_{ğ‘âˆˆ\text{ğ´ğ¶ğ‘‡ğ¼ğ‘‚ğ‘ğ‘†}(ğ‘ )}\ \text{MINIMAX}(\text{ğ‘…ğ¸ğ‘†ğ‘ˆğ¿ğ‘‡}(ğ‘ ,\ ğ‘ )) ,\ \text{TO-MOVE}(s) = \text{MAX}
$$
$$
\text{MINIMAX}(s)=\max_{ğ‘âˆˆ\text{ğ´ğ¶ğ‘‡ğ¼ğ‘‚ğ‘ğ‘†}(ğ‘ )}\ \text{MINIMAX}(\text{ğ‘…ğ¸ğ‘†ğ‘ˆğ¿ğ‘‡}(ğ‘ ,\ ğ‘ )) ,\ \text{TO-MOVE}(s) = \text{MIN}
$$
- MAX sceglie l'opzione che massimizza MINIMAX
	- Il punteggio di MAX dipende dalle mosse di min infatti come vediamo nella figura nel turno di MAX (root) il valore Ã¨ 3 perchÃ¨ considera che min faccia la scelta migliore e quindi MAX deve scegliere tra 3 e 2 che sono i valori minimi quando tocca a MIN
- MIN sceglie l'opzione che minimizza MINIMAX
	- Dato che ha l'obbiettivo opposto

> Utilizzando l'algoritmo Minimax:
> 1. **Esegui una ricerca in profonditÃ  (depth-first search) dalla radice**Â per trovare tutti gli stati terminali (foglie)
> 2. **Ottieni l'utilitÃ  delle foglie**Â (i valori numerici associati ai nodi terminali).
> 3. **Ripercorri all'indietro (backtrack) per calcolare il valore Minimax di tutti i nodi intermedi**:
    - Se il nodo genitore Ã¨ un nodoÂ **MIN**, prendi il valoreÂ **minimo**Â tra i valori Minimax dei figli.
    - Se il nodo genitore Ã¨ un nodoÂ **MAX**, prendi il valoreÂ **massimo**Â tra i valori Minimax dei figli.

ES:
Immagina un gioco dove MAX e MIN alternano mosse:
- MAX parte dalla radice e sceglie traÂ $A_1$â€‹,Â $A_2$â€‹,Â $A_3$â€‹.
- Se sceglieÂ $A_1$â€‹, vede 3, 12, 8, ma sa che MIN minimizzerÃ  a 3.
- SceglieÂ $A_1$Â perchÃ© 3 Ã¨ il massimo tra 3, 2, e 2 (valori diÂ $A_1$â€‹,Â $A_2$â€‹,Â $A_3$â€‹)


> [!important] MinMax Search
> - **Completa** -> **SI** con stati finiti
> - **Ottimale** -> **SI**, contro un giocatore perfetto
> - **ComplessitÃ  in Tempo** -> $O(b^m)$
> - **ComplessitÃ  Spazio** -> $O(bm)$ = Depth-First

#### Giocare contro un Avversario Sub-Ottimale
> Minimax ha dei leggeri inconvenienti

A seconda del giocatore puÃ² fare mosse perfette oppure come nella maggior parte dei casi sbagliare e quindi non avere un gioco perfetto.
Se io gioco contro di lui e se la posizione Ã¨ un pareggio con un gioco perfetto, potresti voler fare una mossa complicata, ma non ottimale per provare a prendere la vittoria:
- Se rispondo con l'unica mossa corretta nella posizione: perdi (p=1%)
- Se rispondo con alcune delle mosse sbagliate: vinci (p=85%)
- Se rispondo con il resto delle mosse (non ottime, ne pessime): Ã¨ un pareggio (p=14%)

Questa Ã¨ la differenza tra le cosiddette "mosse del computer" e le "mosse umane"
- Ci sono programmi per computer che cercano di imitare il modo in cui gli esseri umani si comportano contro gli avversari non ottimali

##### ComplessitÃ 
- Branching factor in chess = 35
- Average game length = 40 moves = 80 ply
- Given the complexity of DFS, you get 3580 states to visit
> Per ridurlo abbiamo bisogno del **PRUNING**

### 5.1.2 $\alpha$ - $\beta$ pruning
> Pota i rami che non portano a nessuna mossa migliore nÃ© per MAX nÃ© per MIN

$\alpha$ = valore MINIMAX piÃ¹ grande finora
$\beta$ = valore MINIMAX piÃ¹ piccolo finora
$[\alpha,\ \beta]$
Iniziale = $[-\infty,\ +\infty]$

![[image-22.png]]

**Regola generale**: considera un nodo con MINIMAX = V. 
- Se un nodo della stessa profonditÃ  (MINIMAX=m') o un nodo piÃ¹ alto (MINIMAX=m) ha un valore MINIMAX migliore, non raggiungerai mai il nodo con MINIMAX=V: **puoi potarlo!**


> [!question] PerchÃ¨ Pruning?
> Non ha effetto sulla soluzione
> Con il migliore pruning possiamo ridurre la complessitÃ  a $O(b^{m/2})$
> - Buono ma non sufficiente per problemi con grandezza ragionevole ($\sqrt b$ fattore su cui ha effetto)
> - Il miglior pruning dipende **dall'ordine delle mosse**
> - La generazione di mosse che consentono di potare prima l'albero porta alla migliore potatura -> faccio mosso che permettono di potare piÃ¹ rami

La ricerca del raggio puÃ² essere utilizzata solo per considerare le mosse "top" k ogni volta, in base ad alcune funzioni di valutazione
- Rischi di potare la mossa migliore, ovviamente.

#### Strategie diverse da DFS
- **Type A** strategy
	- Esplorare completamente lo spazio della ricerca fino ad una profonditÃ  limitata
	- Usare un euristica per valutare utility per i nodi alla profonditÃ  finale
- **Type B** strategy
	- Non esplorare le mosse che sembrano pessime basandosi su una data euristica
	- Segui le mosse piÃ¹ promettenti dove possibile -> possibilmente fino alla fine del gioco

Type A funziona se il fattore di brenching (Ã¨ il numero massimo di figli che un nodo puÃ² avere in un albero) non Ã¨ molto largo, Type B Ã¨ utile quando il fattore di brenching Ã¨ molto alto.

Le euristiche vengono **apprese** o sfruttate come una combinazione ponderata di caratteristiche di gioco
**Effetto orizzonte**: l'euristica valuta la posizione come buona, ma esplorando un livello di profonditÃ  in piÃ¹ potrebbe cambiare completamente la valutazione.

### 5.1.3 MCTS - Monte-Carlo Tree Search
> Utile quando il fattore di brenching Ã¨ enorme, Anche con la potatura, saresti limitato a piccole profonditÃ 
> - Utile quando non hai una funzione di valutazione buona -> anche se puoi sempre aggiungerla se ce l'hai

**Rollout - Playout** (lanci)-> da un determinato stato, riproduci un intero episodio fino alla fine e ottieni l'utilitÃ .
In MCTS, il valore di uno stato Ã¨ l'**utilitÃ  finale media** su un certo numero di lanci da quello stato

**Playout Policy** -> funzione che decide quqle mossa prendere ad ogni step
- Questa Ã¨ la parte dove il reinforcement learning aiuta molto
- La politica di lancio non Ã¨ tutta la storia

**Pure Monte-Carlo Search** -> dato uno stato, calcolare il suo valore effettuando $N$ rollouts
- Nessun search tree necessario

#### Passaggi di MCTS (Monte Carlo Tree Search)

- **Registra la simulazione in un albero di ricerca**  
	- All'inizio, l'albero di ricerca contiene solo lo stato iniziale.

- Dato un albero di ricerca con uno stato come radice, ripeti:  
	- **Selezione**: Dalla radice, usa la politica di selezione per scegliere un percorso verso una foglia.  
	- **Espansione**: Usa la politica di simulazione per generare un nuovo nodo a partire dalla foglia corrente.  
	- **Simulazione**: Esegui una rollout (simulazione completa) dal nodo appena espanso usando la politica di simulazione. L'albero di ricerca non viene aggiornato.  
	- **Back-propagation (non quella!)**: Aggiorna il valore dei nodi nell'albero di ricerca dal nodo appena aggiunto fino alla radice, seguendo il percorso di selezione.  
	- **Ritorno**: Scegli la mossa con il numero piÃ¹ alto di simulazioni (vedi piÃ¹ avanti).  
	- **Esecuzione della mossa**: Esegui la mossa, ottieni il nuovo stato e ripeti i passaggi sopra.

#### Politica di selezione
- **Definizione**: Una funzione che determina quale mossa seguire in un dato albero di ricerca.  
- **Bilanciamento**: Concilia **sfruttamento** (simulazioni da stati buoni e ben esplorati) e **esplorazione** (simulazioni da stati meno frequentati).  

MCTS combina esplorazione casuale e strategia per migliorare le decisioni in giochi complessi, aggiornando l'albero solo con i risultati delle simulazioni.

![[image-23.png]]

#### Tempo di Esplorazione
$$
\sqrt{\frac{log_N(\text{Parent}(n))}{N(n)}}
$$
- $N(\text{Parent}(n))\ge N(n)$
	- Come ogni volta che visiti n visiti Parent(n)
- $log_N(\text{Parent}(n))\le N(n)$
	- Da un certo numero di playouts in avanti

Supponiamo che Parent$(n)$ riceve 10 playouts piÃ¹ di $n$
Il temine di esplorazione va a 0 quando il numero di playouts aumenta
- non vuoi esplorare per sempre

![[image-24.png|468x359]]

![[image-25.png]]

#### Deterministic Games
- MCTS non richiede un euristica
	- Puoi usarlo su tutti i giochi basta conoscere le regole 
	- Puoi usare un euristica se ne hai una (ad esempio per migliorare la politica)
- MCTS si basa sull'esplorazione, quindi potrebbe volerci del tempo per determinare il valore di una mossa ovviamente buona/cattiva.
	- L'euristica puÃ² aiutare qui
- MCTS con politica/euristica appresa raggiunge costantemente prestazioni sovrumane
	- Chess

Nota: gli approcci moderni combinano ricerca e apprendimento e altro ancora!
#### Stochastic Games
Nodo casuale che rappresenta tutti i possibili risultati (ad esempio, il risultato di un lancio di dadi)
![[image-26.png|461x352]]

#### Minimax Previsto
Lo stesso di minimax, ma devi gestire anche i nodi casuali
Il valore di un nodo casuale Ã¨ il valore atteso di tutti i possibili risultati del nodo

$$
\text{ExpectedMINIMAX}(s) =\sum_r P(r) \text{ğ¸ğ‘¥ğ‘ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘€ğ¼ğ‘ğ¼ğ‘€ğ´ğ‘‹}(\text{ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡}(ğ‘ ,\ ğ‘Ÿ))
$$
-Puoi applicare un ragionamento simile a giochi con informazioni parziali, sia deterministici (ad esempio, scacchi ciechi) che stocastici (ad esempio, poker). 
- **Sono preferibili aggiustamenti ad hoc**, ma non entreremo nel dettaglio.
---
# 6 Ottimizzazioni
#slide8
## 6.1 Artificial Life
La vita artificiale cerca di capire le proprietÃ  essenziali dei sistemi viventi sintetizzando un comportamento realistico in software, hardware e prodotti biochimici.
- **Soft Artificial Life** -> simulazioni digitali processi e organismi viventi
- Hard Artificial life -> implementazione hardware (soft robot). evoluzione dei controller e della morfologia ossia la struttura fisica
- Wet Artificial life -> sintetizza sistemi viventi delle sostanze biochimiche, ossia cellule artificiali
- **Impegno ingegneristico non necessario** -> non vogliamo necessariamente risolvere un problema con un certo margine di errore
- Vogliamo capire come le cose funzionano e scoprire nuove cose, se qualcosa diventa anche utile meglio.

### 6.1.1 Prioneria dell'Artificial Life
> La vita artificiale slega il vincolo materiale al carbonio permettendo di studiare i principi generali senza essere legati al caso specifico (carbonio) ma analizzando in generale il fenomeno

#### Origini
La vita Ã¨ un processo iterativo guidato dall'evoluzione -> avere infiniti loop Ã¨ potente
- condizione iniziale -> come la vita Ã¨ nata
- Evoluzione Biologica -> algoritmo
	- Come la vita progredisce e diventa varia
	- Come gli organismi moderni sono i discendenti dei loro antenati
#### Natural Selection
> Condizioni **necessarie e sufficienti** per l'evoluzione tramite natural selection

- Tasso di riproduzione diverso
	- ProprietÃ  dell'ambiente che favoriscono un un certo tratto
	- Sopravvivono gli individue che combaciano di piÃ¹ all'ambiente
- EreditarietÃ  -> tratti che vengono passati  alla nuova generazione
- Variazioni -> tratti differenti in una popolazione

> Tratti vantaggiosi sopravvivono in una popolazione mentre gli altri spariscono -> **convergono verso una popolazione omogenea** fino alla successiva mutazione e a quelle giÃ  presenti

Punti chiave dell'evoluzione:
- Adattamento -> una caratteristica Ã¨ favorita dalla selezione naturale diventa sempre piÃ¹ importante per un dato compito
- Esitazione -> trovare un uso diverso per un tratto prima utilizzato per un diverso uso.
- DiversitÃ :
	- Mutazioni casuali -> crea nuovi geni
	- Ricombinazione -> cross-over, mischiare i geni
	- Selezione naturale -> cambiata in base al fitness
		- Fitness -> rappresenta quanto Ã¨ buono ad essere trasmesso un genotipo comparato agli altri
		- Selezione sessuale e artificiale
	- Processi divergenti -> non collassa, ma crea novitÃ 
#### Condizioni Iniziali e Algoritmo di evoluzione
Bisogna approssimare le condizioni iniziali di ambiente e di popolazione composta da molti individui.
Un sistema caotico e complesso: ogni approssimazione porta a grandi differenze negli stati futuri.

**Problema di Ottimizzazione** -> partiamo giÃ  da un processo ben definito: evoluzione tramite selezione naturale.
Possiamo simulare le condizioni necessarie e sufficienti:
- variazione
- Riproduzione differenze
- EreditarietÃ 

### 6.1.2 Skeleton of EAs
- Inizializzazione della **popolazione** $P_0$ di **individui** con $N$ caratteristiche (features)
	- Le caratteristiche sono espresse in un dominio specifico di linguaggio (bit-string o numeri reali)
- **Valutazione** della **fitness** della popolazione $P_0$
- Per ogni step $t=1,1,...$
	- Selezione un genitore di $P_0$ in base alla fitness
	- **Riproduzione e ricombinazione** delle caratteristiche del **figlio**
	- Applica una mutazione delle caratteristiche (random o rate di mutazione di adattamento)
	- **Valuta** la fitness della nuova popolazione
	- Selezione un sottoinsieme di $K$ individui sopravvissuti prendendo $P_t$ (esempi casuali pesati dalla fitness)

#### EAs -> Dettagli e Variazioni
- Basandosi sulle regioni dove la fitness Ã¨ alta -> la sfruttiamo
	- Hill-climbing nello spazio della fitness
- Seleziona i genitori basandosi solo sul rank relativo della fitness
- Il numero di figli Ã¨ proporzionale alla differenza tra la media e la fitness individuale -> *ossia se ha una fitness alta avrÃ  piÃ¹ figli*
- **Rate di mutazioni diversi** per ogni caratteristica individuale
- La ricombinazione puÃ² mediare le caratteristiche o prendere il massimo/minimo
	- La ricombinazione cross-over inserisce un punto di taglio e cambia le caratteristiche

### 6.1.3 Diversi Tipi di ALGORITMI Evolutivi
**Strategie di Evoluzione -> Basate sulla distribuzione**
- Mutazioni random + valutazione + ricombinazione
- Programmazione Evolutiva -> mutazione random + valutazione

**Algoritmi Genetici -> Basati sulla popolazione**
- Come input abbiamo i genotipi codificati come stringhe di bit
- Le mutazioni sono rare e la ricombinazione Ã¨ il fattore chiave
- La fitness Ã¨ rilevante anche per selzionare i genitori

**Programmazione Genetica**
- l'individuo Ã¨ un set di istruzioni -> albero
- Algoritmi genetici per codice genetico

### 6.1.4 Genetic Programming
Fenomeni tipici:
1. Fixing Sorting -> l'AI invece di fare un ordinamento complesso sceglie la strada semplice ossia cancellare tutti gli elementi cosÃ¬ la lista Ã¨ sicuramente ordinata, questo perÃ² rende il codice inutile -> controllo: deve contenere gli stessi elementi di input
2. L'evoluzione puÃ² portare a cheat ossia eliminare il file target, modificando l'ambiente 
3. **Bloating** -> Le mutazioni sono casuali e distruttive. Se hai un codice perfetto di 3 righe, una mutazione probabilmente lo romperÃ .
	L'evoluzione tende ad aggiungere enormi quantitÃ  di codice inutile (definito "introns" o junk code) come cicli che non fanno nulla.
	- *FUNZIONE:* perchÃ¨ aumento il codice che le mutazioni vanno ad intaccare riducendo la probabilitÃ  che vengano intaccate le parti vitali

### 6.1.5 Black-box Ottimizzazioni
1. Algoritmo di ricerca da massimizzare $f:S\to R$
	1. $S$ Ã¨ lo spazio di ricerca
	2. Di solito: $SâŠ‚  R^n$
2. $f$ puÃ² essere qualsiasi funzione -> piÃ¹ utile se $f$ non Ã¨ convessa
3. L'unica informazione Ã¨ la valutazione della funzione $f$ in un punto
	1. I punti sono presi dallo spazio di ricerca, deve essere in grado di campionare
	2. Non ha bisogno di altre informazioni, **no derivate** 
4. **Metriche di Performance**
	1. Richiede una valutazione per convergere
	2. Valore della soluzione finale
	3. Note -> assumiamo che il costo della valutazione Ã¨ indipendente dal numero degli esempi

### 6.1.6 Strategie di Evoluzione
1. Ottimizzazione black-box **stocastica**
2. $f$ Ã¨ una funzione **continua**
3. Inizia con un insieme di individui
	1. Valuta gli individui
	2. Muta e ricombina individui
4. Individui all'inizio della simulazione $x^0\in S âŠ‚ R^n$
	1. fenotipo, oggetto o **parametri di decisione**
5. Un set di **parametri strategici** $\sigma^o$ per ogni individuo
	1. Tasso di mutazione dei parametri decisionali
	2. Parametri che possono evolversi

> Mutazione = **aggiungere rumore casuale** (Gaussiano)
> - Isotropic Gaussian (1 parametro -varianza) = $N(m,\sigma^2\ l)$)
> - 1 parametro per ogni dimensione = $N(m,\ D^2)$ con $D$ diagonale

Esplora uniformemente lo spazio di ricerca ossia Ã¨ come fare l'assunzione minima sulla struttura di $f$
**Mixing number  $\rho$** -> indica quanti genitori generano figli
- I genitori sono scelti casualmente
- $\rho=1$ -> clone
- Selezione -> (ğœ‡, ğœ†)-ES e (ğœ‡ + ğœ†)-ES
	- Ossia come vedremo dopo la selezione nel primo caso tra solo i figli e nel secondo la selezione dall'insieme combinato di figli e genitori

Se vuoi generare $\lambda$ figli, devi scegliere $\lambda$ gruppi di $\rho$ genitori
**Ricombinazione** -> molte possibilitÃ 
- Ad esempio -> selezionare delle features $j$ dai genitori $i$ (random), media (multi-recombination), media pesata

## 6.2 Self-adaptive ES
> Adapting strategy parameters
> - *ES = Evolution Strategies*

### 6.2.1 (ğœ‡, ğœ†)-ES
- Popolazione $X^g\in R^{\micro ,n}$, step size $\sigma^g\in R^\micro$ -> strategy parameter
	- Uguali per tutte le features do un dato individuo -> isotopic gaussian
- Genera $\lambda$ figli ($k=1,...,\lambda$)
	- Seleziona parametri a caso $P^g\in R^{\rho ,n}$
	- $\sigma^{g+1}_k =recombine(\sigma^g |P^g)e^{\epsilon_k\sim N(0,1)}$
	- $x^{g+1}_k =recombine(P^g)+N(0,\sigma^{g+1}_k)$
	- Valutare la fitness
- Teniamo i meglio figli $\micro$ tra tutti i figli $\lambda$
	- $\lambda > \micro$ iperparametro -> non viene cambiato durante l'evoluzione
- I genitori vengono **sempre scartati**

Come candidati possiamo prendere:
- gli individui **migliori**
- La **media** degli individui
- La **media pesata** degli individui
![[image-27.png|414x386]]

### 6.2.2 (ğœ‡ + ğœ†)-ES
> **I figli competono anche con i genitori**
> - Ã¨ meglio per spazi finiti grandi o problemi combinatori

- ($1+1$)-ES fu il primo ES
	- Seleziona un individuo tra due possibilitÃ  il genitore o il figlio
- Preserva il migliore fino ad ora
- Avvolte puÃ² mischiare i numeri
	- $(\micro /\rho + \lambda)$-ES
	- $(\micro /\rho , \lambda)$-ES

#### Descrizione Algoritmo
Questo pseudocodice descrive il funzionamento generale di unaÂ **Strategia Evolutiva**Â (Evolution Strategy - ES), mostra le due alternative viste primaÂ $(\mu/\rho \stackrel{+}{,} \lambda)$.

Ecco cosa fa passo dopo passo in parole semplici:
1. Creazione dei figli -> Da un gruppo di genitori ($\mu$), l'algoritmo genera un numero maggiore di figli ($\lambda$). Per creare ogni figlio:
    - **Marriage & Recombination:**Â Seleziona alcuni genitori ($\rho$) e mescola le loro informazioni (sia il codice geneticoÂ $y$Â che i parametri strategiciÂ $s$).
    - **Mutation:**Â Applica mutazioni casuali. Nota importante: prima muta i parametri di strategia ($s$, che controllanoÂ _quanto_Â mutare) e poi usa quelli per mutare il codice genetico vero e proprio ($y$).
    - **Fitness:**Â Calcola il punteggio ($F$) del nuovo figlio.
2. Selezione dei sopravvissuti (Line 14-17) -> Alla fine del ciclo, deve decidere chi passa alla generazione successiva. Il codice mostra due modalitÃ  alternative:
    - **$(\mu, \lambda)$Â - Selezione a virgola:**Â I genitori vengono scartati a priori. I nuovi sopravvissuti sono sceltiÂ **solo tra i figli**. (Utile per evitare di rimanere bloccati in ottimi locali).
    - **$(\mu + \lambda)$Â - Selezione col piÃ¹:**Â I nuovi sopravvissuti sono i migliori scelti dall'insieme unito diÂ **genitori E figli**. (Molto elitario, preserva sempre la soluzione migliore trovata finora).

In sintesi:Â **Genera una prole numerosa rimescolando i genitori, mutala, e poi tieni solo i migliori (o rimpiazzando i vecchi o facendoli competere).**

#### PerchÃ¨ Dovremmo Usare (ğœ‡, ğœ†)-ES se non Ã¨ Elitaria
> *ELITARIA* -> significa che la soluzione ottima trovata fino ad ora sopravvive sempre, quindi NON-ELITARIA significa che le soluzione vecchie vengono sempre scartate anche se erano le migliori

Per evitare l'overfitting verso l'oggetto
- `,`-selection -> Ã¨ un esploratore migliore dello spazio di ricerca
- `+`-selection -> Ã¨ uno sfruttatore migliore dello spazio di ricerca

- **Il Paradosso:**Â A volte, per risolvere un problema, deviÂ **smettere di cercare direttamente l'obiettivo**.
- **L'Esempio del Labirinto:**
    - _La Metrica:_Â Immagina che il punteggio sia la "distanza in linea d'aria dall'uscita".
    - _L'Inganno:_Â Se ti trovi vicinissimo all'uscita ma separato da un muro, il tuo punteggio Ã¨ alto.
    - _La NecessitÃ :_Â Per vincere devi allontanarti dal muro (peggiorare il punteggio momentaneamente) per aggirare l'ostacolo.
- **Il Paesaggio di Ricerca (Landscape):**
    - Ãˆ pieno diÂ **ottimi locali**: punti che sembrano soluzioni (punteggio alto) ma sono in realtÃ  vicoli ciechi (come il punto davanti al muro).
- **La Soluzione:**
    - PoichÃ© Ã¨ difficile capire quando un problema Ã¨ "ingannevole", la strategia migliore Ã¨Â **aumentare l'esplorazione** (fare cose diverse) invece di seguire ciecamente la metrica di ottimizzazione, accettiamo di fare un passo peggiorativo per permetterci di migliorare in seguito.

#### Comparazione `,` e `+` -selection
Cosa succede se $\micro = \lambda$, ossia il numero di figli Ã¨ uguale al numero dei nodi che devo prendere, nel caso di (ğœ‡, ğœ†)-ES?
- Tutti i figli sono presi
- Non c'Ã¨ selezione
- Passi casuali nello spazio di ricerca
Cosa succede invece in *(ğœ‡ + ğœ†)-ES*?
- Non ci sono problemi la selezione continua a funzionare
- Tieni i meglio $\micro = \lambda$ individui predi dal set formato dai figli e genitori $\micro + \lambda$
- Qui non cambia niente dato che l'insieme dove applico la selezione Ã¨ piÃ¹ grande dei soli figli dato che comprende anche i genitori

### 6.2.3 CMA-ES - Covariance Matrix Adaptation
> Le mutazioni seguono un matrice di covarianza **adattiva** che si aggiusta basandosi sulla forma locale del paesaggio della funzione 
> - Come il momentum -> velocizza quando c'Ã¨ una pianure e rallenta quando ci sono colline

- Popolazione campionata dalla gaussiana multivariata -> $N(m^g, C^g)$
	- $C^g$ simmetrica
	- $\frac{n^2+n}{2}$ gradi di libertÃ 
- Il vettore medio Ã¨ preso come soluzione candidata
- Inizializza $C^0=I$, $m^g\in R^n$ - random

#### 1 Generazione della Prole -> Sampling
- **L'Azione:**Â Creiamo nuovi candidati (figli) partendo dalla distribuzione attuale. 
- La Formula:$$x_k^{g+1} = m_g + \sigma \cdot N(0, C_g)$$
	- **$m_g$:**Â Il punto centrale attuale (la media della popolazione "buona").
	- **$\sigma$Â (Sigma):**Â L'ampiezza del passo (step-size), quanto lontano osiamo andare.    
    - **$N(0, C_g)$:**Â Il "rumore" casuale modellato dalla matrice di covarianzaÂ $C_g$Â (che determina laÂ _forma_Â della ricerca, es. un cerchio o un ellisse).
- **In sintesi:**Â Prendiamo il centro attuale e "spariamo"Â $\lambda$Â figli casuali attorno ad esso.
#### 2 Valutazione e Ordinamento -> Selection
- **Ordinamento:**Â Una volta generati i figli, calcoliamo la loro fitness (punteggio).
- **Ranking:**Â Li mettiamo in fila dal migliore al peggiore ($x_{1:\lambda}$Â Ã¨ il migliore,Â $x_{\lambda:\lambda}$Â Ã¨ il peggiore).

#### 3 Aggiornamento della Media -> Recombination
- **L'Obiettivo:**Â Spostare il centro della ricerca ($m_{g+1}$) verso la zona promettente trovata dai figli migliori. 
- La Formula:$$m_{g+1} = \sum_{i=1}^{\mu} w_i \cdot x_i^{g+1}$$
    - Si calcola laÂ **media pesata**Â solo dei miglioriÂ $\mu$Â figli.
    - **$w_i$Â (Pesi):**Â Danno piÃ¹ importanza ai figli migliori ($w_1 > w_2 > \dots$). La somma dei pesi Ã¨ 1.

#### 4 Il Concetto Chiave: Selezione Troncata -> Truncated Selection
- **La Domanda:**Â PerchÃ© usiamo soloÂ $\mu$Â genitori se ne abbiamo generatiÂ $\lambda$Â ($\mu < \lambda$)?
- **Il Motivo:**Â Ãˆ questo che crea laÂ **pressione evolutiva**.
    - Se usassimoÂ _tutti_Â i figli ($\mu = \lambda$) per calcolare la nuova media, il centro rimarrebbe praticamente fermo (la media del rumore casuale Ã¨ zero).
    - Scartando i peggiori e tenendo solo i migliori (Troncamento), forziamo la media a spostarsi fisicamente verso la "salita", guidando l'evoluzione.

Ecco il seguito dello schema, strutturato per collegarsi direttamente ai punti precedenti (Sampling, Selection, Mean Update). Qui entriamo nel cuore matematico del CMA-ES.

#### 5 Effective Sample Size ($\mu_{eff}$) - La QualitÃ  dell'Informazione
- **Il Concetto:**Â Misura quanta informazione stiamo realmente utilizzando per aggiornare la distribuzione, basandosi su come sono distribuiti i pesiÂ $w_i$.
    - La Formula:$$\mu_{eff} = \frac{1}{\sum_{i=1}^{\mu} w_i^2}$$
    - **Il Range ($1 \le \mu_{eff} \le \mu$):**
	- Se tutti i pesi sono uguali ($w_i = 1/\mu$):Â $\mu_{eff} = \mu$. Significa che "ascoltiamo" tutti i genitori allo stesso modo (massima informazione, ma bassa pressione selettiva).
    - Se un peso Ã¨ 1 e gli altri 0:Â $\mu_{eff} = 1$. Significa che tutto dipende dal singolo figlio migliore (massima selezione, rischio instabilitÃ ). 
- **Regole Pratiche (Heuristics):**
    - Solitamente si sceglieÂ $\mu \approx \lambda / 2$Â (si tengono metÃ  dei figli).
    - L'obiettivo Ã¨ avereÂ $\mu_{eff} \approx \lambda / 4$.
    - I pesi decrescono linearmente:Â $w_i \propto \mu - i + 1$.

#### 6 Aggiornamento della Matrice di Covarianza ($C$) - Parte I

Questo Ã¨ il passaggio cruciale che permette all'algoritmo di "imparare" la forma del problema (es. capire se c'Ã¨ una valle stretta e allungare l'ellisse di ricerca in quella direzione).
- L'Intuizione:
    Vogliamo che la nuova matrice $C_{g+1}$ aumenti la probabilitÃ  di generare vettori simili a quelli che hanno avuto successo nella generazione corrente ($x_{g+1}$). Se andare a "Nord-Est" ha pagato, deformiamo la ricerca verso "Nord-Est".
- Riscrivere l'aggiornamento della Media:
    Possiamo vedere la nuova media non come una posizione assoluta, ma come la vecchia media piÃ¹ uno spostamento pesato: $$\mathbf{m}_{g+1} = \mathbf{m}_g + \sum_{i=1}^{\mu} w_i (\mathbf{x}_i^{g+1} - \mathbf{m}_g)$$
    (Stiamo sommando i vettori differenza "successo - vecchia media")

- Stima della Covarianza ($C_{\lambda}^{g+1}$):
    Prima di aggiornare la matrice vera e propria, stimiamo la forma della distribuzione attuale basandoci sui campioni appena estratti: $$C_{\lambda}^{g+1} = \frac{1}{\lambda} \sum_{i=1}^{\lambda} (\mathbf{x}_i^{g+1} - \mathbf{m}_g)(\mathbf{x}_i^{g+1} - \mathbf{m}_g)^T$$
    - **Concetto chiave:**Â Questo Ã¨ unoÂ **stimatore corretto (unbiased estimator)**Â diÂ $C_g$. Significa cheÂ $E[C_{\lambda}^{g+1}] = C_g$. In parole povere: se avessimo infiniti campioni, questa formula ci ridarebbe esattamente la forma della matrice originale.
#### 7 Stima Migliorata della Covarianza ($C_{\mu}$)
- **L'Idea:**Â Invece di stimare la forma usandoÂ _tutti_Â i figli (che darebbe una stima neutra/unbiased), usiamo solo iÂ **miglioriÂ $\mu$Â figli**. 
- La Formula:$$C_{\mu}^{g+1} = \sum_{i=1}^{\mu} w_i \frac{(x_i^{g+1} - m_g)}{\sigma} \frac{(x_i^{g+1} - m_g)^T}{\sigma}$$
- **Il Significato:**
    - La somma ora arriva fino aÂ $\mu$Â (nonÂ $\lambda$): stiamo ignorando i figli scarsi.
    - **Effetto:**Â Campionare da questa matriceÂ $C_{\mu}$Â tenderÃ  a riprodurre i passi che hanno avuto successo in questa generazione.

#### 8 Rank-$\mu$Â Update (L'Aggiornamento Robusto)
- **Il Problema:**Â Non possiamo buttare via la vecchia matriceÂ $C_g$Â e sostituirla conÂ $C_{\mu}$Â ogni volta (sarebbe instabile e dimenticherebbe tutto ciÃ² che ha imparato prima).
- **La Soluzione (Exponential Smoothing):**Â Uniamo la vecchia conoscenza con la nuova stima.
- La Formula:$$C_{g+1} = (1 - c_{\mu}) C_g + c_{\mu} \frac{1}{\sigma^2} C_{\mu}^{g+1}$$
    - **$C_g$:**Â La "memoria" storica delle generazioni passate.
    - **$C_{\mu}^{g+1}$:**Â La "novitÃ " imparata dai migliori figli di oggi.
    - **$c_{\mu}$Â (Learning Rate):**Â Un valore tra 0 e 1. Determina quanto velocemente l'algoritmo impara (o dimentica).
        - $c_{\mu}$Â bassoÂ $\rightarrow$Â Memoria lunga (stabile).
        - $c_{\mu}$Â altoÂ $\rightarrow$Â Adattamento rapido (ma rischioso)

#### 8 Rank-One Update (L'Aggiornamento Aggressivo)
- **L'Intuizione:**Â E se invece di fare la media pesata diÂ $\mu$Â vettori, ci fidassimo ciecamenteÂ **solo del primo della classe**Â (il figlio migliore assolutoÂ $x_1$)?
- La Formula: $$C_{g+1} = (1 - c_1) C_g + c_1 \frac{(x_1^{g+1} - m_g)}{\sigma} \frac{(x_1^{g+1} - m_g)^T}{\sigma}$$
- **L'Effetto:**
    - Aumenta drasticamente la probabilitÃ  di generare vettori lungo la linea dell'unico passo migliore appena fatto.
    - Ãˆ molto efficiente quando c'Ã¨ una direzione chiara e dominante da seguire.
#### 9 Evolution Path (Percorso Evolutivo)
- Per aggiornare la matrice di covarianza, si considera unaÂ **sequenza di passi successivi**Â avvenuti nel corso di piÃ¹ generazioni (non solo l'ultima).
#### 10 Il "Vero" Aggiornamento CMA-ES
- L'algoritmo completo si ottiene combinando due tecniche:
    - **Rank-one update:**Â applicato agliÂ _evolution paths_.
    - **Rank-$\mu$Â update:**Â (quello classico basato sulla media dei migliori).

#### 11 Adattamento diÂ $\sigma$Â (Step Size)
- La grandezza del passoÂ $\sigma$Â **non Ã¨ fissa**.
- Esiste una regola di aggiornamento che permette laÂ **self-adaptation**Â (l'algoritmo regola da solo quanto grandi devono essere i passi).
### 6.2.4 Rissunto
#### Struttura degli Algoritmi Evolutivi (EAs)
- **Ciclo di Base (Scheletro):**
       1. Inizializzazione della popolazione.
    2. Valutazione (Fitness).
    3. Selezione dei genitori.
    4. Variazione (Crossover e Mutazione).
    5. Sostituzione (creazione della nuova generazione)Â 5.
- **Tipologie Principali:**
    - **Genetic Algorithms:**Â Usano stringhe di bit; la ricombinazione Ã¨ il fattore principale.
    - **Genetic Programming:**Â L'individuo Ã¨ un programma (es. albero di sintassi); usato per generare codice.
    - **Evolution Strategies (ES):**Â Ottimizzazione a valori reali, basata su mutazione stocastica (es. rumore Gaussiano.
- **Problemi Comuni:**Â L'evoluzione puÃ² "barare" (Cheating) trovando scorciatoie tecniche che soddisfano la fitness senza risolvere il problema reale (es. cancellare il file target invece di eguagliarlo) o soffrire di "Bloating" (codice spazzatura.

#### Evolution Strategies (ES)
- **Obiettivo:**Â Ottimizzazione "Black-box" di funzioni continue dove non si conoscono derivate.
- **Self-Adaptation:**Â Ãˆ fondamentale evolvere non solo la soluzione ($x$), ma anche i parametri di strategia ($\sigma$, step size) per evitare di rimanere bloccati quando la fitness migliora.
- **Meccanismi di Selezione:**
    - **Selezione a VirgolaÂ $(\mu, \lambda)$:**Â I genitori vengono scartati. Non Ã¨ elitaria, favorisce l'esplorazione e l'adattamento dinamico del passo (step size).Â Ãˆ generalmente preferita per evitare ottimi locali.
    - **Selezione col PiÃ¹Â $(\mu + \lambda)$:**Â I genitori competono con i figli.Â Ãˆ elitaria (il migliore sopravvive sempre), ottima per sfruttare una zona (exploitation) ma rischia stagnazione.

#### CMA-ES (Covariance Matrix Adaptation)
- **Funzionamento:**Â Ãˆ lo stato dell'arte per l'ottimizzazione continua.Â Adatta una matrice di covarianza ($C$) per modellare la forma del paesaggio di ricerca (es. trasformando la ricerca da circolare a ellittica).
- **Aggiornamento della Media:**Â La nuova media Ã¨ una media pesata dei miglioriÂ $\mu$Â figli (Selection), spingendo la ricerca verso le zone promettenti.
- **Aggiornamento della Covarianza ($C$):**Â Combina due metodi:
    - **Rank-$\mu$Â update:**Â Usa le informazioni della popolazione (stabilitÃ ).
    - **Rank-one update:**Â Usa il percorso evolutivo (evolution path) per sfruttare il "momento" e la direzione dei passi precedenti.
- **Controllo del Passo ($\sigma$):**Â La dimensione del passo si adatta confrontando la lunghezza del cammino percorso con quella di una camminata casuale (Path Length Control).

# 7 Artificial Life
> Evolution of Complexity -> da cellule base a organismi piÃ¹ complessi

**Tierra** -> Programmi software (creature) che competono per il tempo della CPU e vivono nello spazio RAM
**Avida** -> Il software ha risorse dedicate, puÃ² acquisirne di piÃ¹ eseguendo attivitÃ  (ad esempio la moltiplicazione binaria)

> Fitness -> vivere abbastanza a lungo

- **SensibilitÃ  alle condizioni iniziali:**Â Una variazione infinitesimale all'inizio porta a risultati finali completamente diversi (Caos).
- **Determinismo vs PrevedibilitÃ :**Â Il sistema segue regole fisse, ma Ã¨Â **imprevedibile**Â perchÃ© Ã¨ impossibile conoscere lo stato iniziale con precisione perfetta.
- **Conclusione:**Â Non possiamo calcolare il futuro con una formula, l'unico modo Ã¨Â **eseguire la simulazione**Â passo dopo passo.

### 7.1.1 Cellular Automata
> Modello matematico semplice di **self-replication** (riproduzione autonoma)

- La descrizione del sistema viene utilizzata per replicare il sistema e per costruirlo
	- Questo Ã¨ stato concepito prima che il meccanismo del DNA fosse pienamente compreso / scoperto
- **Spazio Discreto** -> multi-dimensional vettore di celle e **tempo discreto**
- 1 variabile discreta per cella
- Regole di Aggiornamento dettano le dinamiche
	- Aggiornamenti sincroni basati su ogni celle dei vicini
- Gestione dei Confine
	- Celle possono rimanere costanti e lo spazio puÃ² essere sovrapposto -> toroidal structure
	- Anche il quartiere puÃ² essere definito in modo diverso

#### Funzionamento base di unÂ **Automa Cellulare Elementare (1D)** - ECA
- **Struttura:**Â Una fila di celle che possono essere soloÂ **0**Â oÂ **1**Â (bianco o nero).
- **Regola di Aggiornamento:**Â Lo stato futuro di una cella dipende solo daÂ **se stessa e dai suoi due vicini**Â (sinistra e destra) al tempo precedente (sjâˆ’1,sj,sj+1).
- **La "Regola":**Â I disegni in basso (111Â â†’Â 0, 110Â â†’Â 1, ecc.) definiscono la "legge fisica" di quel mondo. Leggendo i risultati in basso come un numero binario (es. 01011010), si ottiene il numero della Regola (es. Rule 30, Rule 90) che determina se il sistema genererÃ  ordine, caos o strutture frattali.

Assunzioni:
- Uno stato iniziale 0-state deve essere mappato a 0 -> L'ultima cifra deve essere 0
- Riflessione Simmetrica -> isotropia: stesso comportamento che riguarda la direzione
	- 110 -> 011, 100 -> 001
- Regole ammissibili del modulo $ğ‘_1ğ‘_2ğ‘_3ğ‘_4ğ‘_2ğ‘_5ğ‘_40$ â†’ 32 possible 

![[image-28.png|488x251]]
![[image-29.png|493x245]]

**Evoluzione**
- Rumore bianco come stato iniziale
	- I valori delle celle non sono correlati tra loro
	- Un parametro p descrive l'intero stato
- Sistema ordinato di conseguenza
	- PiÃ¹ parametri per descriverlo
- ProprietÃ  di auto-organizzazione
	- Un altro segno distintivo di sistemi non lineari complessi
- 300 fasi temporali di evoluzione con regola 126
- Stato iniziale casuale
	- $P(x=1) = 0,5$
- Ãˆ anche possibile lo studio delle proprietÃ  globali
	- Teoria classica dei sistemi dinamici

##### Regole in ECA
1. Type I
	- Evoluzione verso gli stati statici
2. Type II
	- Evoluzione alla struttura periodica
3. Type III
	- L'evoluzione a schemi caotici -> porta alla (pseudo) casualitÃ 
4. Type IV
	- Evoluzione a modelli complessi -> limite del caos

##### CA con k Stati per Cella
> Formazione di membrane protettive che proteggono il loro contenuto dagli effetti esterni
> Quando due membrane si scontrano possono distruggersi a vicenda

![[image-30.png|326x278]]

## 7.2 Universal Computer
