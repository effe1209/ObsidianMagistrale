---
aliases:
tags:
  - schema
dg-publish: true
---
# 1 Agents

> Ãˆ qualunque cosa che puÃ² interagire

Possiede:

- **Sensori** â†’ per percepire l'ambiente
- **Attuatori** â†’ per effettuare azioni nell'ambiente

$f:P^{*}\to A$

Dove:

- $P^{*}$ â†’ Ã¨ la storia delle percezioni
- $A$ â†’ Ã¨ l'insieme delle possibili azioni di un agent 

![[allegati/Image.png|711x394]]

+ *The First Agent*
    semplice senza valutare nessun tipo di valutazione, completa l'obbiettivo senza considerare il tempo o i cicli. Non considera i cambiamenti intermedi

```cpp
function Reflex-Vacuum-Agent( [location,status]) returns an action
  if status = Dirty then return Suck
  else if location = A then return Right
  else if location = B then return Left
```

**Rational Agents** â†’ Ã¨ un agente che sceglie le proprie azioni allo scopo di massimizzare le metriche i performance, dato lo storico delle percezioni ^RationalAgent

- Non Ã¨ onnisciente
- Non prevede il futuro
- Non ha sempre successo

*AMBIENTE STATICO* â†’ ossia completamente osservabile e prevedibile (azioni deterministiche con regole chiare) incoraggiando ad imparare comportamenti fissi.

- La soluzione ottima non cambia, stesso obbiettivo
- Soluzione fragile
    - *Esempio Vespa*: che come c'Ã¨ un cambiamento riparte dall'azione che scaturisce la percezione quindi spreco di cicli inutile

## 1.1 PEAS

***P***erformance metric

***E***nviroment

***A***ctuators

***S***ensors

PiÃ¹ Ã¨ ristretto l'ambiente â†’ piÃ¹ l'agente sarÃ  semplice da costruire

- Agent Architecture = $A + S$
- Agent = Architecture + Function
+ Example Taxi

Performance metric: safety, destination, profits, legality, comfort, ....
Environment: streets/freeways, traffic, pedestrians, weather, ...
Actuators: steering, accelerator, brake, horn, speaker/display, â€¦
Sensors: camera, accelerometers, gauges, engine sensors, keyboard, GPSâ€¦

## 1.2 Enviroments - Ambienti

![[allegati/Image (2).png]]

> **Single-Agent** â†’ ossia che nell'ambiente agisce un solo agente e non deve competere con altri

> Real Agent â†’ Ã¨ come il taxi non comprende nessuna delle caratteristiche, il che lo rende molto complicato da realizzare

- **Table-Driven Agent** â†’ ossia prende decisioni consultando una **tabella di lookup** in cui, per ogni possibile percezione (o sequenza di percezioni), Ã¨ giÃ  memorizzata lâ€™azione da compiere.
    - ComplessitÃ  alta per la creazione della tabella
- **Reflex Agent** â†’ insieme di regole condizionali dirette â€œcondizione â†’ azioneâ€ ^ReflexAgent

![[Image (3).png|755x455]]

***Effectiveness-Efficiency Trade-off***

| **TA**                                     | **SRA**                                                            |
| ------------------------------------------ | ------------------------------------------------------------------ |
| Alti costi per gestire e creare la tabella | Riduce i costi creando solo delle condizioni di attivazione        |
| Scala esponenzialmente il tempo â†’ $O(n^2)$ | Scala linearmente il tempo â†’ $O(n)$                                |
|                                            | Semplifica un approccio giÃ  base e stupido â†’ osservabile e piccolo |

## 1.3 Tipi di Agenti

### 1.3.1 Model-Based Agent

^5938be

![[Image (4).png|748x383]]

Aggiunge â†’ *STATE* al [[#^ReflexAgent]

- Stima lo stato attuale dell'ambiente basandosi sullo stato interno dell'agente
    - Lo stato interno dipende dallo storico delle percezioni
- Lo stato interno serve per catturare il modello delle transizioni dell'ambiente
    - Conseguenze dell'azione dell'agente
    - Dinamiche dell'ambiente
- *Sensor Model* â†’ come i sensori dell'agente rappresentano il mondo - rumore e limitazioni
    - Approssima l'intero stato â†’ efficace in ambianti parzialmente osservabili

### 1.3.2 Goal-Based Agent

![[Image (5).png|702x430]]

â“Massimizzare la ricompensa immediata vs massimizzare la ricompensa a lungo termine

- **Search** â†’ come Ã¨ meglio navigare tra le possibili soluzioni? Decision Tree
- **Planning** â†’ sfruttare la conoscenza del mondo per eseguire le migliori azioni

> I modelli goal-based rappresentano il loro obbiettivo (*goal*) e possono modificarlo

### 1.3.3 Utility-Based Agent

![[Image (6).png|727x448]]

> Invece di utilizzare un solo stato, assegna un punteggio ad ogni stato interno che indica quanto Ã¨ buono

- Permette di analizzare varie traiettorie e quindi tenere in considerazione non solo se arrivo al traguardo ma anche come ci arrivo â†’ permette di selezionare la soluzione migliore tra quelle esistenti risparmiando sulle mosse da fare per esempio

Ãˆ fondamentale quando:

- ci sono stati finali **non tutti equivalenti** (es. vincere con 10 mosse vs con 100 mosse),
- lâ€™ambiente Ã¨ **stocastico** (es. backgammon),
- ci sono piÃ¹ obiettivi in conflitto (es. â€œarrivare a destinazione **velocemente ma in sicurezza**â€)

### 1.3.4 Learning-Based Agent

![[Image (7).png|782x329]]

Tutti i modelli di agenti possono essere learning-based o no.

L'apprendimento puÃ² modificare qualche componente.

- **Elementi prestazionali** â†’ i precedenti agenti ricevevano il contesto e sceglievano l'azione da effettuare
- **Critic** â†’ guida l'apprendimento tramite la valutazione del comportamento attuale con la metrica delle performance esterne
- **Problem** â†’ allontanarsi dal comportamento greedy provando nuove azioni (greedy = sceglie l'opzione migliore senza considerare altre soluzioni) 

## 1.4 Summary

- **Agenti** interagiscono con **l'ambiente** tramite **attuatori** e **sensori**
- La **funzione** degli agenti â†’  [Agent = Architecture + Function](craftdocs://open?blockId=4F5E2847-6565-43CC-A12A-EB81130E4DFF&spaceId=4b28628f-7dfd-54ce-27f9-28712867f81f) â†’ descrive cosa fa l'agente in tutte le circostanze
- La **misurazione delle prestazioni** valuta la sequenza dell'ambiente
- Un [[#^RationalAgent]]
- [PEAS](./ğŸ”¥ Schema Slide.md#peas) â†’ descrive gli ambienti di attivitÃ 
- [Enviroments - Ambienti](./ğŸ”¥ Schema Slide.md#enviroments--ambienti) â†’ Osservabile, Deterministico, Episodico, Statico, Discreto
- Molti tipi di Agenti
    - [[#^ReflexAgent]]
    - [[#1.3.1 Model-Based Agent]]
    - [[#1.3.2 Goal-Based Agent]]]
    - [[#1.3.2 Goal-Based Agent]]

---

# 2 Search

> Search Alorithm â†’ dato un problema, ritorna una sequenza di azioni, che formano un path verso lo stato *goal* o *fail*

**Open-Loop** - *offline* â†’ un agente cerca la sequenza ottimale di azioni e successivamente la esegue
- *Completamente osservabile*
- *Deterministico* â†’ risposte dell'ambiente conosciute

**Closed-Loop** - *online* â†’ l'agente valuta continuamente i feedback provenienti dall'ambiente
- *parzialmente osservabile*
- *Stocastico* â†’ le risposte variano
	- Devo controllare il mondo per sapere dove sono, in quale stato

![[Image (8).png|752x448]]

> **Scelte di Design â†’ Guida veloce**

- **Deterministica e completamente osservabile** â†’ problema a stato singolo
    - l'agente sa esattamente in quel stato si troverÃ  â†’ la soluzione Ã¨ una sequenza di stati o azioni
- > **Non osservabile** â†’ problema conforme senza sensore
    - L'agente non percepisce lo stato â†’ soluzione Ã¨ una sequenza di azioni
- *Non deterministico e parzialmente osservabile** â†’ problema di contingenza
    - le percezioni prendono nuove informazioni riguardo allo stato
    - la soluzione Ã¨ un piano contingente o una policy
    - spesso ricerca interlacciata
- **Spazio degli Stati Sconosciuto** â†’ problema di esplorazione (online - closed-loop)
    - l'agente deve scoprire lo spazio degli stati

![[Image (9).png|741x278]]

![[Image (10).png|738x361]]

## 2.1 Tree Search

> Inizia dagli stati giÃ  visitati e gli espande â†’ genera gli stati successivi

- > **Frontiera** â†’ insieme di nodi non espansi che separano i nodi espansi da quelli non raggiunti
- > **Nodi Raggiunti** â†’ *frontiera* + *nodi espansi*
- > Ricerca **Uniforme**

```cpp
function Tree-Search(problem, strategy) return a solution or failure
  inizializza con initial state del problema
  loop do
    if non ci sono candidati return failure
    scegli un nodo da espandere in base alla strategia
    if nodo contine goal state return soluzione
    else espandi il nodo e aggiungi il nodo alla ricerca ad albero
  end
```

![[Image (11).png]]

![[Image (12).png]]

- **State** â†’ uno stato Ã¨ una rappresentazione di una configurazione fisica
- **Nodo** â†’ Ã¨ una struttura dati che costituisce una parte della ricerca ad albero
    - include genitori, figli. profonditÃ  e costo dei path (cammini)
- **La funzione di Espansione**
    - Crea nuovi nodi
    - Riempie i vari campi
    - Usa la funzione *Successore* del problema per creare gli stati corrispondenti

> La ricerca ad albero non corrisponde all'intero spazio degli stati â†’ un insieme di satti finito puÃ² portare ad una ricerca infinita (loops, path ridondanti)

> Nodi nella ricerca ad albero = stati dello spazio

Gli stati sono tutti diversi nello spazio

I nodi non sono unici in un albero di ricerca â†’ se abbiamo un nodo che ritorna ad un altro giÃ  esplorato devo comunque selezionarlo

### 2.1.1 Scegliere una Strategia di Ricerca

> Una strategia di ricerca Ã¨ definita **in base all'ordine con cui i nodi vengono espansi**

Una **strategia** Ã¨ **valutata** tramite queste dimensioni:

- **Completezza** â†’ trova sempre una soluzione
- **ComplessitÃ  di Tempo** â†’ numero di nodi generati o espansi
- **ComplessitÃ  dello Spazio** â†’ numero massimo di nodi in memoria
- **Ottimale** â†’ trova sempre la soluzione con meno costo

ComplessitÃ  di Tempo e Spazio sono misurati in:

- $b$ â†’ fattore massimo di ramificazione dell'albero di ricerca (quanti nodi espando)
- $d$ â†’ profonditÃ  della soluzione migliore (minor costo)
- $m$ â†’ massima profonditÃ  dello spazio degli stati (puÃ² essere $\infty$)

## 2.2 Uninformed Tree Search - Non Informata

> Una ricerca non informata usa solo le informazioni disponibili quando definiamo il problema, non sappiamo quanto siamo lontani dal goal (obbiettivo)

- Breadth-first search
- Uniform-cost search
- Depth-first search
- Depth-limited search
- Iterative deepening search

### 2.2.1 Breadth-First Search
- Frontiera â†’ Ã¨ una coda FIFO
- Espande prima in larghezza poi in profonditÃ 

![[Image (13).png]]

![[Image (14).png]]

| **Completo**    | **Time**                                         | **Space**                      | **Optimal**                                       |
| --------------- | ------------------------------------------------ | ------------------------------ | ------------------------------------------------- |
| **Si**          | $\mathbf{O}(b^d)$                                | $\mathbf{O}(b^d)$              | **Si**                                            |
| Se $b$ Ã¨ finito | Dato che fa tutti i nodi ad una certa profonditÃ  | Prende tutti i nodi in memoria | Se il costo per step Ã¨ $1$ non ottimo in generale |

#### Dijkstra's Algorithm â†’ ricerca costo uniforme

> Le azioni hanno un costo differente â†’ espandere i nodi con il cammino di costo minimo

> La frontiera Ã¨ una coda pesata â†’ i path con costo minore vanno per primi

| **Completo**                                          | **Time**                                                                                                                | **Space**                | **Optimal**                                                |
| ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ------------------------ | ---------------------------------------------------------- |
| **Si**                                                | $\mathbf{O}(b^{1+[\frac{C^*}{\eta}]})$                                                                                  | $\mathbf{O}(b^d)$        | **Si**                                                     |
| Se il costo minimo delle azioni Ã¨ positivo ossia $>0$ | Dove $C^*$ Ã¨ il costo del path ottimale â†’ puÃ² eplorare percorsi a basso costo che non servono a niente puÃ² essere $b^d$ | **Stesso della memoria** | I nodi sono espansi in ordine in base al costo del cammino |

### 2.2.2 Depth-First Search

> Espande prima i nodi in profonditÃ 

> Coda â†’ LIFO â†’ mette i nodi in cima (Ultimo ad entrare primo ad uscire Last In - First Out)

![[Image (15).png]]

![[Image (16).png]]

| **Completo**                 | **Time**                                                                       | **Space**        | **Optimal** |
| ---------------------------- | ------------------------------------------------------------------------------ | ---------------- | ----------- |
| **Si**                       | $\mathbf{O}(b^m)$                                                              | $\mathbf{O}(bm)$ | No          |
| Se $b$ Ã¨ finito e senza loop | Dato che vado prima in profonditÃ  esploro prima tutti i fligli fino al massimo | **Lineare**      |             |

> DFS puÃ² essere implementato in vari modi:

- > **Depth-Limited search** â†’ con profonditÃ  di esplorazione fissata
- > **Iterative Deeping Search** â†’ iterazioni con valore di profonditÃ  che aumenta
- > **Bidirectional Search** â†’ iniziamo la ricerca dallo stato iniziale e dallo stato obbiettivo verso lo stato iniziale

#### Come Riconoscere uno Stato Ripetuto?

- **Ricordare gli stati raggiunti**
    - Posso riconoscere e conservare i path di costo minore
    - Se ho spazio
- Non controllo se il problema non produce path ridondanti
- **Controlla solo i loop**
    - Conserva i genitori per ogni nodo
    - Segue la catena dei parenti per vedere se ricade in un loop
    - Posso seguire la catena fino al root â†’ puntatori al padre
    - **Non c'Ã¨ bisogno di memoria in piÃ¹** solo **piÃ¹ tempo per controllare la catena**

### 2.2.3 Sommario â†’ Uninformed Search

![[Image (17).png]]

## 2.3 Heauristic - Informed Search

> Espandere i nodi piÃ¹ promettenti

> Promettenti Ã¨ una valutazione euristica â†’ propone una stima del minor costo da uno stato al nodo $n$ fino al goal

> Frontiera â†’ Ã¨ una coda pesata e ordinata in base al valore euristico

Best-First Search Algorithm:

- Greedy Search
- A* Search

### 2.3.1 Greedy Search

> Seleziono ad ogni nodo quello con il costo euristico minore

- Non ha meccanismi per trovare il cammino migliore guardando i successivi
- Potrebbe portare a cammini di costo maggiore anche se il primo passo Ã¨ piÃ¹ conveniente

| **Completo**                                                                                                | **Time**                                       | **Space**                     | **Optimal** |
| ----------------------------------------------------------------------------------------------------------- | ---------------------------------------------- | ----------------------------- | ----------- |
| No                                                                                                          | $\mathbf{O}(b^m)$                              | $\mathbf{O}(b^m)$             | No          |
| PuÃ² rimanere incastrato in un loop.<br/>Completo in uno spazio finito con il controllo degli stati ripetuti | Una buona euristca puÃ² portare a miglioramenti | Tiene tutti i nodi in memoria |             |

### 2.3.2 A* Search

> Idea â†’ evitare di espandere i cammini giÃ  costosi

> Controllo se un cammino Ã¨ piÃ¹ costoso di un altro cammino che ho giÃ  esplorato se si allora continuo il cammino con il costo minore e cosÃ¬ via

> In questo modo tengo in considerazione il cammino di costo minimo generale, in modo da poter scegliere quello minore analizzando anche cammini alternativi

**Funzione di Valutazione** â†’ $f(n)=g(n)+h(n)$

- $g(n)$ = costo fino a questo momento per raggiungere $n$ 
- $h(n)$ = costo stimato al goal da $n$ â†’ euristica greedy
- $f(n)$ = costo stimato del path migliore passando attraverso $n$ per raggiungere il goal
- Questa funzione valuta ad ogni nodo il costo decidendo come proseguire

**ComplessitÃ  di Spazio** â†’ la complessitÃ  scala con il numero di nodi espanso

- $A*$ espande tutti i nodi con $f(n)<C^*$
- $A*$ espande qualche nodo con $f(n)=C^*$
- $A*$ non espande nodi con $f(n)>C^*$
- Con $C^*$ costo del path ottimale 

**ComplessitÃ  di Tempo** â†’ Ã¨ esponenziale nell'errore di $h$

A* utilizza un euristica *ammissibile*

- $h(n)\le h^*(n)$ dove $h^*(n)$ Ã¨ il vero costo da $n$
- $h(n)\ge 0$ tale che $h(G)=0$ per ogni goal $G$
- Un euristica ammissibile non sovrastima mai la distanza dal goal

***A** Ã¨ completa**

***A$^*$* Ã¨ ottima se *h* Ã¨ ammissibile**

> **Euristica Consistente** â†’ $h(n)\le c(n,a,n \prime)+h(n \prime),\ \forall n\prime$

- PiÃ¹ forte dell'ammissibilitÃ  â†’ il costo dei cammini Ã¨ monotona crescente
    - Ossia il valore di $f(n)$ non scende mai lungo il cammino
- Quando raggiungi uno stato Ã¨ giÃ  sulla la strada ottimale per arrivare ad $n$
    - Se l'euristica fosse solo ammissibile invece potrei estrarre un nodo con un cammino sub-ottimale e doverlo riconsiderare in seguito

![[Image (18).png]]

### 2.3.3 Problema
> A* visita troppi nodi â†’ richiede che vengano esplorati ed espansi molti nodi
> â†’ **A* Pesato**: sub-optimal ma abbastanza buono
> $f(n)=g(n)+W\ h(n),\ \ W>1$
> Trova una soluzione con costo tra $C^*$ e $W\ C^*$
> -> Spesso vicina a $C^*$

#### Contorni con A*

![[Image (19).png|714x431]]

![[Image (20).png]]

![[Image (21).png]]

#### Beam Search
LaÂ **beam search**Â Ã¨ un algoritmo di ricercaÂ basato suÂ **euristiche** Â che esplora un grafo espandendo il nodo piÃ¹ promettente in un insieme limitato di nodi.
La beam search  simile aÂ [[#2.2.1 Breadth-First Search]] limitata.Â che mira a ridurre i requisiti di memoria. Nella beam search Ã¨ tenuto in considerazione solo un numero predefinito di migliori soluzioni parziali.Â Di conseguenza, Ã¨ considerato unÂ algoritmo greedy.
- Lâ€™euristica non Ã¨ necessariamente ammissibile nÃ© ottimale
- **Obiettivo tipico**: generare sequenze plausibili in problemi di decoding
- Mantiene iÂ **B migliori cammini parziali**Â in un albero di ricerca.
- Ad ogni passo: espande ognuno, valuta i successori, sceglie i B migliori, scarta gli altri.
- Non garantisce la soluzione ottimale

---
# 3 Local Search
Cosa vogliamo:
- Veloce
- Economico
- Abbastanza buono
- Ricerca non sistematica
	- Non ci interessa come ci arriviamo all'obbiettivo ma vogliamo arrivarci

> Local search restituisce una soluzione non un path
> - Non salva gli stati visitati
> - Non esplora l'intero spazio degli stati

## 3.1 Hill Climbing
> Hill-climbing puÃ² rimanere bloccato
	- Ottimo locale
	- Greedy Local Search


> [!NOTE] Gradient Descent
> Se uso il gradiente per valutare e guidare la funzione di ricerca otteniamo la discesa del gradiente.

- si blocca su i massimi locali
- Si blocca su parti pari (flat)
### 3.1.1 Soluzioni
**Random Restart**
- Riesce a scapapre dall'ottimo locale
- Completo ma molto lento
**Sideway Move** -> *movimento laterale*
- Riesce ad uscire dalle parti flat se non sono il massimo
- Fisso -> Si sposta di massimo $k$ step
![[image-1.png|525x302]]

```c title="Hill Climbing"
function Hill-Climbing(problem) returns a state that is a local maximum
    inputs: problem, a problem definition
    local variables: current, a node, neighbor, a node
    
    current â† Make-Node(Initial-State(problem))
    loop do
        neighbor â† a highest-valued successor of current
        if Value(neighbor) â‰¤ Value(current)
        then return State(current)
        current â† neighbor
end
```

## 3.2 Simulated Annealing
> Uscire dall'ottimo locale facendo un passo random
> - un passo random non rimane mai incastrato -> ma Ã¨ lento


> [!important] Simulated Annealing
> Corrisponde a -> *hill climbing $+$ random moves*
> - I movimenti random diventano meno frequenti nel tempo
> - I movimenti casuali migliorano con il tempo
> - **Temperature** -> Ã¨ un parametro che controlla il trade-off tra hill-climbing e movimenti random
> 	- La temperatura cambia con il tempo permettendo all'inizio di fare molti passi casuali e mano a mano che vado avanti i passi casuali diminuiscono
> 	- Alla fine $T=0$ -> nessun passo casuale


### 3.2.1 Temperature Scheduling
$$
e^{\frac{x}{T}},\quad T>0,\quad x>0
$$
- $x$ Grande -> Ã¨ una mossa molto sbagliata, c'Ã¨ una bassa probabilitÃ  di accettare lo spostamento
- Aumentando $T$ -> aumento la probabilitÃ  di accettare spostamenti
Se $T\to 0$ allora anneiling diventa deterministico -> non effettua mosse casuali
- Accetta solo spostamenti buoni $e^{\frac{x}{T}}\to 0$ 
**Anneiling Scheduling** -> inizia con T grande e lo diminuisce con il passare del tempo
- Se T viene diminuite abbastanza lentamente l'algoritmo converge al *massimo globale* 
	- Se invece Ã¨ troppo lento visito tutti gli stati -> lentissimo
- *Diminuzione Esponenziale* -> $T_t = k\ T_{t-1},\quad 0<k<1$
- *Power-law Decrese* -> $T_t=\frac{T_0}{t^a}$ es: $a=1$
### 3.2.2 Local Beam Search
> Invece di prendere solo la prima soluzione prendo le prime $k$
> Espando tutto e seleziono i top-$k$ stati
> - Ha senso quando la funzione non converge sicuramente


> [!important] Funzionamento
> Ãˆ la versione stocastica della [[#Beam Search]]
> Gli stati interagiscono con gli altri -> abbandono i path con meno importanza, concentrandomi su quelli piÃ¹ promettenti
> - Non Ã¨ una ricerca parallela su $k$ path
> - *Local beam -> parte da k stati, tiene i migliori k successori globalmente.*
> - **Non garantisce ottimalitÃ **

#### Beam search decoder in NLP

NLP -> Natural Language Processing
Tutto ciÃ² che concerne l'elaborazione del linguaggio naturale -> nell'esempio la traduzione da inglese ad italiano tramite encoder (trasforma la frase in vettori-embedding) e deconder con vari livelli di linar model e softmax per avere l'output
*Esempio:*
- *Input*: how are you?
- *Output*: come stai?
![[image-2.png]]
##### 3.3 Beam Search vs Greedy Search in NLP
**Greedy Search** -> semplicemente prende la parola piÃ¹ simile per ogni posizione
- Ogni parola Ã¨ considerata indipendente
- Funziona ma puÃ² essere migliorato
**Beam Search**
- Trova il path migliore e tiene in considerazione cosa ha giÃ  selezionato
- Prende $N$ migliori candidati per ogni posizione
- Espande gli attuali $N$ migliori candidati e prende i migliori $N$
- Nell'ultima posizione prende il miglior risultato tra gli $N$ candidati finali
##### Beam Search Decoder in NLP
Bisogna eseguire il decoder $N$ volte per ogni posizione:
- Costa tanto
- Ma il risultato spesso Ã¨ il migliore
![[image-3.png]]
## 3.3 Ambiente Non Deterministico

> Alcune azione hanno un risultato non deterministico

Belief State (Stato di Fede) -> un insieme di stati futuri per ogni stato corrente e coppia di azioni
- Non sai in anticipo dove ti porta l'azione
Conditional Plan -> la solzuione Ã¨ un albero (if-then-else chain) non una sequenza
- Puoi eseguire la soluzione e in base al risultato dell'ambiente scegliere l'azione corretta
- **AND-OR Search Trees** -> Ã¨ la soluzione per il conditional plan
	- **AND node** -> Fornito dall'ambiente insieme al set di tutti i possibili stati successivi
	- **OR node** -> Fornito dall'agente insieme all'azione da eseguire
### 3.3.1 Esempio Erratic Vacuum World
Non deterministic action
- Se lo stato corrente Ã¨ sporco, "suck" pulisce il quadrato corrispondente ma avvolte pulisce anche il quadrato adiacente
- Se lo stato corrente Ã¨ pulito, "suck" puÃ² depositare la sporcizia

"Left" e "Right" sono deterministiche

**Belief state**
- $Result(s,a)=?$
**Conditional plan**
- Â«suckÂ»; `if 5 do [Â«rightÂ», Â«suckÂ»]`
- Starting from state 1
- $Result(1, Â«suckÂ»)= \{5, 7\}$
> In questo modo se "suck" pulisce entrambi io avrÃ² che `if 5` non sarÃ  rispettato quindi faccio un altra azione
### 3.3.2 AND-OR Search Tree
- Necessario per evitare loop
- L'algoritmo Ã¨ leggermente piÃ¹ coprente ma l'albero si espande ancora
- Esiste una versione di $A^*$ per l'albero AND-OR

**Solzuione**
Un sotto-albero dove:
-  ogni foglia Ã¨ un goal
- Ha un azione per ogni nodo OR
- Include un ramo per tutti i risultati dei nodi AND
![[image-4.png|465x412]]

## 3.4 Searching Senza Osservazioni
- Non ci sono percezioni
- Senza sensori -> *Conformant Problem*

> L'agente deve stimale lo stato
> **Soluzione** -> Ã¨ una sequenza di azioni
> - Non puÃ² essere un conditional plan se non puoi ricevere un feedback dall'ambiente (non posso fare `if`)

**Belief Space** -> lo spazio delle rappresentazioni interne di ciÃ² che credo sia vero sul mondo (non certe)

Le azioni possono portare informazioni su possibili stati futuri
- *belief state* $= \{1,2,3,4,5,6,7,8\}$, *action*$=Â«rightÂ»$, *next belief state* $= ?$
- $\{2, 4, 6, 8\}$
#### Search in Conformant Problem
> Idea -> cercare nel belief space (completamente osservabile) invece che nello spazio degli stati (non osservabile)

- Belief state space = powerset of states -> 2ğ‘› possible beliefs instead of N
- Initial belief state = all the N states
- Le azioni *Actions(B)* che posso fare sono limitate dall'unione e l'intersezione di tutte le azioni *Actions(S)* pr ogni $S$ in $B$
- Transition model (for deterministic actions): Quando esegui unâ€™azione, non vai in un singolo stato, ma in unÂ **nuovo belief state**.
- **Goal Test**
	- **Possibly achieve**Â il goal: se almeno uno degli stati nel belief Ã¨ un goal.
	- **Necessarily achieve**Â il goal: seÂ _tutti_Â gli stati del belief sono goal.
- **Action cost**: Se lâ€™azione costa in modo diverso nei vari stati, allora devi trovare un criterio
	- Questo serve per decidere quanto â€œvaleâ€ unâ€™azione in un belief state.

##### Check Visited States
###### Come controllare gli stati visitati?
- Esempio:
	- ğµ1 = {5, 7}
	- ğµ2 = {1, 3, 5, 7}
- PuoiÂ **potare**Â (cioÃ¨ scartare) i rami di ğµ2, perchÃ© qualsiasi soluzione valida per $\{1, 3, 5, 7\}$ sarÃ  anche una soluzione per $\{5, 7\}$.
ğŸ‘‰ In generale: puoi scartare i rami che appartengono aÂ **superset**Â di belief state giÃ  visitati.
###### Ragionamento inverso
- Se hai giÃ  risolto il problema per un certo belief state, allora lo hai risolto anche per qualsiasiÂ **sottoinsieme**Â di esso.
###### ComplessitÃ 
- Nonostante queste ottimizzazioni, la ricerca nei problemi conformant (cioÃ¨ senza osservazioni, dove lâ€™agente non sa esattamente dove si trova) rimane molto costosa, perchÃ© lo spazio dei belief state Ã¨ enorme.
###### Ricerca incrementale nei belief state
1. Trova una soluzione che funziona per ilÂ **primo stato**Â in B.
2. Controlla se quella soluzione funziona anche per gli altri stati in B.
    - Se sÃ¬ â†’ fermati.
    - Se no â†’ riprova.
3. **Fallire velocemente**Â (cioÃ¨ accorgersi presto se una soluzione non funziona) puÃ² migliorare la velocitÃ  di convergenza
# 4 CSP - Problema di Soddisfazione del Vincolo
> **C**onstraint **S**atisfaction  **P**roblem

Introduciamo la rappresentazione degli stati come rappresentazione fattoriale
- Stato -> $X=\{X_1,\ X_2,\ ...,\ X_n\}$ con $n$ variabili
- Dominio -> $X_i\in D_i$ 
- La **conoscenza del dominio** Ã¨ espressa con un set di **vincoli C**
	- Es. $X_1\le 0,\ \ X_2\in [1,3], ...$ 


> [!info] $CSP(X, D, C)$
> **Goal Test** -> Assegno valori alle variabili per soddisfare i vincoli
> - **Completo vs Assegnazione Parziale** -> tutte o solo una parte delle variabili hanno un valore assegnato
> - **Assegnamento Consistente** -> Assegnare variabili soddisfa le condizioni
> - **Soluzione Parziale** -> un assegnamento parziale che Ã¨ consistente
> 
> Obbiettivo CSP Ã¨ trovare una soluzione completa e consistente


> [!important]  Factored Representation
> State Rappresentation -> Qui rappresentiÂ **ogni stato possibile come unâ€™unica entitÃ  indivisibile**Â (un â€œpuntoâ€ nello spazio degli stati).
>  
> Factored Representation -> Qui inveceÂ **uno stato non Ã¨ rappresentato come un singolo nodo â€œpiattoâ€**, ma comeÂ **una tupla di valori di variabili**:
> $$\text{Stato }S=(X_1â€‹=v_1â€‹,\ X_2â€‹=v_2â€‹,\ â€¦,\ X_nâ€‹=v_nâ€‹)$$
> Quindi:
> - Invece di rappresentareÂ **esplicitamente tutti gli stati**,
> - **rappresenti le variabili e i vincoli**Â che definiscono implicitamente lo spazio degli stati.


### 4.1.1 Esempio - Map Coloring
*Obbiettivo* -> colorare gli stati adiacenti con un colore diverso
![[image-9.png|274x226]]
Quindi abbiamo:
- $X = \{WA,\ NT,\ SA,\ Q,\ NSW,\ V,\ T\}$
- $D ={r,\ g,\ b}\ \ \forallğ‘‹_ğ‘–$
- $C =$
	- $WA â‰  ğ‘ğ‘‡$
	- $ğ‘Šğ´,\ ğ‘ğ‘‡ âˆˆ { ğ‘Ÿ,\ ğ‘” ,\ ğ‘Ÿ,\ ğ‘ ,\ ğ‘”,\ ğ‘Ÿ ,\ ğ‘”,\ ğ‘,\ â€¦ }$
	- Entrambi -> ripetere per ogni coppia di stati

![[image-10.png|274x242]]
> Le variabili sono i nodi e gli archi sono i vincoli
> Il grafico aiuta la ricerca, come possiamo vedere dal nodo "Tasmania" che non ha vincoli quindi possiamo assegnarli un colore casuale

| **Dominio**                    | **Vincoli**    | **RisolvibilitÃ **                                         |
| ------------------------------ | -------------- | --------------------------------------------------------- |
| Discreto finito                | Qualsiasi      | Decidibile (es. backtracking, CSP classici)               |
| Discreto infinito              | Lineari o meno | Difficile â†’ servono metodi speciali o bounds              |
| Discreto + vincoli lineari     | Lineari        | Solvibile (anche se NP-hard)                              |
| Discreto + vincoli non lineari | Non lineari    | **Indecidibile in generale**                              |
| Continuo + vincoli lineari     | Lineari        | **Solvibile in modo efficiente**Â (programmazione lineare) |
| Continuo + vincoli non lineari | Non lineari    | PiÃ¹ complesso; decidibilitÃ  dipende dal tipo di vincolo   |
## 4.2 Tipi di Vincoli
- **Unitario**
	- Una variabile coinvolta -> $A\ne 1$
- **Binario**
	- Due variabili coinvolte -> $A\le B$
- **Globale** -> vincoli con $n$ variabili coinvolte
	- Non necessariamente tutte le variabili
	- Alldif$(A,\ B,\ C,\ D)$
- **Vincoli di Preferenza**
	- Vincoli leggeri che rappresentano le preferenze
	- Modellato con un costo associato a ciascun incarico -> problema di ottimizzazione vincolato
	- Es. Linear Model

### 4.2.1 Vincoli Hyper-Graph
Non posso rappresentare i vincoli come archi dato che per definizione gli archi connettono solo due nodi
![[image-11.png]]
- **Nodi:**Â I cerchi (variabili lettera:Â F,T,U,W,R,O) e i quadrati (variabili di riporto:Â C1â€‹,C2â€‹,C3â€‹).
- **Archi:**Â Le linee che collegano i nodi rappresentano le relazioni o i vincoli tra le variabili definite dalle equazioni.

> Per vincoli con dominio finito, puoi sempre ridurre un hyper-graph in un grafico normale
> - vincoli globali possono essere divisi in un set di vincoli binari

Qualche vincolo popolare (es. Alldiff) gode di algoritmi studiati appositamente
- Non serve ridurre il grafo a vincoli binari
- Sono piÃ¹ facili da leggere da una prospettiva umana

## 4.3 Come Cercare in CSP
**Gli stati sono definiti dai valori assegnati finora**
*Initial State - stato iniziale:*
- Assegnazione vuota -> $\{\}$
- Fattore di ramificazione alla radice -> $=nd$

*Azioni:*
- Assegna un valore alle variabili non assegnate che non causa un conflitto con l'assegnamento corrente
- Fattore di ramificazione al secondo livello -> $=(n-1)d$

*Ogni soluzione completa appare alla profonditÃ  $n$:*
- Problema -> $n!$ o $d^n$ foglie ([[#^6f1c56|Riduce ComplessitÃ ]])
- Risoluzione -> Sfrutta la struttura del problema 

*Assegnamenti illegali* -> ritornano fallimento - non risolvibile
*Goal Test* -> controlla se l'assegnamento attuale Ã¨ completo

### 4.3.1 Backtracking Search

> Fissa un ordine tra gli stati -> rimuove la complessitÃ  $n!$
> Ossia -> Non câ€™Ã¨ bisogno di considerare tutti gli ordini delle variabili: possiamoÂ **fissare a priori un unico ordine**, e assegnare le variabili sempre in quellâ€™ordine.


> [!attention] Assegnazione Stati vs Singolo Nodo
> Se tu considerassiÂ **ogni stato come una possibile sequenza parziale di assegnamenti senza un ordine fisso tra le variabili**, allora:
> - Devi esplorareÂ **tutte le possibili permutazioni delle variabili**
> - Ci sonoÂ $n!$Â possibili ordini diversi in cui potresti assegnarle.
> 
> Invece di avereÂ **ogni nodo come â€œuna configurazione interaâ€ di n variabili**, ogni nodo nella ricerca corrisponde aÂ **â€œassegnare un valore a una singola variabileâ€**, secondo lâ€™ordine prefissato. ^6f1c56

**Ogni nodo Ã¨ considerato un singolo assegnamento** -> non l'intero stato
- solamente $d^n$ foglie 
- Costruisco un albero con ogni nodo che Ã¨ un assegnazione di una singola variabile non di un assegnazione di piÃ¹ variabili

Algoritmo non informato per CSP
- Depth-first co assegnamento a variabile singola
- La soluzione Ã¨ ancora un cammino nell'albero


> [!seealso] Logica Algoritmo
> Sceglie ripetutamente una variabile non assegnata, e poi prova tutti i valori nel dominio di quella variabile a turno, cercando di estendere ciascuno in una soluzione tramite una chiamata ricorsiva. Se la chiamata ha successo, la soluzione viene restituita e, se fallisce, l'assegnazione viene ripristinata allo stato precedente e proviamo il valore successivo. Se nessun valore funziona, restituiamo il fallimento.

![[image-12.png|536x438]]
- Con stati atomici, gli algoritmi non informati non sfruttano l'euristica
- Nella rappresentazione fattorizzata esistono euristiche indipendenti dal dominio
	- Possono migliorare la velocitÃ  di ricerca

## 4.4 Miglioramenti 
- [[#4.4.2 Miglioramenti Tramite Inferenza|4.4.2 Miglioramenti Tramite Inferenza]]
### 4.4.1 Miglioramenti di Ricerca
- [[#MEVs - Minimum Remaining Values|MEVs - Minimum Remaining Values]]
- [[#Degree Heuristic - Gradi di Euristica|Degree Heuristic - Gradi di Euristica]]
- [[#Least-Constraint Values|Least-Constraint Values]]

#### MEVs - Minimum Remaining Values
> Scegli la variabile con meno valori legali -> utilizza quella per avere un approccio "**prima fallimenti**" cosÃ¬ da fare una specie di **pruning** sui nodi con meno speranze cosÃ¬ da eliminarli subito

#### Degree Heuristic - Gradi di Euristica
>LaÂ **degree heuristic**Â sceglie, tra le variabili non ancora assegnate,Â **quella che Ã¨ coinvolta nel maggior numero di vincoli con altre variabili non assegnate**, cosÃ¬ da massimizzare la riduzione futura del branching.  
>Serve spesso comeÂ **criterio di spareggio**Â quando piÃ¹ variabili hanno lo stesso numero di valori legali (dopo la MRV).

#### Least-Constraint Values
> Questa Ã¨ la descrizione dellâ€™**euristica LCV (Least Constraining Value)**Â ğŸ‘¨â€ğŸ«
> - **Idea**: quando scegli il valore per una variabile, assegnaÂ **quello che elimina il minor numero di valori possibili per le altre variabili ancora non assegnate**, cosÃ¬ daÂ **lasciare piÃ¹ libertÃ  futura**.
> - **Differenza con MRV**:
> 	- **MRV (Minimum Remaining Values)**Â sceglieÂ _quale variabile_Â assegnare â†’ cerca di fallire presto per potare lâ€™albero.
> 	- **LCV (Least Constraining Value)**Â sceglieÂ _quale valore_Â assegnare â†’ cerca diÂ **non restringere troppo**Â il resto del problema, per non imboccare precocemente un vicolo cieco.
> 
> ğŸ‘‰ Entrambe accelerano la ricerca, maÂ **da prospettive opposte**: MRV anticipa i fallimenti, LCV mantiene aperte le possibilitÃ .

![[image-13.png]]

### 4.4.2 Miglioramenti Tramite Inferenza
- [[#Forward checking|Forward checking]]
- [[#Arc Consistency|Arc Consistency]]

#### Forward checking
Assegna un valore aÂ $X$  
Per ogni vincolo traÂ $X$Â eÂ $Y$:  
â†’Â **elimina**Â dal dominio diÂ $Y$Â tutti i valori che violano il vincolo con $X$.  
Se il dominio di Y rimane vuoto â†’Â **backtrack!**
- Significa che lâ€™assegnamento precedente era errato.
- Esistono molte strategie di backtracking

ğŸ‘‰ Questo Ã¨ un esempio diÂ **inferenza nei CSP**:
- Non si fa ricerca, siÂ **riduce lo spazio delle possibili assegnazioni**.
- Lâ€™inferenza puÃ² essere eseguitaÂ **prima**Â eÂ **durante**Â la ricerca.
	- Ad esempio,Â **prima della ricerca**Â si puÃ² imporre laÂ **coerenza di stato**Â eliminando i valori di dominio che violano vincoliÂ **unari**.

![[image-14.png]]

#### Arc Consistency
**coerenza dâ€™arco**
- UnaÂ **variabile X Ã¨ arc-consistent**Â rispetto a unâ€™altra variabile $Y$ se:  
$$\forall x \in \text{dom}(X), \ \exists y \in \text{dom}(Y) \ \text{tale che} \ (x,y) \ \text{rispetta il vincolo tra X e Y}.$$

ğŸ‘‰ In altre parole:Â **ogni valore nel dominio di X deve avere almeno un valore compatibile nel dominio di Y**.   
- UnÂ **grafo Ã¨ arc-consistent**Â seÂ **tutte le variabili sono arc-consistent rispetto a tutte le altre**Â (cioÃ¨ per ogni arco Xâ†’Y vale la condizione sopra).

> [!tip] Grafo Arc-Consistency
> - **Per ogni valore possibile di $X$**, deve esserciÂ **almeno un valore compatibile in $Y$**.
> - Se esiste un valoreÂ $x$Â che non ha nessunÂ $y$Â compatibile, allora quelÂ $x$Â va eliminato dal dominio diÂ $X$.
> 
> Quando elimini un valoreÂ xxÂ dalÂ **dominio di una variabile X**Â tramite AC-3 o altra inferenza
> - StaiÂ **riducendo lo spazio di ricerca**, perchÃ© quellâ€™assegnamentoÂ **non verrÃ  mai esplorato**Â durante il backtracking.
> - QuestoÂ **non elimina variabili**, elimina solo possibili valori per ciascuna variabile.

##### âš™ï¸Â Come rendere un grafo arc-consistent: algoritmo AC-3
Lâ€™algoritmoÂ **AC-3**Â Ã¨ un procedimento di inferenza cheÂ **rende il CSP arc-consistente**, oppure segnala fallimento se qualche dominio si svuota.

**Passi principali:**
1. **Inizializza una coda**Â con tutti gli archi (X â†’ Y) del CSP.
2. **Ripeti finchÃ© ci sono cambiamenti**:
    - Estrai un arco X â†’ Y dalla coda.
    - **Rendi X arc-consistent rispetto a Y**:
        - Elimina dal dominio di X tutti i valori x per cuiÂ **non esiste nessun y nel dominio di Y compatibile**con x.
    - Se il dominio di X Ã¨ stato modificato
        - Aggiungi alla codaÂ **tutti gli archi K â†’ X**, cioÃ¨ quelli che puntano verso X, perchÃ© il cambiamento potrebbe renderli incoerenti.
    - Se il dominio di una variabile si svuota â†’Â **fallimento**Â (il CSP non ha soluzione).
##### â±ï¸Â ComplessitÃ 
Per un CSP con:
-  **c**Â = numero di archi binari
- **d**Â = dimensione massima dei domini
ğŸ‘‰ La complessitÃ  Ã¨Â **O(c Â· dÂ³)**

- **Ogni arco**Â puÃ² essere controllato inÂ **O(dÂ²)**Â (perchÃ© bisogna confrontare ogni valore di X con ogni valore di Y).
- Ogni arco puÃ² essere reinserito nella coda al massimoÂ **d volte**, perchÃ© ogni variabile puÃ² perdere al massimo d valori â†’ da quiÂ **O(c Â· dÂ³)**.

##### ğŸ“Â Riassunto finale
- **Arc-consistency**Â = ogni valore di ogni variabile ha un supporto compatibile nei vicini.
- **AC-3**Â = algoritmo di inferenza che:
    - non fa ricerca,
    - riduce i domini eliminando valori impossibili,
    - puÃ² essere eseguito prima o durante la ricerca per potare fortemente lo spazio
- Se dopo AC-3 qualche dominio Ã¨ vuoto â†’Â **il CSP non Ã¨ risolvibile**.

#### Maintaining Arc Consistency
MAC (Maintaining Arc-Consistency)Â **estende il Forward Checking**Â chiamando AC-3 dopo ogni assegnamento durante la ricerca.
- Dato un assegnamento a ($X$), si chiama AC-3 con una coda contenente tutti gli archi ($Y \to X$), per tutte le variabili ($Y$) ancora non assegnate.
- Se AC-3 fallisce â†’Â **backtrack**.

MACÂ **potatura piÃ¹ aggressiva rispetto al Forward Checking**, perchÃ© verifica ricorsivamente eventuali incoerenze.

## 4.5 Local Search for CSP
Formulazione aÂ **stato completo**: ogni stato assegna un valore aÂ **tutte le variabili**.
- La ricerca cambia il valore diÂ **una variabile alla volta**.
    - Quindi, lo stato iniziale Ã¨ unÂ **assegnamento casuale di tutte le variabili**.

**Euristica Min-Conflicts**: scegli il valore cheÂ **violerebbe il minor numero di vincoli**, cosÃ¬ da avvicinarti alla soluzione.
- Ãˆ possibileÂ **pesare diversamente i vincoli**Â per dare prioritÃ  a quelli piÃ¹ importanti,
    - perchÃ© la soluzione finale potrebbe non essere ottimale rispetto a tutti i vincoli.
### 4.5.1 Min-Conflict Heuristic
**Euristica Min-Conflicts**: scegli il valore cheÂ **violerebbe il minor numero di vincoli**.
- Funziona molto bene,Â **tranne in una regione critica**.
    - Ad esempio, risolve il problema delleÂ **N-queens**Â in un numero di passi quasi costante, indipendentemente daÂ $N$Â (circa 50 passi).
### 4.5.2 Sfruttiamo la Struttura del Problema
Anche avendoÂ **un arsenale di milioni di trucchi**Â (e ce ne sono molti altri!), iÂ **CSP rimangono comunque molto difficili in generale**.
- Caso peggiore: ( $d^n$ ) foglie nellâ€™albero di ricerca (come abbiamo visto).
#### Scomposizione in sottoproblemi indipendenti
- Alcuni problemi possono essereÂ **divisi in sottoproblemi indipendenti**, ad esempio â€œTâ€ e â€œmainlandâ€.
- GliÂ **algoritmi sui grafi**Â possono identificare questeÂ **sottostrutture nel grafo dei vincoli**, come leÂ **componenti connesse**.
- LeÂ **componenti connesse**Â sono indipendenti tra loro, quindi laÂ **soluzione complessiva**Â del CSP Ã¨ semplicementeÂ **lâ€™unione delle soluzioni di ciascuna componente**.
#### âš¡ Vantaggio computazionale

- Se una componente ha $c < n$ variabili, allora la ricerca su quella componente scala come $d^c$ , invece di $d^n$.
- In altre parole,Â **scomporre il problema in sottoproblemi riduce drasticamente la complessitÃ **, sfruttando lâ€™indipendenza tra le variabili.

### 4.5.3 Tree-structured CSPs
Se ilÂ **grafo dei vincoli Ã¨ un albero**, risolvere il CSP costa $O(n d^2)$
- Lineare in $n!$

Dato unÂ **grafo dei vincoli senza cicli**Â (un albero), lâ€™algoritmo di ricerca procede cosÃ¬:
1. **Ordinamento topologico**: scegli una qualsiasi variabile come radice e ordina le variabili in modo che ciascuna sia figlia della propria variabile genitore.
2. **Rendi lâ€™albero arc-consistente**; se fallisce â†’ termina con fallimento.
    - Ci sono ( n-1 ) archi e ( d^2 ) combinazioni di valori da controllare per ciascun arco.
3. **Assegna a ogni nodo**Â un qualsiasi valore consistente con quello del genitore; se non esiste â†’ fallimento

> In questo casoÂ **non serve mai fare backtracking**.

![[image-15.png]]

### 4.5.4 ğŸŒ³ â€œPiantare alberi dove non ce ne sonoâ€

- Possiamo risolvereÂ **rapidamente CSP strutturati ad albero**.
- Ma cosa succede se il CSPÂ **non Ã¨ un albero**? Possiamo provare aÂ **forzarlo a diventarlo**.
#### Procedura per trasformare un CSP in albero
1. **Assegna un valore a una variabile**Â (scegli un nodo).
2. **Rendi gli archi coerenti**Â (arc-consistency).
3. **Rimuovi la variabile assegnata**

- Se il grafo risultanteÂ **puÃ² essere trasformato in un albero**, allora puoiÂ **verificare rapidamente se il CSP Ã¨ risolvibile**.
- Altrimenti, puoi provareÂ **un altro valore per la variabile**Â oÂ **scegliere un altro nodo da assegnare**.
#### âš¡ Cutset Conditioning

- Supponiamo di dover provare ( c ) variabili (ilÂ **cutset**), cioÃ¨ quelle che â€œrompono i cicliâ€ del grafo
- ComplessitÃ  approssimativa:  
$$
    O(d^c (n-c) d^2)  
$$
- Qui:
    - $d^c$ = tutte le combinazioni di valori per le variabili nel cutset
    - $n-c$ = numero di variabili rimanenti che ora formano un albero
    - $d^2$ = costo per verificare la coerenza sugli archi rimanenti

- In pratica, riduce drasticamente il problema, perchÃ©Â **la parte ciclica viene gestita separatamente**, mentre il resto Ã¨ un albero â†’ risolvibile in tempo lineare.
![[image-16.png]]
## 4.6 Riassunto

### 4.6.1 I. Fondamenti dei Constraint Satisfaction Problems (CSP)

#### 5.1.1 Definizione e Struttura dello Stato

- **Stato Fattorizzato:**Â Lo stato $X$ Ã¨ rappresentato da un insieme di $n$ variabili ${X_1, X_2, \dots, X_n}$.
- **Dominio ($D_i$):**Â Ogni variabile $X_i$ assume valori da un dominio $D_i$.
- **Vincoli ($C$):**Â La conoscenza del dominio Ã¨ espressa da un insieme di vincoli che devono essere soddisfatti (es. $X_1 \le 0$, $X_2 \in {1, 3}$).
- **Obiettivo del CSP:**Â Trovare unaÂ **soluzione completa e consistente**.
    - **Assegnazione Completa:**Â Tutte le variabili hanno un valore assegnato.
    - **Assegnazione Parziale:**Â Solo alcune variabili hanno un valore assegnato.
    - **Assegnazione Consistente:**Â Le variabili assegnate soddisfano i vincoli.
    - **Soluzione Parziale:**Â Un'assegnazione parziale che Ã¨ consistente.

#### 4.5.2 Rappresentazione dei Vincoli

- **Grafo dei Vincoli:**Â Le variabili sono rappresentate comeÂ **nodi**Â e i vincoli sono rappresentati comeÂ **archi**. La struttura del grafo puÃ² aiutare la ricerca.
- **Vincoli Comuni:**
    - **Vincoli Unari:**Â Coinvolgono una sola variabile (es. $A \ne 1$).
    - **Vincoli Binari:**Â Coinvolgono due variabili (es. $A \le B$).
    - **Vincoli Globali:**Â Coinvolgono un numero qualsiasi di variabili (non necessariamente tutte), es. $Alldiff(A, B, C, D)$.
- **Iper-Grafo dei Vincoli:**Â Necessario quando ci sono vincoli globali, poichÃ© un arco per definizione connette solo due nodi.
    - **Riduzione:**Â Per i vincoli con dominio finito, un iper-grafo puÃ² sempre essere ridotto a un grafo normale suddividendo i vincoli globali in un insieme di vincoli binari. Questo Ã¨ utile se si vogliono usare algoritmi per soli vincoli binari.
- **Vincoli di Preferenza (Soft Constraints):**Â Rappresentano una preferenza (es. se possibile, assegna questo rispetto a quello). Sono modellati tramite un costo associato a ciascuna assegnazione (diventando un problema di ottimizzazione vincolata).

### 4.6.2 II. Tecniche di Ricerca di Soluzioni
- [[#A. Framework di Ricerca (Stato Fattorizzato)|A. Framework di Ricerca (Stato Fattorizzato)]]
- [[#B. Ricerca Non Informata: Backtracking Search|B. Ricerca Non Informata: Backtracking Search]]
- [[#C. Miglioramenti tramite Euristiche (Domain-Independent)|C. Miglioramenti tramite Euristiche (Domain-Independent)]]
- [[#D. Ricerca Locale (Local Search)|D. Ricerca Locale (Local Search)]]
#### A. Framework di Ricerca (Stato Fattorizzato)

- **Stati:**Â Definiti dai valori assegnati finora.
- **Stato Iniziale:**Â L'assegnazione vuota ${\ }$.
- **Azioni:**Â Assegnare un valore a una variabile non assegnata che non Ã¨ in conflitto con l'assegnazione corrente.
- **ProfonditÃ  della Soluzione:**Â Ogni soluzione completa appare alla profonditÃ  $n$.
- **Problema di ComplessitÃ :**Â Il numero di foglie (senza ottimizzazioni) Ã¨ $n! d^n$.

#### B. Ricerca Non Informata: Backtracking Search

- **Algoritmo:**Â Backtracking Search Ã¨ essenzialmente una ricerca in profonditÃ  (Depth-First Search) con assegnazione a variabile singola.
- **Fissare l'Ordine:**Â Fissando un ordine per le variabili si elimina la complessitÃ  $n!$, riducendo il numero di foglie a $d^n$.
- **Processo:**Â Sceglie ripetutamente una variabile non assegnata, prova tutti i valori nel suo dominio, ed estende ricorsivamente l'assegnazione. Se la chiamata fallisce, l'assegnazione viene ripristinata e si prova il valore successivo. Se nessun valore funziona, restituisce fallimento.

#### C. Miglioramenti tramite Euristiche (Domain-Independent)

Nello stato fattorizzato esistono euristiche indipendenti dal dominio che velocizzano la ricerca.

- **1. Minimum Remaining Values (MRVs):**
    
    - **Scelta:**Â Scegliere la variabile con ilÂ **minor numero di valori legali**Â (possibili).
    - **Logica:**Â Cerca il fallimento il prima possibile (_Failure-First_) in modo che la potatura (_pruning_) avvenga prima.
- **2. Degree Heuristic:**
    
    - **Scelta:**Â Scegliere la variabile con ilÂ **maggior numero di vincoli**Â tra le variabili ancora da assegnare.
    - **Uso:**Â Serve comeÂ _tie-breaker_Â per variabili con lo stesso numero di mosse legali (stesso MRV).
- **3. Least-Constraining Values (LCVs):**
    
    - **Scelta:**Â Scegliere un valore che escluda ilÂ **minor numero di valori**Â nelle altre variabili ancora da assegnare.
    - **Logica:**Â Mantiene aperte le opzioni, cercando di evitare percorsi stretti che potrebbero essere sbagliati.

#### D. Ricerca Locale (Local Search)

- **Formulazione a Stato Completo:**Â Ogni stato assegna un valore aÂ _ogni_Â variabile.
- **Inizio:**Â Lo stato iniziale Ã¨ un'assegnazione casuale di tutte le variabili.
- **Azione:**Â La ricerca cambia il valore di una sola variabile alla volta.
- **Heuristica Min-Conflicts:**
    - **Scelta:**Â Scegliere il valore cheÂ **rompe il minor numero di vincoli**.
    - **Obiettivo:**Â Avvicina lo stato alla soluzione.
    - **Efficacia:**Â Funziona molto bene; ad esempio, risolve il problema delle N-regine in un numero costante di passi, indipendentemente da $N$.

### 4.6.3 III. Tecniche di Inferenza e Propagazione dei Vincoli

L'inferenza non Ã¨ una ricerca, ma riduce lo spazio di possibili assegnazioni. PuÃ² essere eseguita prima o durante la ricerca.

#### Forward Checking (FC)

- **Funzionamento:**Â Dopo aver assegnato un valore a una variabile $X$, per ogni vincolo $X-Y$, siÂ **eliminano i valori dal dominio di $Y$**Â che violano il vincolo.
- **Azione in caso di fallimento:**Â Se non rimangono valori in un dominio, si esegue ilÂ _backtrack_, poichÃ© l'assegnazione precedente era sbagliata.

#### Arc Consistency (AC)

- **Definizione:**Â Una variabile $X$ Ã¨Â **arc-consistent**Â se per ogni arco $X \to Y$, per ogni valore $x$ nel dominio di $X$, esiste almeno un valore $y$ nel dominio di $Y$ che Ã¨ permesso.
- **Grafo Arc-Consistent:**Â Un grafo Ã¨ arc-consistent se tutte le variabili sono arc-consistent.

#### Algoritmo AC-3

- **Scopo:**Â Rende un grafo arc-consistent o restituisce fallimento.
- **Processo:**
    1. Creare una coda con tutti gli archi.
    2. Ripetere finchÃ© non ci sono piÃ¹ cambiamenti:
        - Selezionare un arco casuale $X \to Y$ e rendere $X$ arc-consistent,Â **riducendo il dominio di $X$**.
        - Se viene rimosso un valore da $X$,Â **aggiungere tutti gli archi $K \to X$ alla coda**Â (perchÃ© $K$ potrebbe non essere piÃ¹ consistente rispetto al nuovo $X$).
        - Se una variabile rimane senza assegnazioni possibili, fallire.
- **ComplessitÃ :**Â $O(cd^3)$, dove $c$ Ã¨ il numero di archi binari e $d$ Ã¨ la dimensione del dominio.

#### Maintaining Arc Consistency (MAC)

- **Funzionamento:**Â MAC aumenta ilÂ _forward checking_Â chiamando AC-3 dopo ogni assegnazione durante la ricerca.
- **Invocazione:**Â Data un'assegnazione a $X$, MAC chiama AC-3 con una coda di archi $Y \to X$ (per tutte le $Y$ non ancora assegnate).
- **Potatura:**Â MAC pota piÃ¹ delÂ _forward checking_Â perchÃ© verifica ricorsivamente le incoerenze.

### 4.6.4 IV. Sfruttare la Struttura del Problema (Semplificazione)

Anche con molte euristiche, i CSP rimangono difficili in generale (worst-case $d^n$). Sfruttare la struttura puÃ² portare a grandi semplificazioni.

#### Componenti Connesse Indipendenti

- **Identificazione:**Â Gli algoritmi sui grafi possono identificare sottostrutture (componenti connesse) nel grafo dei vincoli.
- **Vantaggio:**Â I componenti connesse sono sottoproblemi indipendenti.
- **Soluzione:**Â La soluzione Ã¨ l'unione delle soluzioni dei componenti.
- **ScalabilitÃ :**Â La complessitÃ  scala come $d^c$, dove $c < n$ sono le variabili coinvolte nel sottoproblema.

#### CSPs a Struttura ad Albero (Tree-Structured CSPs)

- **Definizione:**Â Se il grafo dei vincoli Ã¨ un albero (senza cicli), il CSP Ã¨ notevolmente piÃ¹ facile.
- **Costo:**Â La risoluzione costa $O(nd^2)$, che Ã¨Â **lineare**Â in $n$ (numero di variabili).
- **Algoritmo (Nessun Backtrack):**
    1. **Ordinamento Topologico:**Â Scegliere una variabile come radice e ordinare le variabili in modo che ciascuna sia figlia del suo genitore.
    2. **Arc-Consistency:**Â Rendere l'albero arc-consistent (o fallire).
    3. **Assegnazione:**Â Assegnare a ogni nodo un valore consistente con il suo genitore (o fallire).

#### Cutset Conditioning (Piantare alberi dove non ci sono)

- **Scopo:**Â Forzare un CSP a diventare strutturato ad albero.
- **Processo:**Â Si assegna un valore a un sottoinsieme di variabili, chiamatoÂ **cutset**Â ($c$ variabili).
- **Vantaggio:**Â Se il grafo risultante (dopo aver rimosso e assegnato il cutset) puÃ² essere trasformato in un albero, si puÃ² verificare rapidamente se Ã¨ risolvibile.
- **ComplessitÃ :**Â $O(d^c (n-c) d^2)$. L'efficacia dipende dalla dimensione $c$ delÂ _cutset_.

# 5 Games
> **Competitive Game** -> Ricerca contraddittoria (*adversarial search*)
> - Giocatori fanno a turno
> - I giocatori hanno goal opposti, in contrasto
> - Somma zero -> uno vince l'altro perde
> - **Perfettamente informato -> fully observable**

- Stato iniziale -> $S_0$
- $\text{TO-MOVE}(s)$ -> Mossa per spostarsi nello stato $s$
- $\text{ACTIONS}(s)$ -> mosse legali per lo stato $s$
- $\text{RESULT}(s,\ a)$ -> dopo lo spostamento (transition model) il risultato Ã¨ il nuovo stato
- $\text{IS-TERMINAL}(s)$ -> il gioco Ã¨ finito o no
- $\text{UTILITY}(s;\ p)$ -> Funzione di utilitÃ  - payoff ottenuta su uno stato terminale $s$ dal giocatore $p$ -> ossia Ã¨ lo stato finale

![[image-17.png|559x174]]

## 5.1 Game Tree -> Min-Max Problem
> Stesse proprietÃ  della ricerca ad albero 
> - PuÃ² essere infinita se lo spazio degli stati Ã¨ infinito
> - PuÃ² essere infinito se lo spazio degli stati Ã¨ finito ma permette ripetizioni degli stati (posizioni)

Il grafico dello spazio degli stati ha i nodi che rappresentano gli stati e gli archi che rappresentano le possibili azioni
- Lo stesso stato puÃ² essere raggiunto da piÃ¹ direzioni - *path*
- The game tree Ã¨ *sovrapposto* al grafico -> nel gioco la radice Ã¨ la posizione di partenza

Non c'Ã¨ speranza di costruire l'intero game tree per qualsiasi gioco anche di dimensione ragionevole
-  $< 9!$ states for tictactoe $â‰ˆ 360 000$ states -> insostenibile
![[image-18.png|299x242]]

### 5.1.1 MIN - MAX
> La strategia per ogni giocatore Ã¨ un *piano condizionato* (**conditional plan**)
> - *Si puÃ² applicare a qualsiasi gioco deterministico con informazione perfetta*
> - Cosa fa MAX se MIN fa qualcosa?


Se il gioco ha solo come risultato win/lose
- Allora abbiamo una ricerca in un ambiente non deterministico
- Le mosse dell'avversario sono modellate come una distribuzione di probabilitÃ 
- Win=goal, lose = not goal
- **AND-OR albero di ricerca**

Se il risultato ha piÃ¹ valori -> **minmax**
- Ãˆ perfetto per giochi deterministici e con informazioni perfette

![[image-21.png|551x150]]
![[image-19.png|551x306]]

#### Minmax Value
Una volta ottenuto il valore minimo massimo per ogni nodo, puoi recuperare la strategia ottimale

Nelle foglie (nodi terminali) -> $\text{MINIMAX}(s) = \text{UTILITY}(s;\ p)$
- Il valore di utility -> ossia valore del risultato finale

Un nodo intermedio, non terminale -> $\text{MINIMAX}(s) =$ il valore dell'utility **per MAX**
- Ossia il valore **MINIMAX(s)** rappresenta il **valore di utilitÃ  garantito per MAX** se entrambi i giocatori (MAX e MIN) giocano in modo ottimale da quel nodo in poi.
- Ãˆ il miglior risultato che MAX puÃ² ottenere partendo daÂ $s$, considerando che MIN cercherÃ  di minimizzare il punteggio di MAX.

$$
\text{MINIMAX}(s)=\max_{ğ‘âˆˆ\text{ğ´ğ¶ğ‘‡ğ¼ğ‘‚ğ‘ğ‘†}(ğ‘ )}\ \text{MINIMAX}(\text{ğ‘…ğ¸ğ‘†ğ‘ˆğ¿ğ‘‡}(ğ‘ ,\ ğ‘ )) ,\ \text{TO-MOVE}(s) = \text{MAX}
$$
$$
\text{MINIMAX}(s)=\max_{ğ‘âˆˆ\text{ğ´ğ¶ğ‘‡ğ¼ğ‘‚ğ‘ğ‘†}(ğ‘ )}\ \text{MINIMAX}(\text{ğ‘…ğ¸ğ‘†ğ‘ˆğ¿ğ‘‡}(ğ‘ ,\ ğ‘ )) ,\ \text{TO-MOVE}(s) = \text{MIN}
$$
- MAX sceglie l'opzione che massimizza MINIMAX
	- Il punteggio di MAX dipende dalle mosse di min infatti come vediamo nella figura nel turno di MAX (root) il valore Ã¨ 3 perchÃ¨ considera che min faccia la scelta migliore e quindi MAX deve scegliere tra 3 e 2 che sono i valori minimi quando tocca a MIN
- MIN sceglie l'opzione che minimizza MINIMAX
	- Dato che ha l'obbiettivo opposto

> Utilizzando l'algoritmo Minimax:
> 1. **Esegui una ricerca in profonditÃ  (depth-first search) dalla radice**Â per trovare tutti gli stati terminali (foglie)
> 2. **Ottieni l'utilitÃ  delle foglie**Â (i valori numerici associati ai nodi terminali).
> 3. **Ripercorri all'indietro (backtrack) per calcolare il valore Minimax di tutti i nodi intermedi**:
    - Se il nodo genitore Ã¨ un nodoÂ **MIN**, prendi il valoreÂ **minimo**Â tra i valori Minimax dei figli.
    - Se il nodo genitore Ã¨ un nodoÂ **MAX**, prendi il valoreÂ **massimo**Â tra i valori Minimax dei figli.

ES:
Immagina un gioco dove MAX e MIN alternano mosse:
- MAX parte dalla radice e sceglie traÂ $A_1$â€‹,Â $A_2$â€‹,Â $A_3$â€‹.
- Se sceglieÂ $A_1$â€‹, vede 3, 12, 8, ma sa che MIN minimizzerÃ  a 3.
- SceglieÂ $A_1$Â perchÃ© 3 Ã¨ il massimo tra 3, 2, e 2 (valori diÂ $A_1$â€‹,Â $A_2$â€‹,Â $A_3$â€‹)


> [!important] MinMax Search
> - **Completa** -> **SI** con stati finiti
> - **Ottimale** -> **SI**, contro un giocatore perfetto
> - **ComplessitÃ  in Tempo** -> $O(b^m)$
> - **ComplessitÃ  Spazio** -> $O(bm)$ = Depth-First

#### Giocare contro un Avversario Sub-Ottimale
> Minimax ha dei leggeri inconvenienti

A seconda del giocatore puÃ² fare mosse perfette oppure come nella maggior parte dei casi sbagliare e quindi non avere un gioco perfetto.
Se io gioco contro di lui e se la posizione Ã¨ un pareggio con un gioco perfetto, potresti voler fare una mossa complicata, ma non ottimale per provare a prendere la vittoria:
- Se rispondo con l'unica mossa corretta nella posizione: perdi (p=1%)
- Se rispondo con alcune delle mosse sbagliate: vinci (p=85%)
- Se rispondo con il resto delle mosse (non ottime, ne pessime): Ã¨ un pareggio (p=14%)

Questa Ã¨ la differenza tra le cosiddette "mosse del computer" e le "mosse umane"
- Ci sono programmi per computer che cercano di imitare il modo in cui gli esseri umani si comportano contro gli avversari non ottimali

##### ComplessitÃ 
- Branching factor in chess = 35
- Average game length = 40 moves = 80 ply
- Given the complexity of DFS, you get 3580 states to visit
> Per ridurlo abbiamo bisogno del **PRUNING**

### 5.1.2 $\alpha$ - $\beta$ pruning
> Pota i rami che non portano a nessuna mossa migliore nÃ© per MAX nÃ© per MIN

$\alpha$ = valore MINIMAX piÃ¹ grande finora
$\beta$ = valore MINIMAX piÃ¹ piccolo finora
$[\alpha,\ \beta]$
Iniziale = $[-\infty,\ +\infty]$

![[image-22.png]]

**Regola generale**: considera un nodo con MINIMAX = V. 
- Se un nodo della stessa profonditÃ  (MINIMAX=m') o un nodo piÃ¹ alto (MINIMAX=m) ha un valore MINIMAX migliore, non raggiungerai mai il nodo con MINIMAX=V: **puoi potarlo!**


> [!question] PerchÃ¨ Pruning?
> Non ha effetto sulla soluzione
> Con il migliore pruning possiamo ridurre la complessitÃ  a $O(b^{m/2})$
> - Buono ma non sufficiente per problemi con grandezza ragionevole ($\sqrt b$ fattore su cui ha effetto)
> - Il miglior pruning dipende **dall'ordine delle mosse**
> - La generazione di mosse che consentono di potare prima l'albero porta alla migliore potatura -> faccio mosso che permettono di potare piÃ¹ rami

La ricerca del raggio puÃ² essere utilizzata solo per considerare le mosse "top" k ogni volta, in base ad alcune funzioni di valutazione
- Rischi di potare la mossa migliore, ovviamente.

#### Strategie diverse da DFS
- **Type A** strategy
	- Esplorare completamente lo spazio della ricerca fino ad una profonditÃ  limitata
	- Usare un euristica per valutare utility per i nodi alla profonditÃ  finale
- **Type B** strategy
	- Non esplorare le mosse che sembrano pessime basandosi su una data euristica
	- Segui le mosse piÃ¹ promettenti dove possibile -> possibilmente fino alla fine del gioco

Type A funziona se il fattore di brenching (Ã¨ il numero massimo di figli che un nodo puÃ² avere in un albero) non Ã¨ molto largo, Type B Ã¨ utile quando il fattore di brenching Ã¨ molto alto.

Le euristiche vengono **apprese** o sfruttate come una combinazione ponderata di caratteristiche di gioco
**Effetto orizzonte**: l'euristica valuta la posizione come buona, ma esplorando un livello di profonditÃ  in piÃ¹ potrebbe cambiare completamente la valutazione.

### 5.1.3 MCTS - Monte-Carlo Tree Search
> Utile quando il fattore di brenching Ã¨ enorme, Anche con la potatura, saresti limitato a piccole profonditÃ 
> - Utile quando non hai una funzione di valutazione buona -> anche se puoi sempre aggiungerla se ce l'hai

**Rollout - Playout** (lanci)-> da un determinato stato, riproduci un intero episodio fino alla fine e ottieni l'utilitÃ .
In MCTS, il valore di uno stato Ã¨ l'**utilitÃ  finale media** su un certo numero di lanci da quello stato

**Playout Policy** -> funzione che decide quqle mossa prendere ad ogni step
- Questa Ã¨ la parte dove il reinforcement learning aiuta molto
- La politica di lancio non Ã¨ tutta la storia

**Pure Monte-Carlo Search** -> dato uno stato, calcolare il suo valore effettuando $N$ rollouts
- Nessun search tree necessario

#### Passaggi di MCTS (Monte Carlo Tree Search)

- **Registra la simulazione in un albero di ricerca**  
	- All'inizio, l'albero di ricerca contiene solo lo stato iniziale.

- Dato un albero di ricerca con uno stato come radice, ripeti:  
	- **Selezione**: Dalla radice, usa la politica di selezione per scegliere un percorso verso una foglia.  
	- **Espansione**: Usa la politica di simulazione per generare un nuovo nodo a partire dalla foglia corrente.  
	- **Simulazione**: Esegui una rollout (simulazione completa) dal nodo appena espanso usando la politica di simulazione. L'albero di ricerca non viene aggiornato.  
	- **Back-propagation (non quella!)**: Aggiorna il valore dei nodi nell'albero di ricerca dal nodo appena aggiunto fino alla radice, seguendo il percorso di selezione.  
	- **Ritorno**: Scegli la mossa con il numero piÃ¹ alto di simulazioni (vedi piÃ¹ avanti).  
	- **Esecuzione della mossa**: Esegui la mossa, ottieni il nuovo stato e ripeti i passaggi sopra.

#### Politica di selezione
- **Definizione**: Una funzione che determina quale mossa seguire in un dato albero di ricerca.  
- **Bilanciamento**: Concilia **sfruttamento** (simulazioni da stati buoni e ben esplorati) e **esplorazione** (simulazioni da stati meno frequentati).  

MCTS combina esplorazione casuale e strategia per migliorare le decisioni in giochi complessi, aggiornando l'albero solo con i risultati delle simulazioni.

![[image-23.png]]

#### Tempo di Esplorazione
$$
\sqrt{\frac{log_N(\text{Parent}(n))}{N(n)}}
$$
- $N(\text{Parent}(n))\ge N(n)$
	- Come ogni volta che visiti n visiti Parent(n)
- $log_N(\text{Parent}(n))\le N(n)$
	- Da un certo numero di playouts in avanti

Supponiamo che Parent$(n)$ riceve 10 playouts piÃ¹ di $n$
Il temine di esplorazione va a 0 quando il numero di playouts aumenta
- non vuoi esplorare per sempre

![[image-24.png|468x359]]

![[image-25.png]]

#### Deterministic Games
- MCTS non richiede un euristica
	- Puoi usarlo su tutti i giochi basta conoscere le regole 
	- Puoi usare un euristica se ne hai una (ad esempio per migliorare la politica)
- MCTS si basa sull'esplorazione, quindi potrebbe volerci del tempo per determinare il valore di una mossa ovviamente buona/cattiva.
	- L'euristica puÃ² aiutare qui
- MCTS con politica/euristica appresa raggiunge costantemente prestazioni sovrumane
	- Chess

Nota: gli approcci moderni combinano ricerca e apprendimento e altro ancora!
#### Stochastic Games
Nodo casuale che rappresenta tutti i possibili risultati (ad esempio, il risultato di un lancio di dadi)
![[image-26.png|461x352]]

#### Minimax Previsto
Lo stesso di minimax, ma devi gestire anche i nodi casuali
Il valore di un nodo casuale Ã¨ il valore atteso di tutti i possibili risultati del nodo

$$
\text{ExpectedMINIMAX}(s) =\sum_r P(r) \text{ğ¸ğ‘¥ğ‘ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘€ğ¼ğ‘ğ¼ğ‘€ğ´ğ‘‹}(\text{ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡}(ğ‘ ,\ ğ‘Ÿ))
$$
-Puoi applicare un ragionamento simile a giochi con informazioni parziali, sia deterministici (ad esempio, scacchi ciechi) che stocastici (ad esempio, poker). 
- **Sono preferibili aggiustamenti ad hoc**, ma non entreremo nel dettaglio.
---
# 6 Ottimizzazioni
#slide8
## 6.1 Artificial Life
La vita artificiale cerca di capire le proprietÃ  essenziali dei sistemi viventi sintetizzando un comportamento realistico in software, hardware e prodotti biochimici.
- **Soft Artificial Life** -> simulazioni digitali processi e organismi viventi
- Hard Artificial life -> implementazione hardware (soft robot). evoluzione dei controller e della morfologia ossia la struttura fisica
- Wet Artificial life -> sintetizza sistemi viventi delle sostanze biochimiche, ossia cellule artificiali
- **Impegno ingegneristico non necessario** -> non vogliamo necessariamente risolvere un problema con un certo margine di errore
- Vogliamo capire come le cose funzionano e scoprire nuove cose, se qualcosa diventa anche utile meglio.

### 6.1.1 Prioneria dell'Artificial Life
> La vita artificiale slega il vincolo materiale al carbonio permettendo di studiare i principi generali senza essere legati al caso specifico (carbonio) ma analizzando in generale il fenomeno

#### Origini
La vita Ã¨ un processo iterativo guidato dall'evoluzione -> avere infiniti loop Ã¨ potente
- condizione iniziale -> come la vita Ã¨ nata
- Evoluzione Biologica -> algoritmo
	- Come la vita progredisce e diventa varia
	- Come gli organismi moderni sono i discendenti dei loro antenati
#### Natural Selection
> Condizioni **necessarie e sufficienti** per l'evoluzione tramite natural selection

- Tasso di riproduzione diverso
	- ProprietÃ  dell'ambiente che favoriscono un un certo tratto
	- Sopravvivono gli individue che combaciano di piÃ¹ all'ambiente
- EreditarietÃ  -> tratti che vengono passati  alla nuova generazione
- Variazioni -> tratti differenti in una popolazione

> Tratti vantaggiosi sopravvivono in una popolazione mentre gli altri spariscono -> **convergono verso una popolazione omogenea** fino alla successiva mutazione e a quelle giÃ  presenti

Punti chiave dell'evoluzione:
- Adattamento -> una caratteristica Ã¨ favorita dalla selezione naturale diventa sempre piÃ¹ importante per un dato compito
- Esitazione -> trovare un uso diverso per un tratto prima utilizzato per un diverso uso.
- DiversitÃ :
	- Mutazioni casuali -> crea nuovi geni
	- Ricombinazione -> cross-over, mischiare i geni
	- Selezione naturale -> cambiata in base al fitness
		- Fitness -> rappresenta quanto Ã¨ buono ad essere trasmesso un genotipo comparato agli altri
		- Selezione sessuale e artificiale
	- Processi divergenti -> non collassa, ma crea novitÃ 
#### Condizioni Iniziali e Algoritmo di evoluzione
Bisogna approssimare le condizioni iniziali di ambiente e di popolazione composta da molti individui.
Un sistema caotico e complesso: ogni approssimazione porta a grandi differenze negli stati futuri.

**Problema di Ottimizzazione** -> partiamo giÃ  da un processo ben definito: evoluzione tramite selezione naturale.
Possiamo simulare le condizioni necessarie e sufficienti:
- variazione
- Riproduzione differenze
- EreditarietÃ 

### 6.1.2 Skeleton of EAs
- Inizializzazione della **popolazione** $P_0$ di **individui** con $N$ caratteristiche (features)
	- Le caratteristiche sono espresse in un dominio specifico di linguaggio (bit-string o numeri reali)
- **Valutazione** della **fitness** della popolazione $P_0$
- Per ogni step $t=1,1,...$
	- Selezione un genitore di $P_0$ in base alla fitness
	- **Riproduzione e ricombinazione** delle caratteristiche del **figlio**
	- Applica una mutazione delle caratteristiche (random o rate di mutazione di adattamento)
	- **Valuta** la fitness della nuova popolazione
	- Selezione un sottoinsieme di $K$ individui sopravvissuti prendendo $P_t$ (esempi casuali pesati dalla fitness)

#### EAs -> Dettagli e Variazioni
- Basandosi sulle regioni dove la fitness Ã¨ alta -> la sfruttiamo
	- Hill-climbing nello spazio della fitness
- Seleziona i genitori basandosi solo sul rank relativo della fitness
- Il numero di figli Ã¨ proporzionale alla differenza tra la media e la fitness individuale -> *ossia se ha una fitness alta avrÃ  piÃ¹ figli*
- **Rate di mutazioni diversi** per ogni caratteristica individuale
- La ricombinazione puÃ² mediare le caratteristiche o prendere il massimo/minimo
	- La ricombinazione cross-over inserisce un punto di taglio e cambia le caratteristiche

### 6.1.3 Diversi Tipi di ALGORITMI Evolutivi
**Strategie di Evoluzione -> Basate sulla distribuzione**
- Mutazioni random + valutazione + ricombinazione
- Programmazione Evolutiva -> mutazione random + valutazione

**Algoritmi Genetici -> Basati sulla popolazione**
- Come input abbiamo i genotipi codificati come stringhe di bit
- Le mutazioni sono rare e la ricombinazione Ã¨ il fattore chiave
- La fitness Ã¨ rilevante anche per selzionare i genitori

**Programmazione Genetica**
- l'individuo Ã¨ un set di istruzioni -> albero
- Algoritmi genetici per codice genetico

### 6.1.4 Genetic Programming
Fenomeni tipici:
1. Fixing Sorting -> l'AI invece di fare un ordinamento complesso sceglie la strada semplice ossia cancellare tutti gli elementi cosÃ¬ la lista Ã¨ sicuramente ordinata, questo perÃ² rende il codice inutile -> controllo: deve contenere gli stessi elementi di input
2. L'evoluzione puÃ² portare a cheat ossia eliminare il file target, modificando l'ambiente 
3. **Bloating** -> Le mutazioni sono casuali e distruttive. Se hai un codice perfetto di 3 righe, una mutazione probabilmente lo romperÃ .
	L'evoluzione tende ad aggiungere enormi quantitÃ  di codice inutile (definito "introns" o junk code) come cicli che non fanno nulla.
	- *FUNZIONE:* perchÃ¨ aumento il codice che le mutazioni vanno ad intaccare riducendo la probabilitÃ  che vengano intaccate le parti vitali

### 6.1.5 Black-box Ottimizzazioni
1. Algoritmo di ricerca da massimizzare $f:S\to R$
	1. $S$ Ã¨ lo spazio di ricerca
	2. Di solito: $SâŠ‚  R^n$
2. $f$ puÃ² essere qualsiasi funzione -> piÃ¹ utile se $f$ non Ã¨ convessa
3. L'unica informazione Ã¨ la valutazione della funzione $f$ in un punto
	1. I punti sono presi dallo spazio di ricerca, deve essere in grado di campionare
	2. Non ha bisogno di altre informazioni, **no derivate** 
4. **Metriche di Performance**
	1. Richiede una valutazione per convergere
	2. Valore della soluzione finale
	3. Note -> assumiamo che il costo della valutazione Ã¨ indipendente dal numero degli esempi

### 6.1.6 Strategie di Evoluzione
1. Ottimizzazione black-box **stocastica**
2. $f$ Ã¨ una funzione **continua**
3. Inizia con un insieme di individui
	1. Valuta gli individui
	2. Muta e ricombina individui
4. Individui all'inizio della simulazione $x^0\in S âŠ‚ R^n$
	1. fenotipo, oggetto o **parametri di decisione**
5. Un set di **parametri strategici** $\sigma^o$ per ogni individuo
	1. Tasso di mutazione dei parametri decisionali
	2. Parametri che possono evolversi

> Mutazione = **aggiungere rumore casuale** (Gaussiano)
> - Isotropic Gaussian (1 parametro -varianza) = $N(m,\sigma^2\ l)$)
> - 1 parametro per ogni dimensione = $N(m,\ D^2)$ con $D$ diagonale

Esplora uniformemente lo spazio di ricerca ossia Ã¨ come fare l'assunzione minima sulla struttura di $f$
**Mixing number  $\rho$** -> indica quanti genitori generano figli
- I genitori sono scelti casualmente
- $\rho=1$ -> clone
- Selezione -> (ğœ‡, ğœ†)-ES e (ğœ‡ + ğœ†)-ES
	- Ossia come vedremo dopo la selezione nel primo caso tra solo i figli e nel secondo la selezione dall'insieme combinato di figli e genitori

Se vuoi generare $\lambda$ figli, devi scegliere $\lambda$ gruppi di $\rho$ genitori
**Ricombinazione** -> molte possibilitÃ 
- Ad esempio -> selezionare delle features $j$ dai genitori $i$ (random), media (multi-recombination), media pesata

## 6.2 Self-adaptive ES
> Adapting strategy parameters
> - *ES = Evolution Strategies*

### 6.2.1 (ğœ‡, ğœ†)-ES
- Popolazione $X^g\in R^{\micro ,n}$, step size $\sigma^g\in R^\micro$ -> strategy parameter
	- Uguali per tutte le features do un dato individuo -> isotopic gaussian
- Genera $\lambda$ figli ($k=1,...,\lambda$)
	- Seleziona parametri a caso $P^g\in R^{\rho ,n}$
	- $\sigma^{g+1}_k =recombine(\sigma^g |P^g)e^{\epsilon_k\sim N(0,1)}$
	- $x^{g+1}_k =recombine(P^g)+N(0,\sigma^{g+1}_k)$
	- Valutare la fitness
- Teniamo i meglio figli $\micro$ tra tutti i figli $\lambda$
	- $\lambda > \micro$ iperparametro -> non viene cambiato durante l'evoluzione
- I genitori vengono **sempre scartati**

Come candidati possiamo prendere:
- gli individui **migliori**
- La **media** degli individui
- La **media pesata** degli individui
![[image-27.png|414x386]]

### 6.2.2 (ğœ‡ + ğœ†)-ES
> **I figli competono anche con i genitori**
> - Ã¨ meglio per spazi finiti grandi o problemi combinatori

- ($1+1$)-ES fu il primo ES
	- Seleziona un individuo tra due possibilitÃ  il genitore o il figlio
- Preserva il migliore fino ad ora
- Avvolte puÃ² mischiare i numeri
	- $(\micro /\rho + \lambda)$-ES
	- $(\micro /\rho , \lambda)$-ES

#### Descrizione Algoritmo
Questo pseudocodice descrive il funzionamento generale di unaÂ **Strategia Evolutiva**Â (Evolution Strategy - ES), mostra le due alternative viste primaÂ $(\mu/\rho \stackrel{+}{,} \lambda)$.

Ecco cosa fa passo dopo passo in parole semplici:
1. Creazione dei figli -> Da un gruppo di genitori ($\mu$), l'algoritmo genera un numero maggiore di figli ($\lambda$). Per creare ogni figlio:
    - **Marriage & Recombination:**Â Seleziona alcuni genitori ($\rho$) e mescola le loro informazioni (sia il codice geneticoÂ $y$Â che i parametri strategiciÂ $s$).
    - **Mutation:**Â Applica mutazioni casuali. Nota importante: prima muta i parametri di strategia ($s$, che controllanoÂ _quanto_Â mutare) e poi usa quelli per mutare il codice genetico vero e proprio ($y$).
    - **Fitness:**Â Calcola il punteggio ($F$) del nuovo figlio.
2. Selezione dei sopravvissuti (Line 14-17) -> Alla fine del ciclo, deve decidere chi passa alla generazione successiva. Il codice mostra due modalitÃ  alternative:
    - **$(\mu, \lambda)$Â - Selezione a virgola:**Â I genitori vengono scartati a priori. I nuovi sopravvissuti sono sceltiÂ **solo tra i figli**. (Utile per evitare di rimanere bloccati in ottimi locali).
    - **$(\mu + \lambda)$Â - Selezione col piÃ¹:**Â I nuovi sopravvissuti sono i migliori scelti dall'insieme unito diÂ **genitori E figli**. (Molto elitario, preserva sempre la soluzione migliore trovata finora).

In sintesi:Â **Genera una prole numerosa rimescolando i genitori, mutala, e poi tieni solo i migliori (o rimpiazzando i vecchi o facendoli competere).**

#### PerchÃ¨ Dovremmo Usare (ğœ‡, ğœ†)-ES se non Ã¨ Elitaria
> *ELITARIA* -> significa che la soluzione ottima trovata fino ad ora sopravvive sempre, quindi NON-ELITARIA significa che le soluzione vecchie vengono sempre scartate anche se erano le migliori

Per evitare l'overfitting verso l'oggetto
- `,`-selection -> Ã¨ un esploratore migliore dello spazio di ricerca
- `+`-selection -> Ã¨ uno sfruttatore migliore dello spazio di ricerca

- **Il Paradosso:**Â A volte, per risolvere un problema, deviÂ **smettere di cercare direttamente l'obiettivo**.
- **L'Esempio del Labirinto:**
    - _La Metrica:_Â Immagina che il punteggio sia la "distanza in linea d'aria dall'uscita".
    - _L'Inganno:_Â Se ti trovi vicinissimo all'uscita ma separato da un muro, il tuo punteggio Ã¨ alto.
    - _La NecessitÃ :_Â Per vincere devi allontanarti dal muro (peggiorare il punteggio momentaneamente) per aggirare l'ostacolo.
- **Il Paesaggio di Ricerca (Landscape):**
    - Ãˆ pieno diÂ **ottimi locali**: punti che sembrano soluzioni (punteggio alto) ma sono in realtÃ  vicoli ciechi (come il punto davanti al muro).
- **La Soluzione:**
    - PoichÃ© Ã¨ difficile capire quando un problema Ã¨ "ingannevole", la strategia migliore Ã¨Â **aumentare l'esplorazione** (fare cose diverse) invece di seguire ciecamente la metrica di ottimizzazione, accettiamo di fare un passo peggiorativo per permetterci di migliorare in seguito.

#### Comparazione `,` e `+` -selection
Cosa succede se $\micro = \lambda$, ossia il numero di figli Ã¨ uguale al numero dei nodi che devo prendere, nel caso di (ğœ‡, ğœ†)-ES?
- Tutti i figli sono presi
- Non c'Ã¨ selezione
- Passi casuali nello spazio di ricerca
Cosa succede invece in *(ğœ‡ + ğœ†)-ES*?
- Non ci sono problemi la selezione continua a funzionare
- Tieni i meglio $\micro = \lambda$ individui predi dal set formato dai figli e genitori $\micro + \lambda$
- Qui non cambia niente dato che l'insieme dove applico la selezione Ã¨ piÃ¹ grande dei soli figli dato che comprende anche i genitori

### 6.2.3 CMA-ES - Covariance Matrix Adaptation
> Le mutazioni seguono un matrice di covarianza **adattiva** che si aggiusta basandosi sulla forma locale del paesaggio della funzione 
> - Come il momentum -> velocizza quando c'Ã¨ una pianure e rallenta quando ci sono colline

- Popolazione campionata dalla gaussiana multivariata -> $N(m^g, C^g)$
	- $C^g$ simmetrica
	- $\frac{n^2+n}{2}$ gradi di libertÃ 
- Il vettore medio Ã¨ preso come soluzione candidata
- Inizializza $C^0=I$, $m^g\in R^n$ - random

#### 1 Generazione della Prole -> Sampling
- **L'Azione:**Â Creiamo nuovi candidati (figli) partendo dalla distribuzione attuale. 
- La Formula:$$x_k^{g+1} = m_g + \sigma \cdot N(0, C_g)$$
	- **$m_g$:**Â Il punto centrale attuale (la media della popolazione "buona").
	- **$\sigma$Â (Sigma):**Â L'ampiezza del passo (step-size), quanto lontano osiamo andare.    
    - **$N(0, C_g)$:**Â Il "rumore" casuale modellato dalla matrice di covarianzaÂ $C_g$Â (che determina laÂ _forma_Â della ricerca, es. un cerchio o un ellisse).
- **In sintesi:**Â Prendiamo il centro attuale e "spariamo"Â $\lambda$Â figli casuali attorno ad esso.
#### 2 Valutazione e Ordinamento -> Selection
- **Ordinamento:**Â Una volta generati i figli, calcoliamo la loro fitness (punteggio).
- **Ranking:**Â Li mettiamo in fila dal migliore al peggiore ($x_{1:\lambda}$Â Ã¨ il migliore,Â $x_{\lambda:\lambda}$Â Ã¨ il peggiore).

#### 3 Aggiornamento della Media -> Recombination
- **L'Obiettivo:**Â Spostare il centro della ricerca ($m_{g+1}$) verso la zona promettente trovata dai figli migliori. 
- La Formula:$$m_{g+1} = \sum_{i=1}^{\mu} w_i \cdot x_i^{g+1}$$
    - Si calcola laÂ **media pesata**Â solo dei miglioriÂ $\mu$Â figli.
    - **$w_i$Â (Pesi):**Â Danno piÃ¹ importanza ai figli migliori ($w_1 > w_2 > \dots$). La somma dei pesi Ã¨ 1.

#### 4 Il Concetto Chiave: Selezione Troncata -> Truncated Selection
- **La Domanda:**Â PerchÃ© usiamo soloÂ $\mu$Â genitori se ne abbiamo generatiÂ $\lambda$Â ($\mu < \lambda$)?
- **Il Motivo:**Â Ãˆ questo che crea laÂ **pressione evolutiva**.
    - Se usassimoÂ _tutti_Â i figli ($\mu = \lambda$) per calcolare la nuova media, il centro rimarrebbe praticamente fermo (la media del rumore casuale Ã¨ zero).
    - Scartando i peggiori e tenendo solo i migliori (Troncamento), forziamo la media a spostarsi fisicamente verso la "salita", guidando l'evoluzione.

Ecco il seguito dello schema, strutturato per collegarsi direttamente ai punti precedenti (Sampling, Selection, Mean Update). Qui entriamo nel cuore matematico del CMA-ES.

#### 5 Effective Sample Size ($\mu_{eff}$) - La QualitÃ  dell'Informazione
- **Il Concetto:**Â Misura quanta informazione stiamo realmente utilizzando per aggiornare la distribuzione, basandosi su come sono distribuiti i pesiÂ $w_i$.
    - La Formula:$$\mu_{eff} = \frac{1}{\sum_{i=1}^{\mu} w_i^2}$$
    - **Il Range ($1 \le \mu_{eff} \le \mu$):**
	- Se tutti i pesi sono uguali ($w_i = 1/\mu$):Â $\mu_{eff} = \mu$. Significa che "ascoltiamo" tutti i genitori allo stesso modo (massima informazione, ma bassa pressione selettiva).
    - Se un peso Ã¨ 1 e gli altri 0:Â $\mu_{eff} = 1$. Significa che tutto dipende dal singolo figlio migliore (massima selezione, rischio instabilitÃ ). 
- **Regole Pratiche (Heuristics):**
    - Solitamente si sceglieÂ $\mu \approx \lambda / 2$Â (si tengono metÃ  dei figli).
    - L'obiettivo Ã¨ avereÂ $\mu_{eff} \approx \lambda / 4$.
    - I pesi decrescono linearmente:Â $w_i \propto \mu - i + 1$.

#### 6 Aggiornamento della Matrice di Covarianza ($C$) - Parte I

Questo Ã¨ il passaggio cruciale che permette all'algoritmo di "imparare" la forma del problema (es. capire se c'Ã¨ una valle stretta e allungare l'ellisse di ricerca in quella direzione).
- L'Intuizione:
    Vogliamo che la nuova matrice $C_{g+1}$ aumenti la probabilitÃ  di generare vettori simili a quelli che hanno avuto successo nella generazione corrente ($x_{g+1}$). Se andare a "Nord-Est" ha pagato, deformiamo la ricerca verso "Nord-Est".
- Riscrivere l'aggiornamento della Media:
    Possiamo vedere la nuova media non come una posizione assoluta, ma come la vecchia media piÃ¹ uno spostamento pesato: $$\mathbf{m}_{g+1} = \mathbf{m}_g + \sum_{i=1}^{\mu} w_i (\mathbf{x}_i^{g+1} - \mathbf{m}_g)$$
    (Stiamo sommando i vettori differenza "successo - vecchia media")

- Stima della Covarianza ($C_{\lambda}^{g+1}$):
    Prima di aggiornare la matrice vera e propria, stimiamo la forma della distribuzione attuale basandoci sui campioni appena estratti: $$C_{\lambda}^{g+1} = \frac{1}{\lambda} \sum_{i=1}^{\lambda} (\mathbf{x}_i^{g+1} - \mathbf{m}_g)(\mathbf{x}_i^{g+1} - \mathbf{m}_g)^T$$
    - **Concetto chiave:**Â Questo Ã¨ unoÂ **stimatore corretto (unbiased estimator)**Â diÂ $C_g$. Significa cheÂ $E[C_{\lambda}^{g+1}] = C_g$. In parole povere: se avessimo infiniti campioni, questa formula ci ridarebbe esattamente la forma della matrice originale.
#### 7 Stima Migliorata della Covarianza ($C_{\mu}$)
- **L'Idea:**Â Invece di stimare la forma usandoÂ _tutti_Â i figli (che darebbe una stima neutra/unbiased), usiamo solo iÂ **miglioriÂ $\mu$Â figli**. 
- La Formula:$$C_{\mu}^{g+1} = \sum_{i=1}^{\mu} w_i \frac{(x_i^{g+1} - m_g)}{\sigma} \frac{(x_i^{g+1} - m_g)^T}{\sigma}$$
- **Il Significato:**
    - La somma ora arriva fino aÂ $\mu$Â (nonÂ $\lambda$): stiamo ignorando i figli scarsi.
    - **Effetto:**Â Campionare da questa matriceÂ $C_{\mu}$Â tenderÃ  a riprodurre i passi che hanno avuto successo in questa generazione.

#### 8 Rank-$\mu$Â Update (L'Aggiornamento Robusto)
- **Il Problema:**Â Non possiamo buttare via la vecchia matriceÂ $C_g$Â e sostituirla conÂ $C_{\mu}$Â ogni volta (sarebbe instabile e dimenticherebbe tutto ciÃ² che ha imparato prima).
- **La Soluzione (Exponential Smoothing):**Â Uniamo la vecchia conoscenza con la nuova stima.
- La Formula:$$C_{g+1} = (1 - c_{\mu}) C_g + c_{\mu} \frac{1}{\sigma^2} C_{\mu}^{g+1}$$
    - **$C_g$:**Â La "memoria" storica delle generazioni passate.
    - **$C_{\mu}^{g+1}$:**Â La "novitÃ " imparata dai migliori figli di oggi.
    - **$c_{\mu}$Â (Learning Rate):**Â Un valore tra 0 e 1. Determina quanto velocemente l'algoritmo impara (o dimentica).
        - $c_{\mu}$Â bassoÂ $\rightarrow$Â Memoria lunga (stabile).
        - $c_{\mu}$Â altoÂ $\rightarrow$Â Adattamento rapido (ma rischioso)

#### 8 Rank-One Update (L'Aggiornamento Aggressivo)
- **L'Intuizione:**Â E se invece di fare la media pesata diÂ $\mu$Â vettori, ci fidassimo ciecamenteÂ **solo del primo della classe**Â (il figlio migliore assolutoÂ $x_1$)?
- La Formula: $$C_{g+1} = (1 - c_1) C_g + c_1 \frac{(x_1^{g+1} - m_g)}{\sigma} \frac{(x_1^{g+1} - m_g)^T}{\sigma}$$
- **L'Effetto:**
    - Aumenta drasticamente la probabilitÃ  di generare vettori lungo la linea dell'unico passo migliore appena fatto.
    - Ãˆ molto efficiente quando c'Ã¨ una direzione chiara e dominante da seguire.
#### 9 Evolution Path (Percorso Evolutivo)
- Per aggiornare la matrice di covarianza, si considera unaÂ **sequenza di passi successivi**Â avvenuti nel corso di piÃ¹ generazioni (non solo l'ultima).
#### 10 Il "Vero" Aggiornamento CMA-ES
- L'algoritmo completo si ottiene combinando due tecniche:
    - **Rank-one update:**Â applicato agliÂ _evolution paths_.
    - **Rank-$\mu$Â update:**Â (quello classico basato sulla media dei migliori).

#### 11 Adattamento diÂ $\sigma$Â (Step Size)
- La grandezza del passoÂ $\sigma$Â **non Ã¨ fissa**.
- Esiste una regola di aggiornamento che permette laÂ **self-adaptation**Â (l'algoritmo regola da solo quanto grandi devono essere i passi).
### 6.2.4 Rissunto
#### Struttura degli Algoritmi Evolutivi (EAs)
- **Ciclo di Base (Scheletro):**
       1. Inizializzazione della popolazione.
    2. Valutazione (Fitness).
    3. Selezione dei genitori.
    4. Variazione (Crossover e Mutazione).
    5. Sostituzione (creazione della nuova generazione)Â 5.
- **Tipologie Principali:**
    - **Genetic Algorithms:**Â Usano stringhe di bit; la ricombinazione Ã¨ il fattore principale.
    - **Genetic Programming:**Â L'individuo Ã¨ un programma (es. albero di sintassi); usato per generare codice.
    - **Evolution Strategies (ES):**Â Ottimizzazione a valori reali, basata su mutazione stocastica (es. rumore Gaussiano.
- **Problemi Comuni:**Â L'evoluzione puÃ² "barare" (Cheating) trovando scorciatoie tecniche che soddisfano la fitness senza risolvere il problema reale (es. cancellare il file target invece di eguagliarlo) o soffrire di "Bloating" (codice spazzatura.

#### Evolution Strategies (ES)
- **Obiettivo:**Â Ottimizzazione "Black-box" di funzioni continue dove non si conoscono derivate.
- **Self-Adaptation:**Â Ãˆ fondamentale evolvere non solo la soluzione ($x$), ma anche i parametri di strategia ($\sigma$, step size) per evitare di rimanere bloccati quando la fitness migliora.
- **Meccanismi di Selezione:**
    - **Selezione a VirgolaÂ $(\mu, \lambda)$:**Â I genitori vengono scartati. Non Ã¨ elitaria, favorisce l'esplorazione e l'adattamento dinamico del passo (step size).Â Ãˆ generalmente preferita per evitare ottimi locali.
    - **Selezione col PiÃ¹Â $(\mu + \lambda)$:**Â I genitori competono con i figli.Â Ãˆ elitaria (il migliore sopravvive sempre), ottima per sfruttare una zona (exploitation) ma rischia stagnazione.

#### CMA-ES (Covariance Matrix Adaptation)
- **Funzionamento:**Â Ãˆ lo stato dell'arte per l'ottimizzazione continua.Â Adatta una matrice di covarianza ($C$) per modellare la forma del paesaggio di ricerca (es. trasformando la ricerca da circolare a ellittica).
- **Aggiornamento della Media:**Â La nuova media Ã¨ una media pesata dei miglioriÂ $\mu$Â figli (Selection), spingendo la ricerca verso le zone promettenti.
- **Aggiornamento della Covarianza ($C$):**Â Combina due metodi:
    - **Rank-$\mu$Â update:**Â Usa le informazioni della popolazione (stabilitÃ ).
    - **Rank-one update:**Â Usa il percorso evolutivo (evolution path) per sfruttare il "momento" e la direzione dei passi precedenti.
- **Controllo del Passo ($\sigma$):**Â La dimensione del passo si adatta confrontando la lunghezza del cammino percorso con quella di una camminata casuale (Path Length Control).

# 7 Artificial Life
> Evolution of Complexity -> da cellule base a organismi piÃ¹ complessi

**Tierra** -> Programmi software (creature) che competono per il tempo della CPU e vivono nello spazio RAM
**Avida** -> Il software ha risorse dedicate, puÃ² acquisirne di piÃ¹ eseguendo attivitÃ  (ad esempio la moltiplicazione binaria)

> Fitness -> vivere abbastanza a lungo

- **SensibilitÃ  alle condizioni iniziali:**Â Una variazione infinitesimale all'inizio porta a risultati finali completamente diversi (Caos).
- **Determinismo vs PrevedibilitÃ :**Â Il sistema segue regole fisse, ma Ã¨Â **imprevedibile**Â perchÃ© Ã¨ impossibile conoscere lo stato iniziale con precisione perfetta.
- **Conclusione:**Â Non possiamo calcolare il futuro con una formula, l'unico modo Ã¨Â **eseguire la simulazione**Â passo dopo passo.

### 7.1.1 Cellular Automata
> Modello matematico semplice di **self-replication** (riproduzione autonoma)

- La descrizione del sistema viene utilizzata per replicare il sistema e per costruirlo
	- Questo Ã¨ stato concepito prima che il meccanismo del DNA fosse pienamente compreso / scoperto
- **Spazio Discreto** -> multi-dimensional vettore di celle e **tempo discreto**
- 1 variabile discreta per cella
- Regole di Aggiornamento dettano le dinamiche
	- Aggiornamenti sincroni basati su ogni celle dei vicini
- Gestione dei Confine
	- Celle possono rimanere costanti e lo spazio puÃ² essere sovrapposto -> toroidal structure
	- Anche il quartiere puÃ² essere definito in modo diverso

#### Funzionamento base di unÂ **Automa Cellulare Elementare (1D)** - ECA
- **Struttura:**Â Una fila di celle che possono essere soloÂ **0**Â oÂ **1**Â (bianco o nero).
- **Regola di Aggiornamento:**Â Lo stato futuro di una cella dipende solo daÂ **se stessa e dai suoi due vicini**Â (sinistra e destra) al tempo precedente (sjâˆ’1,sj,sj+1).
- **La "Regola":**Â I disegni in basso (111Â â†’Â 0, 110Â â†’Â 1, ecc.) definiscono la "legge fisica" di quel mondo. Leggendo i risultati in basso come un numero binario (es. 01011010), si ottiene il numero della Regola (es. Rule 30, Rule 90) che determina se il sistema genererÃ  ordine, caos o strutture frattali.

Assunzioni:
- Uno stato iniziale 0-state deve essere mappato a 0 -> L'ultima cifra deve essere 0
- Riflessione Simmetrica -> isotropia: stesso comportamento che riguarda la direzione
	- 110 -> 011, 100 -> 001
- Regole ammissibili del modulo $ğ‘_1ğ‘_2ğ‘_3ğ‘_4ğ‘_2ğ‘_5ğ‘_40$ â†’ 32 possible 

![[image-28.png|488x251]]
![[image-29.png|493x245]]

**Evoluzione**
- Rumore bianco come stato iniziale
	- I valori delle celle non sono correlati tra loro
	- Un parametro p descrive l'intero stato
- Sistema ordinato di conseguenza
	- PiÃ¹ parametri per descriverlo
- ProprietÃ  di auto-organizzazione
	- Un altro segno distintivo di sistemi non lineari complessi
- 300 fasi temporali di evoluzione con regola 126
- Stato iniziale casuale
	- $P(x=1) = 0,5$
- Ãˆ anche possibile lo studio delle proprietÃ  globali
	- Teoria classica dei sistemi dinamici

##### Regole in ECA
1. Type I
	- Evoluzione verso gli stati statici
2. Type II
	- Evoluzione alla struttura periodica
3. Type III
	- L'evoluzione a schemi caotici -> porta alla (pseudo) casualitÃ 
4. Type IV
	- Evoluzione a modelli complessi -> limite del caos

##### CA con k Stati per Cella
> Formazione di membrane protettive che proteggono il loro contenuto dagli effetti esterni
> Quando due membrane si scontrano possono distruggersi a vicenda

![[image-30.png|326x278]]

## 7.2 Universal Computer - Calcolo Universale
#rileggere

Questo capitolo spiega come gli Automi Cellulari (CA) non siano solo generatori di pattern visivi, ma possano fungere da veri e propri sistemi di calcolo.
- **Il Caso dell'ECA 110:**Â La "Regola 110" degli Automi Cellulari Elementari (identificata da Stephen Wolfram) Ã¨ un esempio di sistema capace diÂ **computazione universale**.
- **Definizione:**Â Essere un computer universale significa che, modificando solo l'input iniziale, il sistema Ã¨ in grado di calcolare qualsiasi funzione computabile.Â Ãˆ teoricamente equivalente a unaÂ **Macchina di Turing universale**.
- **Nota sulla complessitÃ :**Â La configurazione necessaria per ottenere la computazione universale non Ã¨ necessariamente la piÃ¹ semplice possibile4.
**Varianti del modello CA - Variazioni**
Oltre al modello classico, esistono molte varianti che ne estendono le capacitÃ :
- **Non-uniformi:**Â Celle diverse possono seguire regole di aggiornamento diverse.
- **Non-locali (Learning rules):**Â La definizione di "vicinato" cambia, ad esempio seguendo la struttura di un grafo (automata network) invece di una griglia fissa.
- **Asincroni:**Â Le celle non si aggiornano tutte contemporaneamente nello stesso istante (Asynchronous updates).
- **Continui:**Â Si abbandona la discretizzazione per avere spazio o tempo continui.
### 7.2.1 Extending GoL (Estensione del Game of Life)

Il concetto classico diÂ _Game of Life_Â viene generalizzato per superare i limiti discreti.
- **Dalla Discretizzazione alla ContinuitÃ :**Â Si passa da griglie fisse e stati binari (0/1) a modelli che ricordano leÂ _Cellular Neural Networks_Â (calcolo analogico/continuo)
- **Definizione Formale:**Â Un Automa Cellulare (CA) Ã¨ definito dalla tuplaÂ $(L, T, S, N, \phi)$, doveÂ $L$Â Ã¨ il reticolo (spazio),Â $T$Â la linea temporale,Â $S$Â l'insieme degli stati,Â $N$Â il vicinato eÂ $\phi$Â la regola locale
- **Obiettivo:**Â Creare comportamenti emergenti complessi in spazi e tempi non necessariamente discreti

### 7.2.2 Lenia
Lenia Ã¨ una generalizzazione "continua" degli Automi Cellulari.
- **ContinuitÃ  Totale:**Â A differenza del CA classico, in Lenia lo spazio, il tempo e gli stati sono continui (i valori tendono a zero invece di essere discreti:Â $1/R, 1/T, 1/P \rightarrow 0$)
- **Ingredienti:**Â La regola di aggiornamento si basa su tre componenti chiave: la distribuzione del Potenziale, la distribuzione della Crescita e il Kernel

### 7.2.3 Potential Distribution - Distribuzione del Potenziale
Rappresenta la "percezione" che una cella ha del suo vicinato.
- **Calcolo:**Â Si calcola tramite unaÂ **convoluzione**Â tra lo stato attuale della griglia ($A^t$) e unÂ _Kernel_Â ($K$):Â $U^t(x) = K * A^t(x)$
- **Il Kernel ($K$):**Â Definisce l'area di influenza. Spesso ha la forma di anelli concentrici (shell) definiti da picchiÂ $\beta$Â e raggi specifici.Â Usa la distanza polare dal centro

![[image-31.png|532x207]]

### 7.2.4 Growth Distribution - Distribuzione della Crescita
Determina come cambia lo stato della cella in base al potenziale percepito.
- **Mapping:**Â Ãˆ una funzioneÂ $G$Â che mappa il valore del potenzialeÂ $U$Â in un valore di crescita nell'intervalloÂ $[-1, 1]$
- **Forma:**Â Solitamente Ã¨ una funzione unimodale (una campana Gaussiana) definita da un centroÂ $\mu$Â e una larghezzaÂ $\sigma$.
- Aggiornamento: Il nuovo stato Ã¨ dato dallo stato precedente piÃ¹ la crescita moltiplicata per un piccolo intervallo di tempo $\Delta t$, il tutto "clippato" tra 0 e 1:$$A^{t+\Delta t} = \text{clip}_{[0,1]}(A^t + \Delta t \cdot G(U^t))$$
- **Efficienza:**Â Le convoluzioni vengono calcolate efficientemente nel dominio delle frequenze usando la FFT (Fast Fourier Transform)
![[image-32.png|467x233]]

### 7.2.5 Extended Lenia
Ãˆ un'espansione ulteriore di Lenia che introduce maggiore complessitÃ  e stabilitÃ .
- **Multidimensionale:**Â Funziona in 3 o piÃ¹ dimensioni, permettendo forme piÃ¹ stabili
- **Kernel Multipli:**Â Usa diversi kernel, ognuno con il proprio raggio e la propria funzione di crescita, aumentando il comportamento caotico
- **Canali Multipli:**Â Introduce diversi "canali" (come mondi paralleli o tipi di sostanze) che interagiscono tra loro, permettendo la specializzazione dei ruoli
- **Risultati:**Â A differenza della Lenia base, la versione estesa mostra fenomeni diÂ **auto-replicazione**, solitoni (strutture che si auto-organizzano) e forme complesse come "serpenti 3D"
### 7.2.6 (Extended) Lenia results + Flow Lenia
- **Risultati Chiave:**Â A differenza della versione base, laÂ **Extended Lenia**Â mostra capacitÃ  diÂ **auto-replicazione**: i "solitoni" (strutture stabili auto-organizzanti) possono dividersi in due parti che si respingono a vicenda.
- **Strutture complesse:**Â Emergono forme tridimensionali (es. "3D Snake") capaci di mangiare e crescere, risultato della coordinazione tra canali multipli con dinamiche diverse
- **Flow-Lenia:**Â Una variante che introduce laÂ **conservazione della massa**Â nel sistema per facilitare la ricerca evolutiva

### 7.2.7 Biomaker CA â€“ Plant-based lifeforms
Un modello di vita artificiale ispirato alle piante.
- **Elementi dell'Ecosistema:**Â Include Terra, Aria, Agenti (Semi), celle Immobili e Sole
- **Metabolismo:**Â Gli agenti acquisiscono energia dai nutrienti della terra (tramite le radici) e dell'aria (tramite le foglie); il Sole rigenera i nutrienti dell'aria, le celle immobili quelli della terra
- **Ciclo Vitale:**
    - **Riproduzione:**Â Avviene tramite spargimento di semi soggetti a mutazione
    - **Competizione:**Â La crescita consuma spazio e nutrienti (che sono finiti); le piante morte restituiscono nutrienti al terreno
- Earth (1), Air (2), Seed Agent (3), Immovable (4), Sun (5)
- Ogni cellula percepisce un quartiere locale
![[image-33.png]]

### 7.2.8 Biomaker CA - Meccaniche Generali
- **Azioni:**Â Le cellule eseguono azioni specifiche (nascere, specializzarsi, muoversi) in base al loro tipo
- **Meta-Evoluzione:**Â Il sistema usa due livelli di ottimizzazione:
    - UnÂ _Outer loop_Â che genera diversi ambienti.
    - UnÂ _Inner loop_Â (ottimizzazione ES) che adatta gli agenti a quegli ambienti        
    - **Nota:**Â Questo processo Ã¨ computazionalmente molto costoso

### 7.2.9 Particle Swarm ALife
Sistemi basati su agenti puntiformi invece che su griglie.
- **Definizione:**Â Ogni particella Ã¨ un pixel con posizione, velocitÃ  e colore (che ne identifica la "famiglia")
- **Dinamica:**Â Il comportamento emerge dalle forze diÂ **attrazione e repulsione**Â tra le diverse famiglie (es. "il rosso attrae il rosso", "il giallo respinge il rosso")
- **Risultato:**Â Si creano comportamenti auto-organizzanti come ilÂ _flocking_Â (stormi) o strutture stabili, gestendo anche le collisioni

### 7.2.10 CA Applications - Applicazioni degli Automi Cellulari
Oltre alla simulazione della vita, i CA hanno usi pratici in ingegneria e informatica:
- **Hardware (VLSI):**Â Progettazione di circuiti e generatori di numeri pseudo-casuali per l'auto-test dei chip
- **Crittografia:**Â Sfruttando la generazione di sequenze complesse
- **Image Processing:**
    - Rilevamento dei bordi e riduzione del rumore
    - **Segmentazione:**Â Segmentare immagini ad alta risoluzione (sfida difficile anche per le reti neurali moderne) usando pochissimi parametri
- **Compressione:**Â Di testo e immagini
## 7.3 Neural Cellular Automata (NCA)

Ãˆ un modello che fonde gli Automi Cellulari con il Deep Learning per studiare laÂ **morfogenesi**Â (come un organismo sviluppa la sua forma) e l'auto-organizzazione.
- **Obiettivo:**Â Creare sistemi capaci di "coltivare" una forma specifica (es. un'immagine) partendo da un singolo seme e di mantenerla stabile nonostante le perturbazioni
### 7.3.1 Struttura della Cella
Ogni cella non Ã¨ un semplice valore binario, ma unÂ **vettore di numeri reali**Â (solitamente 16 canali):
- **Primi 3 canali:**Â RGB (colore visibile)
- **Quarto canale:**Â Alpha (trasparenza/vitalitÃ ).Â SeÂ $\alpha > 0.1$Â la cella Ã¨ "viva", altrimenti Ã¨ "morta" o in crescitaÂ 
- **Canali rimanenti:**Â Stati nascosti (hidden state) per la comunicazione locale.

### 7.3.2 Funzionamento (Architettura)
L'aggiornamento avviene in due fasi cicliche:
- **Percezione:**Â Ogni cella "vede" il suo vicinatoÂ $3 \times 3$Â attraverso filtri convoluzionali fissi (es. filtri di Sobel per rilevare gradientiÂ $x$Â eÂ $y$)
- **Aggiornamento (Update Rule):**
    - UnÂ **piccolo network neurale**Â (rete densa/MLP) riceve il vettore di percezione e calcola la variazione di statoÂ $\Delta s$Â 
    - La stessa rete Ã¨ condivisa daÂ **tutte**Â le celle (come in un CA classico, le regole sono locali e uniformi)
    - **Aggiornamento Stocastico:**Â Per simulare l'asincronia naturale, ogni cella si aggiorna con una probabilitÃ  del 50% a ogni passo

### 7.3.3 StabilitÃ  e Rigenerazione
- **Training:**Â Si parte da una singola cella viva e si addestra la rete affinchÃ©, dopoÂ $N$Â passi, la griglia assomigli all'immagine target
- **Pool di Stati:**Â Per rendere il pattern stabile (un "attrattore"), si usa un pool di stati passati durante l'addestramento.Â Il sistema impara a correggere errori partendo non solo dall'inizio, ma anche da stati intermedi o degradatiÂ 
- **Rigenerazione:**Â Grazie a questo addestramento robusto, se una parte dell'immagine viene cancellata ("trauma"), le celle rimanenti comunicano per ricostruire la parte mancante

### 7.3.4 Applicazione: Self-classifying MNIST
- Un esempio diÂ **classificazione distribuita**: invece di un classificatore centrale, ogni cella di un'immagine (es. una cifra MNIST) decide autonomamente quale numero rappresenta.
- Le celle devono raggiungere un "accordo" (consenso) tramite interazioni locali.Â Gli ultimi 10 canali del vettore di stato rappresentano le probabilitÃ  delle classi (0-9).

![[image-34.png]]

### 7.3.5 Adding Stability (Aggiungere StabilitÃ )

Per evitare che l'immagine generata si degradi o cambi una volta completata, bisogna rendere il pattern target unÂ **attrattore**Â stabile.
- **Il Problema:**Â Se addestriamo solo partendo dal seme iniziale, la rete non sa cosa fare se l'immagine devia leggermente una volta finita.
- **La Soluzione (Sample Pool):**Â Si utilizza una "pool" (riserva) di stati passati.Â Invece di ricominciare sempre da zero, la rete viene addestrata a riprendere da questi stati intermedi o finiti salvati nella pool, imparando a correggerli e mantenerli stabili nel tempo

### 7.3.6 Regenerate after Trauma - Rigenerazione dopo un Trauma
L'NCA Ã¨ capace di autoripararsi se viene danneggiato
- **Metodo di Training:**Â Per insegnare questa capacitÃ , durante l'addestramento si applicano danni intenzionali agli stati prelevati dalla pool.
- **Il Danno:**Â Solitamente si azzera (cancella) una regione circolare casuale dell'immagine.Â La rete Ã¨ cosÃ¬ costretta a imparare come ricostruire le informazioni mancanti basandosi sulle celle sopravvissute nel vicinato

### 7.3.7 Self-classifying MNIST
In questa applicazione, l'NCA non viene usato per generare immagini, ma perÂ **classificarle**Â (riconoscere cifre scritte a mano).
- **Consenso Distribuito:**Â Non c'Ã¨ un decisore centrale; ogni singola cella vede solo il suo vicinato e propone una predizione (0-9).Â La classificazione finale emerge dall'**accordo**Â (consenso) della maggioranza delle celle.

![[image-35.png|546x280]]

Ecco un riassunto strutturato dei concetti chiave del documento "Logical Agents", organizzato per essere inserito direttamente nei tuoi schemi di studio.

---
# 8 LOGICAL AGENTS
#rileggere
## 8.1 Agenti Basati su Conoscenza - Knowledge-Based Agents
Questi agenti mantengono uno **stato interno** (conoscenza) per operare in ambienti parzialmente osservabili.
- **Knowledge Base (KB):**Â Un insieme di "frasi" (fatti/regole) espresse in un linguaggio formale che rappresentano ciÃ² che l'agente sa1.
- **Approccio Dichiarativo vs Procedurale:**
    - _Dichiarativo:_Â Diciamo all'agenteÂ _cosa_Â deve sapere (TELL) e chiediamoÂ _cosa_Â fare (ASK).
    - _Procedurale:_Â Codifichiamo direttamenteÂ _come_Â eseguire le azioni.
- **Ciclo di vita:**Â L'agente percepisceÂ $\rightarrow$Â Aggiorna la KB (TELL)Â $\rightarrow$Â Interroga la KB per decidere l'azione (ASK)Â $\rightarrow$Â Esegue l'azioneÂ $\rightarrow$Â Aggiorna la KB con l'azione eseguita (TELL).
![[image-36.png|200x295]]

```c
function KB-Agent(percept) returns an action
	static: KB, aknowledge base
			t, a counter, initially 0, indicating time
	Tell(KB, Make-Percept-Sentence(percept, t))
	action <- Ask(KB, Make-Action-Query(t))
	Tell(KB, Make-Action-Sentence(action, t))
	t <- t+1
	return action
```

#### Wupus World
#### 1. Definizione PEAS (Performance, Environment, Actuators, Sensors)
- **Performance:**Â +1000 per l'oro, -1000 se muori (Wumpus o Pozzo), -1 per ogni passo (incentiva l'efficienza), -10 per scoccare la freccia.
- **Environment:**Â Una griglia 4x4 di stanze.
    - C'Ã¨ unÂ **Wumpus**Â (mostro) che mangia l'agente se entra nella sua stanza.
    - Ci sono deiÂ **Pozzi (Pits)**Â in cui si cade (morte).
    - C'Ã¨ dell'**Oro**.
- **Actuators:**Â Muoversi (Avanti, Gira Dx, Gira Sx), Afferrare (Grab), Tirare (Shoot).
- **Sensors (Cruciale!):**Â L'agente percepisce solo la casella attuale.
    - **Puzza (Stench):**Â Se il Wumpus Ã¨ in una casella adiacente (non diagonale).
    - **Brezza (Breeze):**Â Se un Pozzo Ã¨ in una casella adiacente.
    - **Luccichio (Glitter):**Â Se l'Oro Ã¨ nella casella attuale.
    - **Urlo (Scream):**Â Se il Wumpus viene ucciso (si sente ovunque).
    - **Bump:**Â Se sbatti contro un muro.

#### 2. PerchÃ© serve la Logica?
Un agente a riflessi (che agisce solo sull'istante) fallirebbe.
- _Esempio:_Â Se sento "Brezza", non soÂ _quale_Â casella vicina ha il pozzo. Se mi muovo a caso, muoio.
- **L'Agente Logico**Â usa laÂ **Knowledge Base (KB)**Â per accumulare indizi nel tempo.
    - _Passo 1:_Â Sono in [1,1], niente puzza/brezzaÂ â†’Â deduco che [1,2] e [2,1] sonoÂ **sicure (OK)**.
    - _Passo 2:_Â Vado in [2,1], sento BrezzaÂ â†’Â deduco che c'Ã¨ un pozzo in [2,2] O in [3,1].
    - _Passo 3:_Â Torno indietro e vado in [1,2], sento PuzzaÂ â†’Â deduco Wumpus in [2,2] O in [1,3]
    - _Inferenza:_Â Incrociando i dati, l'agente puÃ² dedurre posizioni sicure che non ha ancora visitato.

### 8.1.2 Logica: Sintassi e Semantica
Per ragionare, serve un linguaggio formale.
- **Sintassi:**Â Le regole che definiscono quali frasi sono "ben formate" (grammatica).
- **Semantica:**Â Definisce la "veritÃ " delle frasi rispetto a unÂ **Modello**Â (un "mondo possibile").Â Esempio: "A Ã¨ vero" significa che nel modelloÂ $m$, la proposizioneÂ $A$Â valeÂ _true_.
- **Entailment (Conseguenza Logica)Â $KB \models \alpha$:**
    - La fraseÂ $\alpha$Â Ã¨ conseguenza logica diÂ $KB$Â se e solo seÂ **in tutti**Â i modelli in cuiÂ $KB$Â Ã¨ vera, ancheÂ $\alpha$Â Ã¨ vera
    - Formalmente:Â $M(KB) \subseteq M(\alpha)$Â -> ossia l'insieme dei mondi dove KB Ã¨ vera Ã¨ contenuto nell'insieme dei mondi doveÂ $\alpha$Â Ã¨ vera
![[image-37.png]]
- Dove $M(\beta)=M(KB)$

**Esempio Wupus**
![[image-38.png|524x242]]
Ora utilizziamo le conoscenze del mondo del wupus e riduciamo le possibili alternative -> passiamo da 8 a 3 possibili
![[image-39.png|524x430]]
### 8.1.3 Inferenza Logica
Il processo per derivare nuove frasi vere da quelle esistenti.
- **Model Checking:**Â Enumerare tutti i modelli possibili per verificare seÂ $KB \models \alpha$Â come ad esempio le Tabelle di VeritÃ .
	- Ãˆ robusto ma costoso ($O(2^n)$)
	- Consiste nel guardare se l'insieme $KB$ Ã¨ sottoinsieme della sentenza $\alpha$
- **Soundness (Correttezza):**Â Un algoritmo Ã¨Â _sound_Â se derivaÂ **solo**Â frasi che sono effettivamente conseguenze logiche (non inventa falsitÃ )
- **Completeness (Completezza):**Â Un algoritmo Ã¨Â _complete_Â se Ã¨ in grado di derivareÂ **tutte**Â le frasi vere possibili

### 8.1.4 Logica Proposizionale (PL)
La forma piÃ¹ semplice di logica, basata su simboli atomici ($P, Q$) e connettivi ($\neg, \wedge, \vee, \Rightarrow, \Leftrightarrow$).
- **ValiditÃ  (Tautologia):**Â Una frase vera inÂ _tutti_Â i modelli (es.Â $P \vee \neg P$).
- **SoddisfacibilitÃ  (SAT):**Â Una frase vera inÂ _almeno_Â un modello.
- **Teorema di Deduzione:**Â $\alpha \models \beta$Â se e solo seÂ $(\alpha \Rightarrow \beta)$Â Ã¨ valida (tautologia)
- **Dimostrazione per Assurdo (Refutazione):**Â $\alpha \models \beta$Â se e solo seÂ $(\alpha \wedge \neg \beta)$Â Ã¨Â **insoddisfacibile**Â (contraddizione).Â _Questo Ã¨ il concetto base della Risoluzione._

### 8.1.5 Algoritmi per la PL
Ci sono due approcci principali per decidere l'entailment:
#### A. Model Checking (Controllo sui Modelli)
Un approccio basato sull'enumerazione dei possibili mondi per verificare l'entailment logico (KBâŠ¨Î±).
1. **Truth Table Enumeration (Enumerazione della Tabella di VeritÃ ):**
    - L'algoritmo costruisce la tabella di veritÃ  completa per le variabili proposizionali coinvolte.
    - Verifica che inÂ **tutti**Â i modelli (righe) in cui laÂ _Knowledge Base_Â Ã¨ vera, anche la fraseÂ Î±Â sia vera.
    - **ProprietÃ :**Â Ãˆ un algoritmoÂ **Sound**Â (corretto) eÂ **Complete**Â (completo), ma la complessitÃ  temporale Ã¨Â **esponenziale**Â ($O(2n)$), rendendolo impraticabile per un numero elevato di simboli.
2. **Local Search (Ricerca Locale):**
    - Le slide menzionano che i metodi di ricerca locale (es. WalkSAT, Simulated Annealing) possono essere utilizzati per trovare un modello soddisfabile.

#### B. Theorem Proving (Deduzione / Inferenza)
L'applicazione di regole di inferenza per derivare nuove frasi (teoremi) da quelle esistenti nella Knowledge Base, senza dover enumerare i modelli.
- **Inference Rules (Regole di Inferenza):**Â Si basano su pattern standard (es.Â _Modus Ponens_,Â _And-Elimination_) per generare nuove frasi in modo deduttivo.
- **Monotonicity (Monotonia):**Â Una proprietÃ  fondamentale della logica: l'insieme delle frasi implicate dalla KB puÃ² solo crescere (o restare uguale) aggiungendo nuove informazioni. Aggiungere conoscenza non invalida mai le conclusioni precedenti.
- **Resolution (Risoluzione):**
    - Ãˆ una regola di inferenza singola che, se usata correttamente, fornisce un algoritmo di inferenzaÂ **Sound e Complete**Â (nota: le slide sottolineano che abbiamo algoritmi completi, e la risoluzione Ã¨ il principale per la logica proposizionale generale).
    - **CNF (Conjunctive Normal Form):**Â Per usare la risoluzione, le frasi devono essere convertite in forma congiuntiva normale (congiunzione di disgiunzioni di letterali).
    - **Refutation (Dimostrazione per Contraddizione):**Â L'algoritmo tipico aggiunge la negazione della tesi (Â¬Î±) alla KB e cerca di derivare laÂ **clausola vuota**Â (contraddizione). Se la si ottiene, alloraÂ Î±Â deve essere vera.

### 8.1.6 Horn Clauses e Chaining
Un sottoinsieme della logica proposizionale molto efficiente.
- **Clausola di Horn:**Â Una disgiunzione conÂ **al massimo un**Â letterale positivo (es.Â Â¬Aâˆ¨Â¬Bâˆ¨C, che equivale aÂ Aâˆ§Bâ‡’C).
- **Vantaggio:**Â L'inferenza con clausole di Horn Ã¨ lineareÂ $O(N)$Â rispetto alla dimensione della KB.

**Algoritmi di Inferenza:**
- **Forward Chaining (Data-Driven):**Â Parte dai fatti noti per derivare nuove conclusioni ("in avanti"). L'idea Ã¨ attivare ogni regola le cui premesse sono soddisfatte, aggiungendo la conclusione alla KB.
    - _Implementazione:_Â Mantiene unÂ **conteggio**Â delle premesse mancanti per ogni regola.Â Quando un simbolo viene inferito, decrementa i contatori delle regole che lo contengono; se un contatore arriva a zero, la regola si attiva.
    - _Pro/Contro:_Â Ãˆ completo, ma puÃ² generare molti fatti inutili se non servono all'obiettivo.
    - Partendo da $A$ e $B$ l'unico punto che posso derivare Ã¨ L dato che gli altri mi ritornano tutti 1 (falso) dato che non sono ancora disponibili.
    - Continua ad esplorare tutto fino a quanto non raggiungo la query $Q$
![[image-40.png|462x288]]
- **Backward Chaining (Goal-Driven):**Â Lavora all'indietro partendo dalla queryÂ q. SeÂ qÂ non Ã¨ noto, cerca le regole che hannoÂ qÂ come conclusione e tenta di dimostrare le loro premesse (sotto-obiettivi) ricorsivamente.
    - _Struttura:_Â Forma un albero di ricerca AND-OR (AND = tutte le premesse di una regola devono essere vere; OR = basta una regola funzionante per provare un fatto).
    - _Pro/Contro:_Â Spesso piÃ¹ efficiente in pratica perchÃ© evita inferenze irrilevanti (base di Prolog).
    - Quando arrivo a questa situazione ho che $A$ e $B$ sono fatti quindi posso prendere definitivamente anche $L$ e di conseguenza risalgo fino a $Q$
![[image-41.png|487x325]]
    
---
# 9 First-Order Logic - FOL Inference
#rileggere
la Logica Proposizionale (PL) alla Logica del Primo Ordine (FOL) Ã¨ dettato dalla necessitÃ  di maggiore espressivitÃ  e composizionalitÃ .
- **Ontological Commitment (Cosa esiste):**Â In FOL il mondo Ã¨ costituito daÂ **Fatti**,Â **Oggetti**Â eÂ **Relazioni**Â (mentre in PL esistono solo Fatti).
- **Epistemological Commitment (Credenze dell'agente):**Â Vero/Falso/Sconosciuto.

### 9.1.1 Sintassi e Semantica
- **Simboli:**Â 
	- Costanti (es.Â $Bacciu$, $Juventus$)
	- Predicati ($BrotherOf$, $>$)
	- Funzioni ($sqrt$)
	- Variabili ($x, y$)
	- Connettivi ($\land, \lor, \neg, \Rightarrow$)
	- Uguaglianza ($=$)
	- Quantificatori ($\forall, \exists$).
- **Modello:**Â Contiene uno o piÃ¹ elementi del dominio (oggetti) e le loro relazioni.
- **Interpretazione:**Â Associa un referente concreto a ogni costante, predicato e funzione.
- **VeritÃ :**Â Una fraseÂ $p(t_1, \dots, t_n)$Â Ã¨ veraÂ _se e sose_Â gli oggetti riferiti dai terminiÂ $t_i$Â sono nella relazioneÂ $p$definita dall'interpretazione.

### 9.1.2 Database Semantics & Closed-World Assumption
Per semplificare la rappresentazione della conoscenza (KB), si adottano spesso assunzioni da database:
1. **Closed-World Assumption (CWA):**Â Le frasi atomiche che non sono note come vere, sono considerate false (niente informazioni "nascoste").
2. **Unique Names Assumption:**Â Ogni costante, predicato o funzione ha un unico referente univoco.

## 9.2 Instanziazione Universale ed Esistenziale
Queste regole permettono di manipolare i quantificatori per inferire nuove frasi.
### 9.2.1 Universal Instantiation - UI
**Definizione:**Â Ogni frase ottenuta sostituendo una variabile quantificata universalmente con unÂ _ground term_Â (termine senza variabili) Ã¨ un'inferenza valida.
Regola di Inferenza:
$$\frac{\forall v ~ \alpha}{\text{Subst}(\{v/g\}, \alpha)}$$
dove $v$ Ã¨ la variabile, $g$ Ã¨ un ground term.
**Es:** Da $âˆ€ğ‘¥\ ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘‘ (ğ‘¥, ğ´ğ¼ğ¹)$ possiamo inferire $Attend(Pietro, AIF), Attend(Paola, AIF), â€¦$

### 9.2.2 Existential Instantiation - EI
**Definizione:**Â Possiamo sostituire una variabile quantificata esistenzialmente con una nuova costante (mai usata prima nel KB).
Regola di Inferenza:
$$\frac{\exists v ~ \alpha}{\text{Subst}(\{v/k\}, \alpha)}$$
dove $k$ Ã¨ una costante Skolem.
**Dettagli Tecnici:**
- **Skolem Constant:**Â La costanteÂ $k$Â "porta in esistenza" un oggetto specifico.
- La nuova KB ottenuta con EIÂ **non Ã¨ logicamente equivalente**Â alla KB originale, ma Ã¨Â _soddisfacibile_Â se e solo se la KB originale lo era.

## 9.3 Riduzione alla Logica Proposizionale - Propositionalization
**Concetto:**Â Ãˆ possibile trasformare unaÂ $KB_{FOL}$Â in unaÂ $KB_{PL}$Â eliminando i quantificatori tramite UI ed EI, preservando l'entailment per frasiÂ _ground_.
Teorema di Herbrand (1930):
Se $KB_{FOL} \models \alpha$, allora esiste una $KB_{PL}$ di dimensione finita tale che $KB_{PL} \models \alpha$
**Algoritmo (Idea intuitiva):**
1. CreareÂ $KB_{PL}$Â istanziando termini fino a profonditÃ Â $n=0, 1, 2...$
2. Fermarsi quando la KB implicaÂ $\alpha$.

**Problema:**Â I simboli di funzione inducono infiniti termini ground ($Father(Father(John))...$). Questo rende il processo inefficiente e genera molte frasi inutili. L'entailment in FOL Ã¨Â **semidecidibile**Â (Turing/Church, 1936).

## 9.4 Unificazione (Unification)
**Definizione:**Â L'unificazione Ã¨ il processo che trova una sostituzioneÂ $\theta$Â che rende identiche due diverse espressioni logiche.
Regola:
$$\text{Unify}(\alpha, \beta) = \theta \iff \alpha\theta = \beta\theta$$
Standardizing Apart:
Se due frasi usano la stessa variabile (es. $x$) ma con significati diversi (ambiti diversi), Ã¨ necessario rinominare una delle variabili per evitare conflitti prima di unificare.
- Esempio:Â $\text{Unify}(Knows(John, x), Knows(y, OJ)) = \{x/OJ, y/John\}$

## 9.5 Generalized Modus Ponens (GMP)
**Definizione:**Â Ãˆ una versione "lifted" (elevata) del Modus Ponens che lavora direttamente con variabili e quantificatori tramite unificazione, applicata aÂ **Definite Clauses**Â (clausole con esattamente un letterale positivo).
Regola di Inferenza:
$$\frac{p'_1, p'_2, \dots, p'_n, (p_1 \land p_2 \land \dots \land p_n \Rightarrow q)}{q\theta}$$
dove $\text{Unify}(p'_i, p_i) = \theta$ per ogni $i$.
**ProprietÃ :**
- **Soundness:**Â Il GMP Ã¨ corretto (sound).
- **Efficienza:**Â Evita la generazione di infinite frasi proposizionali irrelevanti.

## 9.6 Forward & Backward Chaining
Algoritmi applicabili a KB composte solo daÂ **Definite Clauses**Â (es. Horn Clauses). In questo sottoinsieme della FOL, l'inferenza Ã¨ ancora semidecidibile.
### 9.6.1 Forward Chaining (FC)
**Funzionamento:**Â ApproccioÂ _data-driven_. Si parte dai fatti noti, si attivano tutte le regole le cui premesse sono soddisfatte e si aggiungono le conclusioni alla KB. Si ripete finchÃ© non si deriva la query o non ci sono piÃ¹ fatti nuovi.
- **Matching:**Â Ãˆ la parte costosa (NP-hard nel caso generale) perchÃ© bisogna confrontare ogni premessa con ogni fatto.
- **ProprietÃ :**Â ÃˆÂ **Sound**Â eÂ **Complete**Â per le clausole definite.

### 9.6.2 Backward Chaining (BC)
**Funzionamento:**Â ApproccioÂ _goal-driven_. Si parte dalla queryÂ $q$Â e si cercano regole che hannoÂ $q$Â come conclusione. Si generano nuove sottoproposizioni (premesse) ricorsivamente.
- **Struttura:**Â Ricerca Depth-First (DFS), AND-OR search.
- **ProprietÃ :**Â ÃˆÂ **Sound**, maÂ **Not Complete**Â (puÃ² finire in loop infiniti o spazi infiniti se non controllata).
- **Utilizzo:**Â Base per laÂ _Logic Programming_.

### 9.6.3 Esempio "Crime" (Slide 27-31)
_Problema:_Â ProvareÂ `Criminal(West)`.
- **FC:**Â Parte daÂ `Missile(M1)`,Â `Owns(Nono, M1)`,Â `American(West)`Â etc., e sale fino a derivareÂ `Criminal(West)`.
- **BC:**Â Parte daÂ `Criminal(West)`, cerca una regola che implicaÂ `Criminal(x)`, unificaÂ $\{x/West\}$, e procede a verificare le premesse (`American(West)`,Â `Weapon(y)`, etc.).

## 9.7 Logic Programming (Prolog)
**Definizione:**Â La computazione viene vista come inferenza logica su una KB.
- I programmi sono insiemi di clausole nella forma:Â `head :- literal1, ..., literaln`.

**Funzionamento (Prolog):**
- ImplementaÂ **Backward Chaining**Â su clausole di Horn.
- Usa ottimizzazioni e puÃ² gestire aritmetica.
- **Closed-World Assumption:**Â `alive(X) :- notDead(X)`Â (Negation as failure).

**Limiti:**
- Prolog Ã¨Â **incompleto**Â per clausole definite a causa della strategia di ricerca DFS (puÃ² bloccarsi in rami infiniti dove una BFS troverebbe la soluzione).

## 9.8 Resolution (Risoluzione)
**Definizione:**Â Un algoritmo di inferenzaÂ **completo**Â per tutta la FOL (non solo clausole definite), basato sulla prova per contraddizione.
Regola di Inferenza (Lifted Resolution):
$$\frac{p_1 \lor \dots \lor p_k, \quad m_1 \lor \dots \lor m_n}{(p_1 \lor \dots \lor p_{i-1} \lor p_{i+1} \lor \dots \lor m_1 \lor \dots \lor m_{j-1} \lor m_{j+1} \lor \dots)\theta}$$
dove $\text{Unify}(p_i, \neg m_j) = \theta$.

### 9.8.1 Algoritmo: Proof by Contradiction
Per provare cheÂ $KB \models \alpha$, si aggiungeÂ $\neg \alpha$Â alla KB e si cerca di derivare la clausola vuota (contraddizione) convertendo tutto inÂ **CNF (Conjunctive Normal Form)**.
### 9.8.2 Conversione in CNF (Passaggi Dettagliati)
1. **Eliminare Implicazioni:**Â $A \Rightarrow B$Â diventaÂ $\neg A \lor B$.
2. **Spostare la Negazione all'interno:**Â Usare le leggi di De Morgan eÂ $\neg \forall x p \equiv \exists x \neg p$.
3. **Standardizzare le Variabili:**Â Rinominare le variabili affinchÃ© ogni quantificatore usi un nome diverso.
4. **Skolemization:**
    - Sostituire variabili quantificate esistenzialmente ($\exists$) con Costanti di Skolem oÂ **Funzioni di Skolem**.
    - Se l'esistenziale Ã¨ dentro un universale ($\forall x \exists y$),Â $y$Â diventaÂ $F(x)$Â (funzione dell'universale che lo precede).
5. **Rimuovere Quantificatori Universali:**Â Assumere implicitamente che tutte le variabili rimaste sianoÂ $\forall$.
6. **DistribuireÂ $\land$Â suÂ $\lor$:**Â Per ottenere una congiunzione di disgiunzioni.

### 9.8.3 ProprietÃ  della Risoluzione
- **Completezza:**Â SeÂ $KB \models \alpha$, la risoluzione derivarÃ  una contraddizione (Teorema di completezza di GÃ¶del/Robinson).
- **SemidecidibilitÃ :**Â Se la fraseÂ _non_Â Ã¨ vera, la risoluzione potrebbe non fermarsi mai (loop infinito). Non possiamo sapere in tempo finito se "non esiste prova".
## 9.9 Ontologie e Semantic Web

**Definizione:**Â Rappresentazione formale della conoscenza di un dominio specifico.
- **Struttura:**Â Categorie (Classi), Sottocategorie, ProprietÃ  degli oggetti (Relazioni), Istanze (Individui).
- **Vantaggi:**Â Controllo automatico della validitÃ , derivazione di nuove proprietÃ , standardizzazione.
- **Linguaggio:**Â FOL Ã¨ il linguaggio base delle ontologie.

**Strumenti citati:**
- **ProtÃ©gÃ©:**Â Editor open-source per ontologie (Stanford).
- **SPARQL:**Â Linguaggio di query per ontologie.
- **Esempi:**Â Wikidata (DB strutturato per Wikipedia), ConceptNet (Semantic network per common sense).
---
# 10 Planning (Classical & Extensions)

## 10.1 Classical Planning e PDDL

### 10.1.1 Definizione di Classical Planning
Il compito di trovare una sequenza di azioni per raggiungere un obiettivo in un ambiente discreto, deterministico, statico e completamente osservabile1.
A differenza degli agenti logici puri o basati su ricerca generica, il Classical Planning utilizza linguaggi ad-hoc e sfrutta sia l'inferenza logica che la ricerca.

### 10.1.2 Planning Domain Definition Language (PDDL)
Linguaggio standard (1998) basato su Lisp
- **Semantica:**Â "Database semantics" conÂ **Closed-world assumption**Â (ciÃ² che non Ã¨ specificato Ã¨ falso) eÂ **Unique-names assumption**Â (nomi diversi indicano oggetti diversi)
- **Struttura:**Â Ogni programma PDDL distingue tra:
    - **Dominio:**Â Conoscenza comune a tutti i problemi (schema delle azioni)
    - **Problema:**Â Conoscenza specifica dell'istanza (stato iniziale e goal)

### 10.1.3 Stati in PDDL
**Definizione:**Â Uno stato Ã¨ una congiunzione diÂ **ground atomic fluents**
- **Ground:**Â Nessuna variabile (es.Â $At(Truck, Pisa)$Â Ã¨ ground,Â $At(Truck, x)$Â no)
- **Atomic:**Â Singolo predicato senza connettivi logici complessi
- **Fluents:**Â Aspetti del mondo che cambiano nel tempo

### 10.1.4 Azioni in PDDL
Le azioni sono definite tramite unÂ **Action Schema**Â che include:
1. **Action name**Â e lista di variabili.
2. **Precondition:**Â Congiunzione di letterali (positivi o negati) che devono essere veri affinchÃ© l'azione sia applicabile.
3. **Effect:**Â Congiunzione di letterali che descrive come cambia lo stato.

**Esempio (Schema):**
Snippet di codice
```
Action(Fly(p, from, to),
    PRECOND: At(p, from) \land Plane(p) \land Airport(from) \land Airport(to)
    EFFECT: \neg At(p, from) \land At(p, to))
```

## 10.2 Definizione del Problema ed Esecuzione
### 10.2.1 Specifica del Problema
Un problema richiede, oltre al dominio, la definizione di:
- **Initial state:**Â Congiunzione di fluenti ground
- **Goal state:**Â Congiunzione di letterali (puÃ² contenere negazioni)

### 10.2.2 ApplicabilitÃ  ed Esecuzione
Regola di ApplicabilitÃ :
Data un'azione con precondizione $P$, l'azione Ã¨ applicabile in uno stato $s$ sse:
$$S \models P$$
Ogni letterale positivo inÂ $P$Â deve valere inÂ $s$Â e ogni letterale negato inÂ $P$Â non deve essere presente inÂ $s$
Risultato dell'Azione:
Il nuovo stato $s'$ Ã¨ calcolato come:
$$s' = Result(s, a) = (s - DEL(a)) \cup ADD(a)$$

- **DEL(a):**Â Fluenti che appaiono negati negli effetti diÂ $a$Â (vengono rimossi).
- **ADD(a):**Â Fluenti che appaiono positivi negli effetti diÂ $a$Â (vengono aggiunti)

## 10.3 Algoritmi di Planning (Search)
### 10.3.1 Forward State-Space Search
Ricerca nello spazio degli stati partendo dallo stato iniziale (Progression search):
1. Inizia allo stato iniziale.
2. Unifica lo stato corrente con le precondizioni di ogni schema di azione per trovare le azioni applicabili.
3. Applica la sostituzione risultante per ottenere un'azione ground.
4. Applica l'azione e genera il nuovo stato

### 10.3.2 Backward Search
Ricerca partendo dal goal verso lo stato iniziale (Regression search):
1. Considera leÂ **azioni rilevanti**: un'azione Ã¨ rilevante se un suo effetto unifica con uno dei letterali del goal e nessuno dei suoi effetti nega parti del goal.
2. Applica le azioni a ritroso fino a raggiungere lo stato iniziale.
    **Vantaggio:**Â IlÂ _branching factor_Â Ã¨ ridotto rispetto alla forward search perchÃ© si considerano solo percorsi rilevanti per il goal

## 10.4 Euristiche e Pruning
Per migliorare l'efficienza della ricerca si usano euristiche basate suÂ **problemi rilassati**
### 10.4.1 Heuristics
1. **Ignore Precondition Heuristic:**
    - Ogni azione Ã¨ applicabile (si ignorano le precondizioni)
    - Si cerca il numero minimo di azioni i cui effetti uniti soddisfano il goal. 
    - **ProprietÃ :**Â Ãˆ un'euristicaÂ **ammissibile**Â (sottostima sempre la lunghezza della soluzione)
2. **Ignore Delete List Heuristic:**
    - Rimuove i letterali negativi dagli effetti delle azioni
    - Le azioni non possono annullare passi precedenti (progresso monotonico).
    - Spesso usata con Hill-Climbing, ma non garantisce la soluzione ottima (approssimazione)

### 10.4.2 Pruning (Potatura)
1. **Symmetry Reduction:**Â Elimina i rami simmetrici dell'albero di ricerca, mantenendone solo uno (es. l'ordine di oggetti indistinguibili non conta)
2. **Forward Pruning:**Â Taglia rami basandosi su una "preferred action" derivata da un piano rilassato
    - **Nota:**Â Si rischia di potare la soluzione ottima

### 10.4.3 Riduzione dello Spazio degli Stati
- **State Abstraction:**Â MappingÂ _many-to-one_Â dagli stati a una rappresentazione astratta (rimuovendo alcuni fluenti) per ottenere soluzioni approssimate
- **Decomposition:**Â Divide il problema in parti (sotto-goal).Â Si basa sullaÂ **Subgoal independence assumption**: il costo totale Ã¨ approssimato dalla somma dei costi dei sotto-goal risolti indipendentemente

## 10.5 Hierarchical Planning
Per mitigare l'esplosione delle azioni, si usa l'astrazione gerarchica.
### 10.5.1 Concetti Chiave
- **High Level Action (HLA):**Â Un'azione astratta (es.Â $Go(Home, Rome)$) che ammette uno o piÃ¹Â **refinements**(raffinamenti) in sequenze di azioni
- **Refinement:**Â Implementazione di un HLA tramite azioni primitive (o altre HLA).
- **High Level Plan (HLP):**Â Una sequenza di HLA

### 10.5.2 Hierarchical Forward Planning
Algoritmo che parte da un HLP contenente una singola HLA "Act".Â Usa una ricercaÂ **Breadth-First**Â per trovare i possibili raffinamenti di ogni HLA nel piano corrente fino a raggiungere azioni primitive

## 10.6 Estensioni: Sensorless e Online Planning
### 10.6.1 Sensorless Planning (Incertezza)
- **Belief State:**Â L'agente mantiene un insieme di stati possibili in cui potrebbe trovarsi
- **Open-world assumption:**Â Se un fluente non appare, il suo valore Ã¨Â **sconosciuto**Â (differenza cruciale rispetto al PDDL classico)
- **Conditional Effect:**Â Effetti nella forma "whenÂ _condition_:Â _effect_".
    - L'effetto si applica solo se laÂ _condition_Â (formula logica) Ã¨ verificata nel belief state corrente

### 10.6.2 Online Planning (Execution Monitoring)
Utile in ambienti non deterministici. Richiede monitoraggio e replanning.
Tre approcci di monitoraggio:
1. **Action monitoring:**Â Verifica che le precondizioni valgano prima di eseguire l'azione
2. **Plan monitoring:**Â Verifica che il piano rimanente possa ancora avere successo
3. **Goal monitoring:**Â Controlla se esiste un set di obiettivi migliore da perseguire

## 10.7 Tempo, Schedulazione e Risorse
Il classical planning decideÂ _cosa_Â fare; lo scheduling decideÂ _quando_.Â Approccio: "Plan first, schedule later".
### 10.7.1 Rappresentazione e Vincoli
- **Risorse:**Â Possono essere consumabili o riutilizzabili.Â Si usa l'aggregazione numerica (es.Â $Inspectors(2)$) per ridurre la complessitÃ .
- **Partial-Order Plan:**Â Un piano Ã¨ un grafo diretto che permette azioni in parallelo

### 10.7.2 Scheduling (CPM - Critical Path Method)
- **Critical Path:**Â Il percorso nel grafo con la durata totale piÃ¹ lunga.Â Determina la durata dell'intero piano
- **Slack (Margine di scorrimento):**Â Finestra temporale in cui un'azione fuori dal percorso critico puÃ² essere eseguita.
    - Definito daÂ $ES$Â (Earliest Start) eÂ $LS$Â (Latest Start).
    - Le azioni sul percorso critico hannoÂ **Slack = 0**
- **Euristiche:**Â Minimum slack heuristic (schedula prima le azioni con meno slack)