---
aliases:
tags:
  - schema
dg-publish: true
---
# 1 Agents

> Ãˆ qualunque cosa che puÃ² interagire

Possiede:

- **Sensori** â†’ per percepire l'ambiente
- **Attuatori** â†’ per effettuare azioni nell'ambiente

$f:P^{*}\to A$

Dove:

- $P^{*}$ â†’ Ã¨ la storia delle percezioni
- $A$ â†’ Ã¨ l'insieme delle possibili azioni di un agent 

![[allegati/Image.png|711x394]]

+ *The First Agent*
    semplice senza valutare nessun tipo di valutazione, completa l'obbiettivo senza considerare il tempo o i cicli. Non considera i cambiamenti intermedi

```cpp
function Reflex-Vacuum-Agent( [location,status]) returns an action
  if status = Dirty then return Suck
  else if location = A then return Right
  else if location = B then return Left
```

**Rational Agents** â†’ Ã¨ un agente che sceglie le proprie azioni allo scopo di massimizzare le metriche i performance, dato lo storico delle percezioni ^RationalAgent

- Non Ã¨ onnisciente
- Non prevede il futuro
- Non ha sempre successo

*AMBIENTE STATICO* â†’ ossia completamente osservabile e prevedibile (azioni deterministiche con regole chiare) incoraggiando ad imparare comportamenti fissi.

- La soluzione ottima non cambia, stesso obbiettivo
- Soluzione fragile
    - *Esempio Vespa*: che come c'Ã¨ un cambiamento riparte dall'azione che scaturisce la percezione quindi spreco di cicli inutile

## 1.1 PEAS

***P***erformance metric

***E***nviroment

***A***ctuators

***S***ensors

PiÃ¹ Ã¨ ristretto l'ambiente â†’ piÃ¹ l'agente sarÃ  semplice da costruire

- Agent Architecture = $A + S$
- Agent = Architecture + Function
+ Example Taxi

Performance metric: safety, destination, profits, legality, comfort, ....
Environment: streets/freeways, traffic, pedestrians, weather, ...
Actuators: steering, accelerator, brake, horn, speaker/display, â€¦
Sensors: camera, accelerometers, gauges, engine sensors, keyboard, GPSâ€¦

## 1.2 Enviroments - Ambienti

![[allegati/Image (2).png]]

> **Single-Agent** â†’ ossia che nell'ambiente agisce un solo agente e non deve competere con altri

> Real Agent â†’ Ã¨ come il taxi non comprende nessuna delle caratteristiche, il che lo rende molto complicato da realizzare

- **Table-Driven Agent** â†’ ossia prende decisioni consultando una **tabella di lookup** in cui, per ogni possibile percezione (o sequenza di percezioni), Ã¨ giÃ  memorizzata lâ€™azione da compiere.
    - ComplessitÃ  alta per la creazione della tabella
- **Reflex Agent** â†’ insieme di regole condizionali dirette â€œcondizione â†’ azioneâ€ ^ReflexAgent

![[Image (3).png|755x455]]

***Effectiveness-Efficiency Trade-off***

| **TA**                                     | **SRA**                                                            |
| ------------------------------------------ | ------------------------------------------------------------------ |
| Alti costi per gestire e creare la tabella | Riduce i costi creando solo delle condizioni di attivazione        |
| Scala esponenzialmente il tempo â†’ $O(n^2)$ | Scala linearmente il tempo â†’ $O(n)$                                |
|                                            | Semplifica un approccio giÃ  base e stupido â†’ osservabile e piccolo |

## 1.3 Tipi di Agenti

### 1.3.1 Model-Based Agent

^5938be

![[Image (4).png|748x383]]

Aggiunge â†’ *STATE* al [[#^ReflexAgent]

- Stima lo stato attuale dell'ambiente basandosi sullo stato interno dell'agente
    - Lo stato interno dipende dallo storico delle percezioni
- Lo stato interno serve per catturare il modello delle transizioni dell'ambiente
    - Conseguenze dell'azione dell'agente
    - Dinamiche dell'ambiente
- *Sensor Model* â†’ come i sensori dell'agente rappresentano il mondo - rumore e limitazioni
    - Approssima l'intero stato â†’ efficace in ambianti parzialmente osservabili

### 1.3.2 Goal-Based Agent

![[Image (5).png|702x430]]

â“Massimizzare la ricompensa immediata vs massimizzare la ricompensa a lungo termine

- **Search** â†’ come Ã¨ meglio navigare tra le possibili soluzioni? Decision Tree
- **Planning** â†’ sfruttare la conoscenza del mondo per eseguire le migliori azioni

> I modelli goal-based rappresentano il loro obbiettivo (*goal*) e possono modificarlo

### 1.3.3 Utility-Based Agent

![[Image (6).png|727x448]]

> Invece di utilizzare un solo stato, assegna un punteggio ad ogni stato interno che indica quanto Ã¨ buono

- Permette di analizzare varie traiettorie e quindi tenere in considerazione non solo se arrivo al traguardo ma anche come ci arrivo â†’ permette di selezionare la soluzione migliore tra quelle esistenti risparmiando sulle mosse da fare per esempio

Ãˆ fondamentale quando:

- ci sono stati finali **non tutti equivalenti** (es. vincere con 10 mosse vs con 100 mosse),
- lâ€™ambiente Ã¨ **stocastico** (es. backgammon),
- ci sono piÃ¹ obiettivi in conflitto (es. â€œarrivare a destinazione **velocemente ma in sicurezza**â€)

### 1.3.4 Learning-Based Agent

![[Image (7).png|782x329]]

Tutti i modelli di agenti possono essere learning-based o no.

L'apprendimento puÃ² modificare qualche componente.

- **Elementi prestazionali** â†’ i precedenti agenti ricevevano il contesto e sceglievano l'azione da effettuare
- **Critic** â†’ guida l'apprendimento tramite la valutazione del comportamento attuale con la metrica delle performance esterne
- **Problem** â†’ allontanarsi dal comportamento greedy provando nuove azioni (greedy = sceglie l'opzione migliore senza considerare altre soluzioni) 

## 1.4 Summary

- **Agenti** interagiscono con **l'ambiente** tramite **attuatori** e **sensori**
- La **funzione** degli agenti â†’  [Agent = Architecture + Function](craftdocs://open?blockId=4F5E2847-6565-43CC-A12A-EB81130E4DFF&spaceId=4b28628f-7dfd-54ce-27f9-28712867f81f) â†’ descrive cosa fa l'agente in tutte le circostanze
- La **misurazione delle prestazioni** valuta la sequenza dell'ambiente
- Un [[#^RationalAgent]]
- [PEAS](./ğŸ”¥ Schema Slide.md#peas) â†’ descrive gli ambienti di attivitÃ 
- [Enviroments - Ambienti](./ğŸ”¥ Schema Slide.md#enviroments--ambienti) â†’ Osservabile, Deterministico, Episodico, Statico, Discreto
- Molti tipi di Agenti
    - [[#^ReflexAgent]]
    - [[#1.3.1 Model-Based Agent]]
    - [[#1.3.2 Goal-Based Agent]]]
    - [[#1.3.2 Goal-Based Agent]]

---

# 2 Search

> Search Alorithm â†’ dato un problema, ritorna una sequenza di azioni, che formano un path verso lo stato *goal* o *fail*

**Open-Loop** - *offline* â†’ un agente cerca la sequenza ottimale di azioni e successivamente la esegue
- *Completamente osservabile*
- *Deterministico* â†’ risposte dell'ambiente conosciute

**Closed-Loop** - *online* â†’ l'agente valuta continuamente i feedback provenienti dall'ambiente
- *parzialmente osservabile*
- *Stocastico* â†’ le risposte variano
	- Devo controllare il mondo per sapere dove sono, in quale stato

![[Image (8).png|752x448]]

> **Scelte di Design â†’ Guida veloce**

- **Deterministica e completamente osservabile** â†’ problema a stato singolo
    - l'agente sa esattamente in quel stato si troverÃ  â†’ la soluzione Ã¨ una sequenza di stati o azioni
- > **Non osservabile** â†’ problema conforme senza sensore
    - L'agente non percepisce lo stato â†’ soluzione Ã¨ una sequenza di azioni
- *Non deterministico e parzialmente osservabile** â†’ problema di contingenza
    - le percezioni prendono nuove informazioni riguardo allo stato
    - la soluzione Ã¨ un piano contingente o una policy
    - spesso ricerca interlacciata
- **Spazio degli Stati Sconosciuto** â†’ problema di esplorazione (online - closed-loop)
    - l'agente deve scoprire lo spazio degli stati

![[Image (9).png|741x278]]

![[Image (10).png|738x361]]

## 2.1 Tree Search

> Inizia dagli stati giÃ  visitati e gli espande â†’ genera gli stati successivi

- > **Frontiera** â†’ insieme di nodi non espansi che separano i nodi espansi da quelli non raggiunti
- > **Nodi Raggiunti** â†’ *frontiera* + *nodi espansi*
- > Ricerca **Uniforme**

```cpp
function Tree-Search(problem, strategy) return a solution or failure
  inizializza con initial state del problema
  loop do
    if non ci sono candidati return failure
    scegli un nodo da espandere in base alla strategia
    if nodo contine goal state return soluzione
    else espandi il nodo e aggiungi il nodo alla ricerca ad albero
  end
```

![[Image (11).png]]

![[Image (12).png]]

- **State** â†’ uno stato Ã¨ una rappresentazione di una configurazione fisica
- **Nodo** â†’ Ã¨ una struttura dati che costituisce una parte della ricerca ad albero
    - include genitori, figli. profonditÃ  e costo dei path (cammini)
- **La funzione di Espansione**
    - Crea nuovi nodi
    - Riempie i vari campi
    - Usa la funzione *Successore* del problema per creare gli stati corrispondenti

> La ricerca ad albero non corrisponde all'intero spazio degli stati â†’ un insieme di satti finito puÃ² portare ad una ricerca infinita (loops, path ridondanti)

> Nodi nella ricerca ad albero = stati dello spazio

Gli stati sono tutti diversi nello spazio

I nodi non sono unici in un albero di ricerca â†’ se abbiamo un nodo che ritorna ad un altro giÃ  esplorato devo comunque selezionarlo

### 2.1.1 Scegliere una Strategia di Ricerca

> Una strategia di ricerca Ã¨ definita **in base all'ordine con cui i nodi vengono espansi**

Una **strategia** Ã¨ **valutata** tramite queste dimensioni:

- **Completezza** â†’ trova sempre una soluzione
- **ComplessitÃ  di Tempo** â†’ numero di nodi generati o espansi
- **ComplessitÃ  dello Spazio** â†’ numero massimo di nodi in memoria
- **Ottimale** â†’ trova sempre la soluzione con meno costo

ComplessitÃ  di Tempo e Spazio sono misurati in:

- $b$ â†’ fattore massimo di ramificazione dell'albero di ricerca (quanti nodi espando)
- $d$ â†’ profonditÃ  della soluzione migliore (minor costo)
- $m$ â†’ massima profonditÃ  dello spazio degli stati (puÃ² essere $\infty$)

## 2.2 Uninformed Tree Search - Non Informata

> Una ricerca non informata usa solo le informazioni disponibili quando definiamo il problema, non sappiamo quanto siamo lontani dal goal (obbiettivo)

- Breadth-first search
- Uniform-cost search
- Depth-first search
- Depth-limited search
- Iterative deepening search

### 2.2.1 Breadth-First Search
- Frontiera â†’ Ã¨ una coda FIFO
- Espande prima in larghezza poi in profonditÃ 

![[Image (13).png]]

![[Image (14).png]]

| **Completo**    | **Time**                                         | **Space**                      | **Optimal**                                       |
| --------------- | ------------------------------------------------ | ------------------------------ | ------------------------------------------------- |
| **Si**          | $\mathbf{O}(b^d)$                                | $\mathbf{O}(b^d)$              | **Si**                                            |
| Se $b$ Ã¨ finito | Dato che fa tutti i nodi ad una certa profonditÃ  | Prende tutti i nodi in memoria | Se il costo per step Ã¨ $1$ non ottimo in generale |

#### Dijkstra's Algorithm â†’ ricerca costo uniforme

> Le azioni hanno un costo differente â†’ espandere i nodi con il cammino di costo minimo

> La frontiera Ã¨ una coda pesata â†’ i path con costo minore vanno per primi

| **Completo**                                          | **Time**                                                                                                                | **Space**                | **Optimal**                                                |
| ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ------------------------ | ---------------------------------------------------------- |
| **Si**                                                | $\mathbf{O}(b^{1+[\frac{C^*}{\eta}]})$                                                                                  | $\mathbf{O}(b^d)$        | **Si**                                                     |
| Se il costo minimo delle azioni Ã¨ positivo ossia $>0$ | Dove $C^*$ Ã¨ il costo del path ottimale â†’ puÃ² eplorare percorsi a basso costo che non servono a niente puÃ² essere $b^d$ | **Stesso della memoria** | I nodi sono espansi in ordine in base al costo del cammino |

### 2.2.2 Depth-First Search

> Espande prima i nodi in profonditÃ 

> Coda â†’ LIFO â†’ mette i nodi in cima (Ultimo ad entrare primo ad uscire Last In - First Out)

![[Image (15).png]]

![[Image (16).png]]

| **Completo**                 | **Time**                                                                       | **Space**        | **Optimal** |
| ---------------------------- | ------------------------------------------------------------------------------ | ---------------- | ----------- |
| **Si**                       | $\mathbf{O}(b^m)$                                                              | $\mathbf{O}(bm)$ | No          |
| Se $b$ Ã¨ finito e senza loop | Dato che vado prima in profonditÃ  esploro prima tutti i fligli fino al massimo | **Lineare**      |             |

> DFS puÃ² essere implementato in vari modi:

- > **Depth-Limited search** â†’ con profonditÃ  di esplorazione fissata
- > **Iterative Deeping Search** â†’ iterazioni con valore di profonditÃ  che aumenta
- > **Bidirectional Search** â†’ iniziamo la ricerca dallo stato iniziale e dallo stato obbiettivo verso lo stato iniziale

#### Come Riconoscere uno Stato Ripetuto?

- **Ricordare gli stati raggiunti**
    - Posso riconoscere e conservare i path di costo minore
    - Se ho spazio
- Non controllo se il problema non produce path ridondanti
- **Controlla solo i loop**
    - Conserva i genitori per ogni nodo
    - Segue la catena dei parenti per vedere se ricade in un loop
    - Posso seguire la catena fino al root â†’ puntatori al padre
    - **Non c'Ã¨ bisogno di memoria in piÃ¹** solo **piÃ¹ tempo per controllare la catena**

### 2.2.3 Sommario â†’ Uninformed Search

![[Image (17).png]]

## 2.3 Heauristic - Informed Search

> Espandere i nodi piÃ¹ promettenti

> Promettenti Ã¨ una valutazione euristica â†’ propone una stima del minor costo da uno stato al nodo $n$ fino al goal

> Frontiera â†’ Ã¨ una coda pesata e ordinata in base al valore euristico

Best-First Search Algorithm:

- Greedy Search
- A* Search

### 2.3.1 Greedy Search

> Seleziono ad ogni nodo quello con il costo euristico minore

- Non ha meccanismi per trovare il cammino migliore guardando i successivi
- Potrebbe portare a cammini di costo maggiore anche se il primo passo Ã¨ piÃ¹ conveniente

| **Completo**                                                                                                | **Time**                                       | **Space**                     | **Optimal** |
| ----------------------------------------------------------------------------------------------------------- | ---------------------------------------------- | ----------------------------- | ----------- |
| No                                                                                                          | $\mathbf{O}(b^m)$                              | $\mathbf{O}(b^m)$             | No          |
| PuÃ² rimanere incastrato in un loop.<br/>Completo in uno spazio finito con il controllo degli stati ripetuti | Una buona euristca puÃ² portare a miglioramenti | Tiene tutti i nodi in memoria |             |

### 2.3.2 A* Search

> Idea â†’ evitare di espandere i cammini giÃ  costosi

> Controllo se un cammino Ã¨ piÃ¹ costoso di un altro cammino che ho giÃ  esplorato se si allora continuo il cammino con il costo minore e cosÃ¬ via

> In questo modo tengo in considerazione il cammino di costo minimo generale, in modo da poter scegliere quello minore analizzando anche cammini alternativi

**Funzione di Valutazione** â†’ $f(n)=g(n)+h(n)$

- $g(n)$ = costo fino a questo momento per raggiungere $n$ 
- $h(n)$ = costo stimato al goal da $n$ â†’ euristica greedy
- $f(n)$ = costo stimato del path migliore passando attraverso $n$ per raggiungere il goal
- Questa funzione valuta ad ogni nodo il costo decidendo come proseguire

**ComplessitÃ  di Spazio** â†’ la complessitÃ  scala con il numero di nodi espanso

- $A*$ espande tutti i nodi con $f(n)<C^*$
- $A*$ espande qualche nodo con $f(n)=C^*$
- $A*$ non espande nodi con $f(n)>C^*$
- Con $C^*$ costo del path ottimale 

**ComplessitÃ  di Tempo** â†’ Ã¨ esponenziale nell'errore di $h$

A* utilizza un euristica *ammissibile*

- $h(n)\le h^*(n)$ dove $h^*(n)$ Ã¨ il vero costo da $n$
- $h(n)\ge 0$ tale che $h(G)=0$ per ogni goal $G$
- Un euristica ammissibile non sovrastima mai la distanza dal goal

***A** Ã¨ completa**

***A$^*$* Ã¨ ottima se *h* Ã¨ ammissibile**

> **Euristica Consistente** â†’ $h(n)\le c(n,a,n \prime)+h(n \prime),\ \forall n\prime$

- PiÃ¹ forte dell'ammissibilitÃ  â†’ il costo dei cammini Ã¨ monotona crescente
    - Ossia il valore di $f(n)$ non scende mai lungo il cammino
- Quando raggiungi uno stato Ã¨ giÃ  sulla la strada ottimale per arrivare ad $n$
    - Se l'euristica fosse solo ammissibile invece potrei estrarre un nodo con un cammino sub-ottimale e doverlo riconsiderare in seguito

![[Image (18).png]]

### 2.3.3 Problema
> A* visita troppi nodi â†’ richiede che vengano esplorati ed espansi molti nodi
> â†’ **A* Pesato**: sub-optimal ma abbastanza buono
> $f(n)=g(n)+W\ h(n),\ \ W>1$
> Trova una soluzione con costo tra $C^*$ e $W\ C^*$
> -> Spesso vicina a $C^*$

#### Contorni con A*

![[Image (19).png|714x431]]

![[Image (20).png]]

![[Image (21).png]]

#### Beam Search
LaÂ **beam search**Â Ã¨ un algoritmo di ricercaÂ basato suÂ **euristiche** Â che esplora un grafo espandendo il nodo piÃ¹ promettente in un insieme limitato di nodi.
La beam search  simile aÂ [[#2.2.1 Breadth-First Search]] limitata.Â che mira a ridurre i requisiti di memoria. Nella beam search Ã¨ tenuto in considerazione solo un numero predefinito di migliori soluzioni parziali.Â Di conseguenza, Ã¨ considerato unÂ algoritmo greedy.
- Lâ€™euristica non Ã¨ necessariamente ammissibile nÃ© ottimale
- **Obiettivo tipico**: generare sequenze plausibili in problemi di decoding
- Mantiene iÂ **B migliori cammini parziali**Â in un albero di ricerca.
- Ad ogni passo: espande ognuno, valuta i successori, sceglie i B migliori, scarta gli altri.
- Non garantisce la soluzione ottimale

---
# 3 Local Search
Cosa vogliamo:
- Veloce
- Economico
- Abbastanza buono
- Ricerca non sistematica
	- Non ci interessa come ci arriviamo all'obbiettivo ma vogliamo arrivarci

> Local search restituisce una soluzione non un path
> - Non salva gli stati visitati
> - Non esplora l'intero spazio degli stati

## 3.1 Hill Climbing
> Hill-climbing puÃ² rimanere bloccato
	- Ottimo locale
	- Greedy Local Search


> [!NOTE] Gradient Descent
> Se uso il gradiente per valutare e guidare la funzione di ricerca otteniamo la discesa del gradiente.

- si blocca su i massimi locali
- Si blocca su parti pari (flat)
### 3.1.1 Soluzioni
**Random Restart**
- Riesce a scapapre dall'ottimo locale
- Completo ma molto lento
**Sideway Move** -> *movimento laterale*
- Riesce ad uscire dalle parti flat se non sono il massimo
- Fisso -> Si sposta di massimo $k$ step
![[image-1.png|525x302]]

```c title="Hill Climbing"
function Hill-Climbing(problem) returns a state that is a local maximum
    inputs: problem, a problem definition
    local variables: current, a node, neighbor, a node
    
    current â† Make-Node(Initial-State(problem))
    loop do
        neighbor â† a highest-valued successor of current
        if Value(neighbor) â‰¤ Value(current)
        then return State(current)
        current â† neighbor
end
```

## 3.2 Simulated Annealing
> Uscire dall'ottimo locale facendo un passo random
> - un passo random non rimane mai incastrato -> ma Ã¨ lento


> [!important] Simulated Annealing
> Corrisponde a -> *hill climbing $+$ random moves*
> - I movimenti random diventano meno frequenti nel tempo
> - I movimenti casuali migliorano con il tempo
> - **Temperature** -> Ã¨ un parametro che controlla il trade-off tra hill-climbing e movimenti random
> 	- La temperatura cambia con il tempo permettendo all'inizio di fare molti passi casuali e mano a mano che vado avanti i passi casuali diminuiscono
> 	- Alla fine $T=0$ -> nessun passo casuale


### 3.2.1 Temperature Scheduling
$$
e^{\frac{x}{T}},\quad T>0,\quad x>0
$$
- $x$ Grande -> Ã¨ una mossa molto sbagliata, c'Ã¨ una bassa probabilitÃ  di accettare lo spostamento
- Aumentando $T$ -> aumento la probabilitÃ  di accettare spostamenti
Se $T\to 0$ allora anneiling diventa deterministico -> non effettua mosse casuali
- Accetta solo spostamenti buoni $e^{\frac{x}{T}}\to 0$ 
**Anneiling Scheduling** -> inizia con T grande e lo diminuisce con il passare del tempo
- Se T viene diminuite abbastanza lentamente l'algoritmo converge al *massimo globale* 
	- Se invece Ã¨ troppo lento visito tutti gli stati -> lentissimo
- *Diminuzione Esponenziale* -> $T_t = k\ T_{t-1},\quad 0<k<1$
- *Power-law Decrese* -> $T_t=\frac{T_0}{t^a}$ es: $a=1$
### 3.2.2 Local Beam Search
> Invece di prendere solo la prima soluzione prendo le prime $k$
> Espando tutto e seleziono i top-$k$ stati
> - Ha senso quando la funzione non converge sicuramente


> [!important] Funzionamento
> Ãˆ la versione stocastica della [[#Beam Search]]
> Gli stati interagiscono con gli altri -> abbandono i path con meno importanza, concentrandomi su quelli piÃ¹ promettenti
> - Non Ã¨ una ricerca parallela su $k$ path
> - *Local beam -> parte da k stati, tiene i migliori k successori globalmente.*
> - **Non garantisce ottimalitÃ **

#### Beam search decoder in NLP

NLP -> Natural Language Processing
Tutto ciÃ² che concerne l'elaborazione del linguaggio naturale -> nell'esempio la traduzione da inglese ad italiano tramite encoder (trasforma la frase in vettori-embedding) e deconder con vari livelli di linar model e softmax per avere l'output
*Esempio:*
- *Input*: how are you?
- *Output*: come stai?
![[image-2.png]]
##### 3.3 Beam Search vs Greedy Search in NLP
**Greedy Search** -> semplicemente prende la parola piÃ¹ simile per ogni posizione
- Ogni parola Ã¨ considerata indipendente
- Funziona ma puÃ² essere migliorato
**Beam Search**
- Trova il path migliore e tiene in considerazione cosa ha giÃ  selezionato
- Prende $N$ migliori candidati per ogni posizione
- Espande gli attuali $N$ migliori candidati e prende i migliori $N$
- Nell'ultima posizione prende il miglior risultato tra gli $N$ candidati finali
##### Beam Search Decoder in NLP
Bisogna eseguire il decoder $N$ volte per ogni posizione:
- Costa tanto
- Ma il risultato spesso Ã¨ il migliore
![[image-3.png]]
## 3.3 Ambiente Non Deterministico

> Alcune azione hanno un risultato non deterministico

Belief State (Stato di Fede) -> un insieme di stati futuri per ogni stato corrente e coppia di azioni
- Non sai in anticipo dove ti porta l'azione
Conditional Plan -> la solzuione Ã¨ un albero (if-then-else chain) non una sequenza
- Puoi eseguire la soluzione e in base al risultato dell'ambiente scegliere l'azione corretta
- **AND-OR Search Trees** -> Ã¨ la soluzione per il conditional plan
	- **AND node** -> Fornito dall'ambiente insieme al set di tutti i possibili stati successivi
	- **OR node** -> Fornito dall'agente insieme all'azione da eseguire
### 3.3.1 Esempio Erratic Vacuum World
Non deterministic action
- Se lo stato corrente Ã¨ sporco, "suck" pulisce il quadrato corrispondente ma avvolte pulisce anche il quadrato adiacente
- Se lo stato corrente Ã¨ pulito, "suck" puÃ² depositare la sporcizia

"Left" e "Right" sono deterministiche

**Belief state**
- $Result(s,a)=?$
**Conditional plan**
- Â«suckÂ»; `if 5 do [Â«rightÂ», Â«suckÂ»]`
- Starting from state 1
- $Result(1, Â«suckÂ»)= \{5, 7\}$
> In questo modo se "suck" pulisce entrambi io avrÃ² che `if 5` non sarÃ  rispettato quindi faccio un altra azione
### 3.3.2 AND-OR Search Tree
- Necessario per evitare loop
- L'algoritmo Ã¨ leggermente piÃ¹ coprente ma l'albero si espande ancora
- Esiste una versione di $A^*$ per l'albero AND-OR

**Solzuione**
Un sotto-albero dove:
-  ogni foglia Ã¨ un goal
- Ha un azione per ogni nodo OR
- Include un ramo per tutti i risultati dei nodi AND
![[image-4.png|465x412]]

## 3.4 Searching Senza Osservazioni
- Non ci sono percezioni
- Senza sensori -> *Conformant Problem*

> L'agente deve stimale lo stato
> **Soluzione** -> Ã¨ una sequenza di azioni
> - Non puÃ² essere un conditional plan se non puoi ricevere un feedback dall'ambiente (non posso fare `if`)

**Belief Space** -> lo spazio delle rappresentazioni interne di ciÃ² che credo sia vero sul mondo (non certe)

Le azioni possono portare informazioni su possibili stati futuri
- *belief state* $= \{1,2,3,4,5,6,7,8\}$, *action*$=Â«rightÂ»$, *next belief state* $= ?$
- $\{2, 4, 6, 8\}$
#### Search in Conformant Problem
> Idea -> cercare nel belief space (completamente osservabile) invece che nello spazio degli stati (non osservabile)

- Belief state space = powerset of states -> 2ğ‘› possible beliefs instead of N
- Initial belief state = all the N states
- Le azioni *Actions(B)* che posso fare sono limitate dall'unione e l'intersezione di tutte le azioni *Actions(S)* pr ogni $S$ in $B$
- Transition model (for deterministic actions): Quando esegui unâ€™azione, non vai in un singolo stato, ma in unÂ **nuovo belief state**.
- **Goal Test**
	- **Possibly achieve**Â il goal: se almeno uno degli stati nel belief Ã¨ un goal.
	- **Necessarily achieve**Â il goal: seÂ _tutti_Â gli stati del belief sono goal.
- **Action cost**: Se lâ€™azione costa in modo diverso nei vari stati, allora devi trovare un criterio
	- Questo serve per decidere quanto â€œvaleâ€ unâ€™azione in un belief state.

##### Check Visited States
###### Come controllare gli stati visitati?
- Esempio:
	- ğµ1 = {5, 7}
	- ğµ2 = {1, 3, 5, 7}
- PuoiÂ **potare**Â (cioÃ¨ scartare) i rami di ğµ2, perchÃ© qualsiasi soluzione valida per $\{1, 3, 5, 7\}$ sarÃ  anche una soluzione per $\{5, 7\}$.
ğŸ‘‰ In generale: puoi scartare i rami che appartengono aÂ **superset**Â di belief state giÃ  visitati.
###### Ragionamento inverso
- Se hai giÃ  risolto il problema per un certo belief state, allora lo hai risolto anche per qualsiasiÂ **sottoinsieme**Â di esso.
###### ComplessitÃ 
- Nonostante queste ottimizzazioni, la ricerca nei problemi conformant (cioÃ¨ senza osservazioni, dove lâ€™agente non sa esattamente dove si trova) rimane molto costosa, perchÃ© lo spazio dei belief state Ã¨ enorme.
###### Ricerca incrementale nei belief state
1. Trova una soluzione che funziona per ilÂ **primo stato**Â in B.
2. Controlla se quella soluzione funziona anche per gli altri stati in B.
    - Se sÃ¬ â†’ fermati.
    - Se no â†’ riprova.
3. **Fallire velocemente**Â (cioÃ¨ accorgersi presto se una soluzione non funziona) puÃ² migliorare la velocitÃ  di convergenza
# 4 CSP - Problema di Soddisfazione del Vincolo
> **C**onstraint **S**atisfaction  **P**roblem

Introduciamo la rappresentazione degli stati come rappresentazione fattoriale
- Stato -> $X=\{X_1,\ X_2,\ ...,\ X_n\}$ con $n$ variabili
- Dominio -> $X_i\in D_i$ 
- La **conoscenza del dominio** Ã¨ espressa con un set di **vincoli C**
	- Es. $X_1\le 0,\ \ X_2\in [1,3], ...$ 


> [!info] $CSP(X, D, C)$
> **Goal Test** -> Assegno valori alle variabili per soddisfare i vincoli
> - **Completo vs Assegnazione Parziale** -> tutte o solo una parte delle variabili hanno un valore assegnato
> - **Assegnamento Consistente** -> Assegnare variabili soddisfa le condizioni
> - **Soluzione Parziale** -> un assegnamento parziale che Ã¨ consistente
> 
> Obbiettivo CSP Ã¨ trovare una soluzione completa e consistente


> [!important]  Factored Representation
> State Rappresentation -> Qui rappresentiÂ **ogni stato possibile come unâ€™unica entitÃ  indivisibile**Â (un â€œpuntoâ€ nello spazio degli stati).
>  
> Factored Representation -> Qui inveceÂ **uno stato non Ã¨ rappresentato come un singolo nodo â€œpiattoâ€**, ma comeÂ **una tupla di valori di variabili**:
> $$\text{Stato }S=(X_1â€‹=v_1â€‹,\ X_2â€‹=v_2â€‹,\ â€¦,\ X_nâ€‹=v_nâ€‹)$$
> Quindi:
> - Invece di rappresentareÂ **esplicitamente tutti gli stati**,
> - **rappresenti le variabili e i vincoli**Â che definiscono implicitamente lo spazio degli stati.


### 4.1.1 Esempio - Map Coloring
*Obbiettivo* -> colorare gli stati adiacenti con un colore diverso
![[image-9.png|274x226]]
Quindi abbiamo:
- $X = \{WA,\ NT,\ SA,\ Q,\ NSW,\ V,\ T\}$
- $D ={r,\ g,\ b}\ \ \forallğ‘‹_ğ‘–$
- $C =$
	- $WA â‰  ğ‘ğ‘‡$
	- $ğ‘Šğ´,\ ğ‘ğ‘‡ âˆˆ { ğ‘Ÿ,\ ğ‘” ,\ ğ‘Ÿ,\ ğ‘ ,\ ğ‘”,\ ğ‘Ÿ ,\ ğ‘”,\ ğ‘,\ â€¦ }$
	- Entrambi -> ripetere per ogni coppia di stati

![[image-10.png|274x242]]
> Le variabili sono i nodi e gli archi sono i vincoli
> Il grafico aiuta la ricerca, come possiamo vedere dal nodo "Tasmania" che non ha vincoli quindi possiamo assegnarli un colore casuale

| **Dominio**                    | **Vincoli**    | **RisolvibilitÃ **                                         |
| ------------------------------ | -------------- | --------------------------------------------------------- |
| Discreto finito                | Qualsiasi      | Decidibile (es. backtracking, CSP classici)               |
| Discreto infinito              | Lineari o meno | Difficile â†’ servono metodi speciali o bounds              |
| Discreto + vincoli lineari     | Lineari        | Solvibile (anche se NP-hard)                              |
| Discreto + vincoli non lineari | Non lineari    | **Indecidibile in generale**                              |
| Continuo + vincoli lineari     | Lineari        | **Solvibile in modo efficiente**Â (programmazione lineare) |
| Continuo + vincoli non lineari | Non lineari    | PiÃ¹ complesso; decidibilitÃ  dipende dal tipo di vincolo   |
## 4.2 Tipi di Vincoli
- **Unitario**
	- Una variabile coinvolta -> $A\ne 1$
- **Binario**
	- Due variabili coinvolte -> $A\le B$
- **Globale** -> vincoli con $n$ variabili coinvolte
	- Non necessariamente tutte le variabili
	- Alldif$(A,\ B,\ C,\ D)$
- **Vincoli di Preferenza**
	- Vincoli leggeri che rappresentano le preferenze
	- Modellato con un costo associato a ciascun incarico -> problema di ottimizzazione vincolato
	- Es. Linear Model

### 4.2.1 Vincoli Hyper-Graph
Non posso rappresentare i vincoli come archi dato che per definizione gli archi connettono solo due nodi
![[image-11.png]]
- **Nodi:**Â I cerchi (variabili lettera:Â F,T,U,W,R,O) e i quadrati (variabili di riporto:Â C1â€‹,C2â€‹,C3â€‹).
- **Archi:**Â Le linee che collegano i nodi rappresentano le relazioni o i vincoli tra le variabili definite dalle equazioni.

> Per vincoli con dominio finito, puoi sempre ridurre un hyper-graph in un grafico normale
> - vincoli globali possono essere divisi in un set di vincoli binari

Qualche vincolo popolare (es. Alldiff) gode di algoritmi studiati appositamente
- Non serve ridurre il grafo a vincoli binari
- Sono piÃ¹ facili da leggere da una prospettiva umana

## 4.3 Come Cercare in CSP
**Gli stati sono definiti dai valori assegnati finora**
*Initial State - stato iniziale:*
- Assegnazione vuota -> $\{\}$
- Fattore di ramificazione alla radice -> $=nd$

*Azioni:*
- Assegna un valore alle variabili non assegnate che non causa un conflitto con l'assegnamento corrente
- Fattore di ramificazione al secondo livello -> $=(n-1)d$

*Ogni soluzione completa appare alla profonditÃ  $n$:*
- Problema -> $n!$ o $d^n$ foglie ([[#^6f1c56|Riduce ComplessitÃ ]])
- Risoluzione -> Sfrutta la struttura del problema 

*Assegnamenti illegali* -> ritornano fallimento - non risolvibile
*Goal Test* -> controlla se l'assegnamento attuale Ã¨ completo

### 4.3.1 Backtracking Search

> Fissa un ordine tra gli stati -> rimuove la complessitÃ  $n!$
> Ossia -> Non câ€™Ã¨ bisogno di considerare tutti gli ordini delle variabili: possiamoÂ **fissare a priori un unico ordine**, e assegnare le variabili sempre in quellâ€™ordine.


> [!attention] Assegnazione Stati vs Singolo Nodo
> Se tu considerassiÂ **ogni stato come una possibile sequenza parziale di assegnamenti senza un ordine fisso tra le variabili**, allora:
> - Devi esplorareÂ **tutte le possibili permutazioni delle variabili**
> - Ci sonoÂ $n!$Â possibili ordini diversi in cui potresti assegnarle.
> 
> Invece di avereÂ **ogni nodo come â€œuna configurazione interaâ€ di n variabili**, ogni nodo nella ricerca corrisponde aÂ **â€œassegnare un valore a una singola variabileâ€**, secondo lâ€™ordine prefissato. ^6f1c56

**Ogni nodo Ã¨ considerato un singolo assegnamento** -> non l'intero stato
- solamente $d^n$ foglie 
- Costruisco un albero con ogni nodo che Ã¨ un assegnazione di una singola variabile non di un assegnazione di piÃ¹ variabili

Algoritmo non informato per CSP
- Depth-first co assegnamento a variabile singola
- La soluzione Ã¨ ancora un cammino nell'albero


> [!seealso] Logica Algoritmo
> Sceglie ripetutamente una variabile non assegnata, e poi prova tutti i valori nel dominio di quella variabile a turno, cercando di estendere ciascuno in una soluzione tramite una chiamata ricorsiva. Se la chiamata ha successo, la soluzione viene restituita e, se fallisce, l'assegnazione viene ripristinata allo stato precedente e proviamo il valore successivo. Se nessun valore funziona, restituiamo il fallimento.

![[image-12.png|536x438]]
- Con stati atomici, gli algoritmi non informati non sfruttano l'euristica
- Nella rappresentazione fattorizzata esistono euristiche indipendenti dal dominio
	- Possono migliorare la velocitÃ  di ricerca

## 4.4 Miglioramenti 
- [[#4.4.2 Miglioramenti Tramite Inferenza|4.4.2 Miglioramenti Tramite Inferenza]]
### 4.4.1 Miglioramenti di Ricerca
- [[#MEVs - Minimum Remaining Values|MEVs - Minimum Remaining Values]]
- [[#Degree Heuristic - Gradi di Euristica|Degree Heuristic - Gradi di Euristica]]
- [[#Least-Constraint Values|Least-Constraint Values]]

#### MEVs - Minimum Remaining Values
> Scegli la variabile con meno valori legali -> utilizza quella per avere un approccio "**prima fallimenti**" cosÃ¬ da fare una specie di **pruning** sui nodi con meno speranze cosÃ¬ da eliminarli subito

#### Degree Heuristic - Gradi di Euristica
>LaÂ **degree heuristic**Â sceglie, tra le variabili non ancora assegnate,Â **quella che Ã¨ coinvolta nel maggior numero di vincoli con altre variabili non assegnate**, cosÃ¬ da massimizzare la riduzione futura del branching.  
>Serve spesso comeÂ **criterio di spareggio**Â quando piÃ¹ variabili hanno lo stesso numero di valori legali (dopo la MRV).

#### Least-Constraint Values
> Questa Ã¨ la descrizione dellâ€™**euristica LCV (Least Constraining Value)**Â ğŸ‘¨â€ğŸ«
> - **Idea**: quando scegli il valore per una variabile, assegnaÂ **quello che elimina il minor numero di valori possibili per le altre variabili ancora non assegnate**, cosÃ¬ daÂ **lasciare piÃ¹ libertÃ  futura**.
> - **Differenza con MRV**:
> 	- **MRV (Minimum Remaining Values)**Â sceglieÂ _quale variabile_Â assegnare â†’ cerca di fallire presto per potare lâ€™albero.
> 	- **LCV (Least Constraining Value)**Â sceglieÂ _quale valore_Â assegnare â†’ cerca diÂ **non restringere troppo**Â il resto del problema, per non imboccare precocemente un vicolo cieco.
> 
> ğŸ‘‰ Entrambe accelerano la ricerca, maÂ **da prospettive opposte**: MRV anticipa i fallimenti, LCV mantiene aperte le possibilitÃ .

![[image-13.png]]

### 4.4.2 Miglioramenti Tramite Inferenza
- [[#Forward checking|Forward checking]]
- [[#Arc Consistency|Arc Consistency]]

#### Forward checking
Assegna un valore aÂ $X$  
Per ogni vincolo traÂ $X$Â eÂ $Y$:  
â†’Â **elimina**Â dal dominio diÂ $Y$Â tutti i valori che violano il vincolo con $X$.  
Se il dominio di Y rimane vuoto â†’Â **backtrack!**
- Significa che lâ€™assegnamento precedente era errato.
- Esistono molte strategie di backtracking

ğŸ‘‰ Questo Ã¨ un esempio diÂ **inferenza nei CSP**:
- Non si fa ricerca, siÂ **riduce lo spazio delle possibili assegnazioni**.
- Lâ€™inferenza puÃ² essere eseguitaÂ **prima**Â eÂ **durante**Â la ricerca.
	- Ad esempio,Â **prima della ricerca**Â si puÃ² imporre laÂ **coerenza di stato**Â eliminando i valori di dominio che violano vincoliÂ **unari**.

![[image-14.png]]

#### Arc Consistency
**coerenza dâ€™arco**
- UnaÂ **variabile X Ã¨ arc-consistent**Â rispetto a unâ€™altra variabile $Y$ se:  
$$\forall x \in \text{dom}(X), \ \exists y \in \text{dom}(Y) \ \text{tale che} \ (x,y) \ \text{rispetta il vincolo tra X e Y}.$$

ğŸ‘‰ In altre parole:Â **ogni valore nel dominio di X deve avere almeno un valore compatibile nel dominio di Y**.   
- UnÂ **grafo Ã¨ arc-consistent**Â seÂ **tutte le variabili sono arc-consistent rispetto a tutte le altre**Â (cioÃ¨ per ogni arco Xâ†’Y vale la condizione sopra).

> [!tip] Grafo Arc-Consistency
> - **Per ogni valore possibile di $X$**, deve esserciÂ **almeno un valore compatibile in $Y$**.
> - Se esiste un valoreÂ $x$Â che non ha nessunÂ $y$Â compatibile, allora quelÂ $x$Â va eliminato dal dominio diÂ $X$.
> 
> Quando elimini un valoreÂ xxÂ dalÂ **dominio di una variabile X**Â tramite AC-3 o altra inferenza
> - StaiÂ **riducendo lo spazio di ricerca**, perchÃ© quellâ€™assegnamentoÂ **non verrÃ  mai esplorato**Â durante il backtracking.
> - QuestoÂ **non elimina variabili**, elimina solo possibili valori per ciascuna variabile.

##### âš™ï¸Â Come rendere un grafo arc-consistent: algoritmo AC-3
Lâ€™algoritmoÂ **AC-3**Â Ã¨ un procedimento di inferenza cheÂ **rende il CSP arc-consistente**, oppure segnala fallimento se qualche dominio si svuota.

**Passi principali:**
1. **Inizializza una coda**Â con tutti gli archi (X â†’ Y) del CSP.
2. **Ripeti finchÃ© ci sono cambiamenti**:
    - Estrai un arco X â†’ Y dalla coda.
    - **Rendi X arc-consistent rispetto a Y**:
        - Elimina dal dominio di X tutti i valori x per cuiÂ **non esiste nessun y nel dominio di Y compatibile**con x.
    - Se il dominio di X Ã¨ stato modificato
        - Aggiungi alla codaÂ **tutti gli archi K â†’ X**, cioÃ¨ quelli che puntano verso X, perchÃ© il cambiamento potrebbe renderli incoerenti.
    - Se il dominio di una variabile si svuota â†’Â **fallimento**Â (il CSP non ha soluzione).
##### â±ï¸Â ComplessitÃ 
Per un CSP con:
-  **c**Â = numero di archi binari
- **d**Â = dimensione massima dei domini
ğŸ‘‰ La complessitÃ  Ã¨Â **O(c Â· dÂ³)**

- **Ogni arco**Â puÃ² essere controllato inÂ **O(dÂ²)**Â (perchÃ© bisogna confrontare ogni valore di X con ogni valore di Y).
- Ogni arco puÃ² essere reinserito nella coda al massimoÂ **d volte**, perchÃ© ogni variabile puÃ² perdere al massimo d valori â†’ da quiÂ **O(c Â· dÂ³)**.

##### ğŸ“Â Riassunto finale
- **Arc-consistency**Â = ogni valore di ogni variabile ha un supporto compatibile nei vicini.
- **AC-3**Â = algoritmo di inferenza che:
    - non fa ricerca,
    - riduce i domini eliminando valori impossibili,
    - puÃ² essere eseguito prima o durante la ricerca per potare fortemente lo spazio
- Se dopo AC-3 qualche dominio Ã¨ vuoto â†’Â **il CSP non Ã¨ risolvibile**.

#### Maintaining Arc Consistency
MAC (Maintaining Arc-Consistency)Â **estende il Forward Checking**Â chiamando AC-3 dopo ogni assegnamento durante la ricerca.
- Dato un assegnamento a ($X$), si chiama AC-3 con una coda contenente tutti gli archi ($Y \to X$), per tutte le variabili ($Y$) ancora non assegnate.
- Se AC-3 fallisce â†’Â **backtrack**.

MACÂ **potatura piÃ¹ aggressiva rispetto al Forward Checking**, perchÃ© verifica ricorsivamente eventuali incoerenze.

## 4.5 Local Search for CSP
Formulazione aÂ **stato completo**: ogni stato assegna un valore aÂ **tutte le variabili**.
- La ricerca cambia il valore diÂ **una variabile alla volta**.
    - Quindi, lo stato iniziale Ã¨ unÂ **assegnamento casuale di tutte le variabili**.

**Euristica Min-Conflicts**: scegli il valore cheÂ **violerebbe il minor numero di vincoli**, cosÃ¬ da avvicinarti alla soluzione.
- Ãˆ possibileÂ **pesare diversamente i vincoli**Â per dare prioritÃ  a quelli piÃ¹ importanti,
    - perchÃ© la soluzione finale potrebbe non essere ottimale rispetto a tutti i vincoli.
### 4.5.1 Min-Conflict Heuristic
**Euristica Min-Conflicts**: scegli il valore cheÂ **violerebbe il minor numero di vincoli**.
- Funziona molto bene,Â **tranne in una regione critica**.
    - Ad esempio, risolve il problema delleÂ **N-queens**Â in un numero di passi quasi costante, indipendentemente daÂ $N$Â (circa 50 passi).
### 4.5.2 Sfruttiamo la Struttura del Problema
Anche avendoÂ **un arsenale di milioni di trucchi**Â (e ce ne sono molti altri!), iÂ **CSP rimangono comunque molto difficili in generale**.
- Caso peggiore: ( $d^n$ ) foglie nellâ€™albero di ricerca (come abbiamo visto).
#### Scomposizione in sottoproblemi indipendenti
- Alcuni problemi possono essereÂ **divisi in sottoproblemi indipendenti**, ad esempio â€œTâ€ e â€œmainlandâ€.
- GliÂ **algoritmi sui grafi**Â possono identificare questeÂ **sottostrutture nel grafo dei vincoli**, come leÂ **componenti connesse**.
- LeÂ **componenti connesse**Â sono indipendenti tra loro, quindi laÂ **soluzione complessiva**Â del CSP Ã¨ semplicementeÂ **lâ€™unione delle soluzioni di ciascuna componente**.
#### âš¡ Vantaggio computazionale

- Se una componente ha $c < n$ variabili, allora la ricerca su quella componente scala come $d^c$ , invece di $d^n$.
- In altre parole,Â **scomporre il problema in sottoproblemi riduce drasticamente la complessitÃ **, sfruttando lâ€™indipendenza tra le variabili.

### 4.5.3 Tree-structured CSPs
Se ilÂ **grafo dei vincoli Ã¨ un albero**, risolvere il CSP costa $O(n d^2)$
- Lineare in $n!$

Dato unÂ **grafo dei vincoli senza cicli**Â (un albero), lâ€™algoritmo di ricerca procede cosÃ¬:
1. **Ordinamento topologico**: scegli una qualsiasi variabile come radice e ordina le variabili in modo che ciascuna sia figlia della propria variabile genitore.
2. **Rendi lâ€™albero arc-consistente**; se fallisce â†’ termina con fallimento.
    - Ci sono ( n-1 ) archi e ( d^2 ) combinazioni di valori da controllare per ciascun arco.
3. **Assegna a ogni nodo**Â un qualsiasi valore consistente con quello del genitore; se non esiste â†’ fallimento

> In questo casoÂ **non serve mai fare backtracking**.

![[image-15.png]]

### 4.5.4 ğŸŒ³ â€œPiantare alberi dove non ce ne sonoâ€

- Possiamo risolvereÂ **rapidamente CSP strutturati ad albero**.
- Ma cosa succede se il CSPÂ **non Ã¨ un albero**? Possiamo provare aÂ **forzarlo a diventarlo**.
#### Procedura per trasformare un CSP in albero
1. **Assegna un valore a una variabile**Â (scegli un nodo).
2. **Rendi gli archi coerenti**Â (arc-consistency).
3. **Rimuovi la variabile assegnata**

- Se il grafo risultanteÂ **puÃ² essere trasformato in un albero**, allora puoiÂ **verificare rapidamente se il CSP Ã¨ risolvibile**.
- Altrimenti, puoi provareÂ **un altro valore per la variabile**Â oÂ **scegliere un altro nodo da assegnare**.
#### âš¡ Cutset Conditioning

- Supponiamo di dover provare ( c ) variabili (ilÂ **cutset**), cioÃ¨ quelle che â€œrompono i cicliâ€ del grafo
- ComplessitÃ  approssimativa:  
$$
    O(d^c (n-c) d^2)  
$$
- Qui:
    - $d^c$ = tutte le combinazioni di valori per le variabili nel cutset
    - $n-c$ = numero di variabili rimanenti che ora formano un albero
    - $d^2$ = costo per verificare la coerenza sugli archi rimanenti

- In pratica, riduce drasticamente il problema, perchÃ©Â **la parte ciclica viene gestita separatamente**, mentre il resto Ã¨ un albero â†’ risolvibile in tempo lineare.
![[image-16.png]]
## 4.6 Riassunto

### 4.6.1 I. Fondamenti dei Constraint Satisfaction Problems (CSP)

#### 5.1.1 Definizione e Struttura dello Stato

- **Stato Fattorizzato:**Â Lo stato $X$ Ã¨ rappresentato da un insieme di $n$ variabili ${X_1, X_2, \dots, X_n}$.
- **Dominio ($D_i$):**Â Ogni variabile $X_i$ assume valori da un dominio $D_i$.
- **Vincoli ($C$):**Â La conoscenza del dominio Ã¨ espressa da un insieme di vincoli che devono essere soddisfatti (es. $X_1 \le 0$, $X_2 \in {1, 3}$).
- **Obiettivo del CSP:**Â Trovare unaÂ **soluzione completa e consistente**.
    - **Assegnazione Completa:**Â Tutte le variabili hanno un valore assegnato.
    - **Assegnazione Parziale:**Â Solo alcune variabili hanno un valore assegnato.
    - **Assegnazione Consistente:**Â Le variabili assegnate soddisfano i vincoli.
    - **Soluzione Parziale:**Â Un'assegnazione parziale che Ã¨ consistente.

#### 4.5.2 Rappresentazione dei Vincoli

- **Grafo dei Vincoli:**Â Le variabili sono rappresentate comeÂ **nodi**Â e i vincoli sono rappresentati comeÂ **archi**. La struttura del grafo puÃ² aiutare la ricerca.
- **Vincoli Comuni:**
    - **Vincoli Unari:**Â Coinvolgono una sola variabile (es. $A \ne 1$).
    - **Vincoli Binari:**Â Coinvolgono due variabili (es. $A \le B$).
    - **Vincoli Globali:**Â Coinvolgono un numero qualsiasi di variabili (non necessariamente tutte), es. $Alldiff(A, B, C, D)$.
- **Iper-Grafo dei Vincoli:**Â Necessario quando ci sono vincoli globali, poichÃ© un arco per definizione connette solo due nodi.
    - **Riduzione:**Â Per i vincoli con dominio finito, un iper-grafo puÃ² sempre essere ridotto a un grafo normale suddividendo i vincoli globali in un insieme di vincoli binari. Questo Ã¨ utile se si vogliono usare algoritmi per soli vincoli binari.
- **Vincoli di Preferenza (Soft Constraints):**Â Rappresentano una preferenza (es. se possibile, assegna questo rispetto a quello). Sono modellati tramite un costo associato a ciascuna assegnazione (diventando un problema di ottimizzazione vincolata).

### 4.6.2 II. Tecniche di Ricerca di Soluzioni
- [[#A. Framework di Ricerca (Stato Fattorizzato)|A. Framework di Ricerca (Stato Fattorizzato)]]
- [[#B. Ricerca Non Informata: Backtracking Search|B. Ricerca Non Informata: Backtracking Search]]
- [[#C. Miglioramenti tramite Euristiche (Domain-Independent)|C. Miglioramenti tramite Euristiche (Domain-Independent)]]
- [[#D. Ricerca Locale (Local Search)|D. Ricerca Locale (Local Search)]]
#### A. Framework di Ricerca (Stato Fattorizzato)

- **Stati:**Â Definiti dai valori assegnati finora.
- **Stato Iniziale:**Â L'assegnazione vuota ${\ }$.
- **Azioni:**Â Assegnare un valore a una variabile non assegnata che non Ã¨ in conflitto con l'assegnazione corrente.
- **ProfonditÃ  della Soluzione:**Â Ogni soluzione completa appare alla profonditÃ  $n$.
- **Problema di ComplessitÃ :**Â Il numero di foglie (senza ottimizzazioni) Ã¨ $n! d^n$.

#### B. Ricerca Non Informata: Backtracking Search

- **Algoritmo:**Â Backtracking Search Ã¨ essenzialmente una ricerca in profonditÃ  (Depth-First Search) con assegnazione a variabile singola.
- **Fissare l'Ordine:**Â Fissando un ordine per le variabili si elimina la complessitÃ  $n!$, riducendo il numero di foglie a $d^n$.
- **Processo:**Â Sceglie ripetutamente una variabile non assegnata, prova tutti i valori nel suo dominio, ed estende ricorsivamente l'assegnazione. Se la chiamata fallisce, l'assegnazione viene ripristinata e si prova il valore successivo. Se nessun valore funziona, restituisce fallimento.

#### C. Miglioramenti tramite Euristiche (Domain-Independent)

Nello stato fattorizzato esistono euristiche indipendenti dal dominio che velocizzano la ricerca.

- **1. Minimum Remaining Values (MRVs):**
    
    - **Scelta:**Â Scegliere la variabile con ilÂ **minor numero di valori legali**Â (possibili).
    - **Logica:**Â Cerca il fallimento il prima possibile (_Failure-First_) in modo che la potatura (_pruning_) avvenga prima.
- **2. Degree Heuristic:**
    
    - **Scelta:**Â Scegliere la variabile con ilÂ **maggior numero di vincoli**Â tra le variabili ancora da assegnare.
    - **Uso:**Â Serve comeÂ _tie-breaker_Â per variabili con lo stesso numero di mosse legali (stesso MRV).
- **3. Least-Constraining Values (LCVs):**
    
    - **Scelta:**Â Scegliere un valore che escluda ilÂ **minor numero di valori**Â nelle altre variabili ancora da assegnare.
    - **Logica:**Â Mantiene aperte le opzioni, cercando di evitare percorsi stretti che potrebbero essere sbagliati.

#### D. Ricerca Locale (Local Search)

- **Formulazione a Stato Completo:**Â Ogni stato assegna un valore aÂ _ogni_Â variabile.
- **Inizio:**Â Lo stato iniziale Ã¨ un'assegnazione casuale di tutte le variabili.
- **Azione:**Â La ricerca cambia il valore di una sola variabile alla volta.
- **Heuristica Min-Conflicts:**
    - **Scelta:**Â Scegliere il valore cheÂ **rompe il minor numero di vincoli**.
    - **Obiettivo:**Â Avvicina lo stato alla soluzione.
    - **Efficacia:**Â Funziona molto bene; ad esempio, risolve il problema delle N-regine in un numero costante di passi, indipendentemente da $N$.

### 4.6.3 III. Tecniche di Inferenza e Propagazione dei Vincoli

L'inferenza non Ã¨ una ricerca, ma riduce lo spazio di possibili assegnazioni. PuÃ² essere eseguita prima o durante la ricerca.

#### Forward Checking (FC)

- **Funzionamento:**Â Dopo aver assegnato un valore a una variabile $X$, per ogni vincolo $X-Y$, siÂ **eliminano i valori dal dominio di $Y$**Â che violano il vincolo.
- **Azione in caso di fallimento:**Â Se non rimangono valori in un dominio, si esegue ilÂ _backtrack_, poichÃ© l'assegnazione precedente era sbagliata.

#### Arc Consistency (AC)

- **Definizione:**Â Una variabile $X$ Ã¨Â **arc-consistent**Â se per ogni arco $X \to Y$, per ogni valore $x$ nel dominio di $X$, esiste almeno un valore $y$ nel dominio di $Y$ che Ã¨ permesso.
- **Grafo Arc-Consistent:**Â Un grafo Ã¨ arc-consistent se tutte le variabili sono arc-consistent.

#### Algoritmo AC-3

- **Scopo:**Â Rende un grafo arc-consistent o restituisce fallimento.
- **Processo:**
    1. Creare una coda con tutti gli archi.
    2. Ripetere finchÃ© non ci sono piÃ¹ cambiamenti:
        - Selezionare un arco casuale $X \to Y$ e rendere $X$ arc-consistent,Â **riducendo il dominio di $X$**.
        - Se viene rimosso un valore da $X$,Â **aggiungere tutti gli archi $K \to X$ alla coda**Â (perchÃ© $K$ potrebbe non essere piÃ¹ consistente rispetto al nuovo $X$).
        - Se una variabile rimane senza assegnazioni possibili, fallire.
- **ComplessitÃ :**Â $O(cd^3)$, dove $c$ Ã¨ il numero di archi binari e $d$ Ã¨ la dimensione del dominio.

#### Maintaining Arc Consistency (MAC)

- **Funzionamento:**Â MAC aumenta ilÂ _forward checking_Â chiamando AC-3 dopo ogni assegnazione durante la ricerca.
- **Invocazione:**Â Data un'assegnazione a $X$, MAC chiama AC-3 con una coda di archi $Y \to X$ (per tutte le $Y$ non ancora assegnate).
- **Potatura:**Â MAC pota piÃ¹ delÂ _forward checking_Â perchÃ© verifica ricorsivamente le incoerenze.

### 4.6.4 IV. Sfruttare la Struttura del Problema (Semplificazione)

Anche con molte euristiche, i CSP rimangono difficili in generale (worst-case $d^n$). Sfruttare la struttura puÃ² portare a grandi semplificazioni.

#### Componenti Connesse Indipendenti

- **Identificazione:**Â Gli algoritmi sui grafi possono identificare sottostrutture (componenti connesse) nel grafo dei vincoli.
- **Vantaggio:**Â I componenti connesse sono sottoproblemi indipendenti.
- **Soluzione:**Â La soluzione Ã¨ l'unione delle soluzioni dei componenti.
- **ScalabilitÃ :**Â La complessitÃ  scala come $d^c$, dove $c < n$ sono le variabili coinvolte nel sottoproblema.

#### CSPs a Struttura ad Albero (Tree-Structured CSPs)

- **Definizione:**Â Se il grafo dei vincoli Ã¨ un albero (senza cicli), il CSP Ã¨ notevolmente piÃ¹ facile.
- **Costo:**Â La risoluzione costa $O(nd^2)$, che Ã¨Â **lineare**Â in $n$ (numero di variabili).
- **Algoritmo (Nessun Backtrack):**
    1. **Ordinamento Topologico:**Â Scegliere una variabile come radice e ordinare le variabili in modo che ciascuna sia figlia del suo genitore.
    2. **Arc-Consistency:**Â Rendere l'albero arc-consistent (o fallire).
    3. **Assegnazione:**Â Assegnare a ogni nodo un valore consistente con il suo genitore (o fallire).

#### Cutset Conditioning (Piantare alberi dove non ci sono)

- **Scopo:**Â Forzare un CSP a diventare strutturato ad albero.
- **Processo:**Â Si assegna un valore a un sottoinsieme di variabili, chiamatoÂ **cutset**Â ($c$ variabili).
- **Vantaggio:**Â Se il grafo risultante (dopo aver rimosso e assegnato il cutset) puÃ² essere trasformato in un albero, si puÃ² verificare rapidamente se Ã¨ risolvibile.
- **ComplessitÃ :**Â $O(d^c (n-c) d^2)$. L'efficacia dipende dalla dimensione $c$ delÂ _cutset_.

# 5 Games
> **Competitive Game** -> Ricerca contraddittoria (*adversarial search*)
> - Giocatori fanno a turno
> - I giocatori hanno goal opposti, in contrasto
> - Somma zero -> uno vince l'altro perde
> - **Perfettamente informato -> fully observable**

- Stato iniziale -> $S_0$
- $\text{TO-MOVE}(s)$ -> Mossa per spostarsi nello stato $s$
- $\text{ACTIONS}(s)$ -> mosse legali per lo stato $s$
- $\text{RESULT}(s,\ a)$ -> dopo lo spostamento (transition model) il risultato Ã¨ il nuovo stato
- $\text{IS-TERMINAL}(s)$ -> il gioco Ã¨ finito o no
- $\text{UTILITY}(s;\ p)$ -> Funzione di utilitÃ  - payoff ottenuta su uno stato terminale $s$ dal giocatore $p$ -> ossia Ã¨ lo stato finale

![[image-17.png|559x174]]

## 5.1 Game Tree -> Min-Max Problem
> Stesse proprietÃ  della ricerca ad albero 
> - PuÃ² essere infinita se lo spazio degli stati Ã¨ infinito
> - PuÃ² essere infinito se lo spazio degli stati Ã¨ finito ma permette ripetizioni degli stati (posizioni)

Il grafico dello spazio degli stati ha i nodi che rappresentano gli stati e gli archi che rappresentano le possibili azioni
- Lo stesso stato puÃ² essere raggiunto da piÃ¹ direzioni - *path*
- The game tree Ã¨ *sovrapposto* al grafico -> nel gioco la radice Ã¨ la posizione di partenza

Non c'Ã¨ speranza di costruire l'intero game tree per qualsiasi gioco anche di dimensione ragionevole
-  $< 9!$ states for tictactoe $â‰ˆ 360 000$ states -> insostenibile
![[image-18.png|299x242]]

### 5.1.1 MIN - MAX
> La strategia per ogni giocatore Ã¨ un *piano condizionato* (**conditional plan**)
> - *Si puÃ² applicare a qualsiasi gioco deterministico con informazione perfetta*
> - Cosa fa MAX se MIN fa qualcosa?


Se il gioco ha solo come risultato win/lose
- Allora abbiamo una ricerca in un ambiente non deterministico
- Le mosse dell'avversario sono modellate come una distribuzione di probabilitÃ 
- Win=goal, lose = not goal
- **AND-OR albero di ricerca**

Se il risultato ha piÃ¹ valori -> **minmax**
- Ãˆ perfetto per giochi deterministici e con informazioni perfette

![[image-21.png|551x150]]
![[image-19.png|551x306]]

#### Minmax Value
Una volta ottenuto il valore minimo massimo per ogni nodo, puoi recuperare la strategia ottimale

Nelle foglie (nodi terminali) -> $\text{MINIMAX}(s) = \text{UTILITY}(s;\ p)$
- Il valore di utility -> ossia valore del risultato finale

Un nodo intermedio, non terminale -> $\text{MINIMAX}(s) =$ il valore dell'utility **per MAX**
- Ossia il valore **MINIMAX(s)** rappresenta il **valore di utilitÃ  garantito per MAX** se entrambi i giocatori (MAX e MIN) giocano in modo ottimale da quel nodo in poi.
- Ãˆ il miglior risultato che MAX puÃ² ottenere partendo daÂ $s$, considerando che MIN cercherÃ  di minimizzare il punteggio di MAX.

$$
\text{MINIMAX}(s)=\max_{ğ‘âˆˆ\text{ğ´ğ¶ğ‘‡ğ¼ğ‘‚ğ‘ğ‘†}(ğ‘ )}\ \text{MINIMAX}(\text{ğ‘…ğ¸ğ‘†ğ‘ˆğ¿ğ‘‡}(ğ‘ ,\ ğ‘ )) ,\ \text{TO-MOVE}(s) = \text{MAX}
$$
$$
\text{MINIMAX}(s)=\max_{ğ‘âˆˆ\text{ğ´ğ¶ğ‘‡ğ¼ğ‘‚ğ‘ğ‘†}(ğ‘ )}\ \text{MINIMAX}(\text{ğ‘…ğ¸ğ‘†ğ‘ˆğ¿ğ‘‡}(ğ‘ ,\ ğ‘ )) ,\ \text{TO-MOVE}(s) = \text{MIN}
$$
- MAX sceglie l'opzione che massimizza MINIMAX
	- Il punteggio di MAX dipende dalle mosse di min infatti come vediamo nella figura nel turno di MAX (root) il valore Ã¨ 3 perchÃ¨ considera che min faccia la scelta migliore e quindi MAX deve scegliere tra 3 e 2 che sono i valori minimi quando tocca a MIN
- MIN sceglie l'opzione che minimizza MINIMAX
	- Dato che ha l'obbiettivo opposto

> Utilizzando l'algoritmo Minimax:
> 1. **Esegui una ricerca in profonditÃ  (depth-first search) dalla radice**Â per trovare tutti gli stati terminali (foglie)
> 2. **Ottieni l'utilitÃ  delle foglie**Â (i valori numerici associati ai nodi terminali).
> 3. **Ripercorri all'indietro (backtrack) per calcolare il valore Minimax di tutti i nodi intermedi**:
    - Se il nodo genitore Ã¨ un nodoÂ **MIN**, prendi il valoreÂ **minimo**Â tra i valori Minimax dei figli.
    - Se il nodo genitore Ã¨ un nodoÂ **MAX**, prendi il valoreÂ **massimo**Â tra i valori Minimax dei figli.

ES:
Immagina un gioco dove MAX e MIN alternano mosse:
- MAX parte dalla radice e sceglie traÂ $A_1$â€‹,Â $A_2$â€‹,Â $A_3$â€‹.
- Se sceglieÂ $A_1$â€‹, vede 3, 12, 8, ma sa che MIN minimizzerÃ  a 3.
- SceglieÂ $A_1$Â perchÃ© 3 Ã¨ il massimo tra 3, 2, e 2 (valori diÂ $A_1$â€‹,Â $A_2$â€‹,Â $A_3$â€‹)


> [!important] MinMax Search
> - **Completa** -> **SI** con stati finiti
> - **Ottimale** -> **SI**, contro un giocatore perfetto
> - **ComplessitÃ  in Tempo** -> $O(b^m)$
> - **ComplessitÃ  Spazio** -> $O(bm)$ = Depth-First

#### Giocare contro un Avversario Sub-Ottimale
> Minimax ha dei leggeri inconvenienti

A seconda del giocatore puÃ² fare mosse perfette oppure come nella maggior parte dei casi sbagliare e quindi non avere un gioco perfetto.
Se io gioco contro di lui e se la posizione Ã¨ un pareggio con un gioco perfetto, potresti voler fare una mossa complicata, ma non ottimale per provare a prendere la vittoria:
- Se rispondo con l'unica mossa corretta nella posizione: perdi (p=1%)
- Se rispondo con alcune delle mosse sbagliate: vinci (p=85%)
- Se rispondo con il resto delle mosse (non ottime, ne pessime): Ã¨ un pareggio (p=14%)

Questa Ã¨ la differenza tra le cosiddette "mosse del computer" e le "mosse umane"
- Ci sono programmi per computer che cercano di imitare il modo in cui gli esseri umani si comportano contro gli avversari non ottimali

##### ComplessitÃ 
- Branching factor in chess = 35
- Average game length = 40 moves = 80 ply
- Given the complexity of DFS, you get 3580 states to visit
> Per ridurlo abbiamo bisogno del **PRUNING**

### 5.1.2 $\alpha$ - $\beta$ pruning
> Pota i rami che non portano a nessuna mossa migliore nÃ© per MAX nÃ© per MIN

$\alpha$ = valore MINIMAX piÃ¹ grande finora
$\beta$ = valore MINIMAX piÃ¹ piccolo finora
$[\alpha,\ \beta]$
Iniziale = $[-\infty,\ +\infty]$

![[image-22.png]]

**Regola generale**: considera un nodo con MINIMAX = V. 
- Se un nodo della stessa profonditÃ  (MINIMAX=m') o un nodo piÃ¹ alto (MINIMAX=m) ha un valore MINIMAX migliore, non raggiungerai mai il nodo con MINIMAX=V: **puoi potarlo!**


> [!question] PerchÃ¨ Pruning?
> Non ha effetto sulla soluzione
> Con il migliore pruning possiamo ridurre la complessitÃ  a $O(b^{m/2})$
> - Buono ma non sufficiente per problemi con grandezza ragionevole ($\sqrt b$ fattore su cui ha effetto)
> - Il miglior pruning dipende **dall'ordine delle mosse**
> - La generazione di mosse che consentono di potare prima l'albero porta alla migliore potatura -> faccio mosso che permettono di potare piÃ¹ rami

La ricerca del raggio puÃ² essere utilizzata solo per considerare le mosse "top" k ogni volta, in base ad alcune funzioni di valutazione
- Rischi di potare la mossa migliore, ovviamente.

#### Strategie diverse da DFS
- **Type A** strategy
	- Esplorare completamente lo spazio della ricerca fino ad una profonditÃ  limitata
	- Usare un euristica per valutare utility per i nodi alla profonditÃ  finale
- **Type B** strategy
	- Non esplorare le mosse che sembrano pessime basandosi su una data euristica
	- Segui le mosse piÃ¹ promettenti dove possibile -> possibilmente fino alla fine del gioco

Type A funziona se il fattore di brenching (Ã¨ il numero massimo di figli che un nodo puÃ² avere in un albero) non Ã¨ molto largo, Type B Ã¨ utile quando il fattore di brenching Ã¨ molto alto.

Le euristiche vengono **apprese** o sfruttate come una combinazione ponderata di caratteristiche di gioco
**Effetto orizzonte**: l'euristica valuta la posizione come buona, ma esplorando un livello di profonditÃ  in piÃ¹ potrebbe cambiare completamente la valutazione.

### 5.1.3 MCTS - Monte-Carlo Tree Search
